{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "T6BeY23EjU0L",
      "metadata": {
        "id": "T6BeY23EjU0L"
      },
      "source": [
        "# Summer School on Biomedical Imaging with Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cgm9viPTP_dN",
      "metadata": {
        "id": "cgm9viPTP_dN"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/albarqounilab/BILD-Summer-School/blob/main/notebooks/day1/classification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xY9NKCq8cO48",
      "metadata": {
        "id": "xY9NKCq8cO48"
      },
      "source": [
        "![alt_text](https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/helpers/notebook-banner.png)\n",
        "\n",
        "BILD 2025 is organized under the umbrella of the [Strategic Arab-German Network for Affordable and Democratized AI in Healthcare (SANAD)](https://albarqouni.github.io/funded/sanad/), uniting academic excellence and technological innovation across borders. This year’s edition is organized by the [Albarqouni Lab](https://albarqouni.github.io/) at the [University Hospital Bonn](https://www.ukbonn.de/) and the [University of Bonn](https://www.uni-bonn.de/en). We are proud to partner with leading institutions in the region—Lebanese American University, University of Tunis El Manar, and Duhok Polytechnic University — to deliver a truly international learning experience. Over five intensive days in Tunis, you will explore cutting-edge deep-learning techniques for medical imaging through expert lectures, hands-on labs, and collaborative case studies. Engage with peers and faculty from Germany, Lebanon, Iraq, and Tunisia as you develop practical skills in building and deploying AI models for real-world healthcare challenges. We look forward to an inspiring week of interdisciplinary exchange and the shared commitment to advancing affordable, life-saving AI in medicine.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5603fa-4f2d-483d-ba04-4aeed8ddf7c6",
      "metadata": {
        "id": "ea5603fa-4f2d-483d-ba04-4aeed8ddf7c6"
      },
      "source": [
        "## Chest-X-Ray Classification [60 mins]\n",
        "\n",
        "### Today's Goals\n",
        "\n",
        "This session is a practical journey into the world of medical image classification. By the end of this notebook, you will be able to:\n",
        "\n",
        "- Prepare Medical Imaging Data: Load, preprocess, and normalize chest X-ray images for deep learning models.\n",
        "\n",
        "- Understand Classification Data Pipelines: Create a custom PyTorch Dataset and DataLoader tailored for image classification tasks.\n",
        "\n",
        "- Train a Classifier Model: Fine-tune state-of-the-art CNN architectures (e.g., DenseNet, EfficientNet) to classify chest X-rays into diagnostic categories.\n",
        "\n",
        "- Master Classification Metrics: Use and interpret evaluation metrics such as Accuracy, Precision, Recall, F1-score, and ROC-AUC.\n",
        "\n",
        "- Perform Model Explainability & Quality Control: Apply advanced interpretability techniques (Grad-CAM, Score-CAM, LIME) to visualize what regions of the image the model relies on, ensuring predictions are trustworthy.\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You’ll see how AI can be trained to identify pathological findings in chest X-rays, a crucial step in computer-assisted diagnosis. You’ll also apply your classification skills to a challenging real-world problem in medical imaging, while learning how to interpret and validate model predictions beyond raw accuracy scores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6d9ccb1-08d3-4673-b513-39b15963f31c",
      "metadata": {
        "id": "a6d9ccb1-08d3-4673-b513-39b15963f31c"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The [NIH ChestX-ray-14](https://nihcc.app.box.com/v/ChestXray-NIHCC) dataset is a large collection of chest X-ray images. Each image comes with information about the patient and labels that tell us which diseases (if any) are present. This dataset is widely used in medical AI research because it helps us train and test models to recognize diseases from X-ray images.\n",
        "\n",
        "**What does the dataset contain?**\n",
        "1. Over 100,000 chest X-ray images, each in PNG format. These are pictures of the inside of the chest, showing the lungs and heart.\n",
        "2. A metadata file (`Data_Entry_2017.csv`) that lists information about each image, such as:\n",
        "   - Which diseases are present (if any)\n",
        "   - Patient age and gender\n",
        "   - How the image was taken\n",
        "3. A file with bounding boxes (`BBox_List_2017.csv`) for about 1,000 images. These boxes show exactly where a disease is located in the image.\n",
        "4. Files that split the data into training and test sets. This is important because we want to train our model on some images and test it on others to see how well it works on new data.\n",
        "\n",
        "**Why do we use this dataset?**\n",
        "- It is large and diverse, which helps our model learn better.\n",
        "- It has real medical labels, making our project more realistic.\n",
        "- It allows us to practice both classification (is there a disease?) and detection (where is the disease?).\n",
        "\n",
        "In this notebook, we will use a smaller sample of this dataset and pre-trained models to make the exercises faster and easier to follow."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Environment Setup\n",
        "\n",
        "We install and import required libraries. Run this once per new environment.\n",
        "\n",
        "> **Note:** The cell will install packages (internet required). If you're offline, skip installation and ensure the environment already has the packages.\n"
      ],
      "metadata": {
        "id": "QDLtGwZxWZV8"
      },
      "id": "QDLtGwZxWZV8"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title import libraries (2 minutes)\n",
        "!pip install grad-cam\n",
        "!pip install lime\n",
        "!pip install -q huggingface_hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "!pip install pydicom -q\n",
        "import pydicom\n",
        "\n",
        "!pip install SimpleITK -q\n",
        "import SimpleITK as sitk\n",
        "\n",
        "from glob import glob\n",
        "import time\n",
        "import cv2\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import tv_tensors\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pydicom # Added import for pydicom\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "!pip install torchmetrics -q\n",
        "from torchmetrics.classification import BinaryAUROC"
      ],
      "metadata": {
        "id": "O36QhLgXWeBY"
      },
      "id": "O36QhLgXWeBY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "## 2. The Dataset: ChestX-ray14 (NIH) ( 3 minutes)\n",
        "\n",
        "We’ll use the NIH ChestX-ray14 dataset, which contains over 112,000 frontal chest X-ray images from 30,805 patients, labeled with 14 thoracic disease categories (including pneumonia, emphysema, fibrosis, hernia, and no finding).\n",
        "\n",
        "- Size: ~42 GB\n",
        "\n",
        "- Format: JPG images + accompanying CSV files with labels\n",
        "\n",
        "- Source: NIH Clinical Center\n",
        "\n",
        "### 2.1 Downloading the Data (3 minutes)\n",
        "Before we can work with the data, we need to download and unzip it. This means we are copying the files from the internet to our computer and making them ready to use.\n",
        "\n",
        "**Why do we do this?**\n",
        "- Machine learning models need data to learn from. Downloading the dataset gives us the images and labels we need for our project.\n",
        "- Unzipping extracts the files from a compressed format so we can access them easily in our code.\n",
        "\n",
        "**Instructions:**\n",
        "- If you have not downloaded the dataset yet, run the following cells to download and unzip the files.\n",
        "- If you already have the data, you can skip these steps by adding a `#` before the `!` in the code (this comments out the line so it won't run).\n",
        "- You can also change the `DATA_PATH` variable if you want to store the data in a different folder.\n",
        "\n",
        "> **Tip:** Downloading large datasets can take a while, depending on your internet speed.\n"
      ],
      "metadata": {
        "id": "LvHCaUJHW5w4"
      },
      "id": "LvHCaUJHW5w4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ac16006",
      "metadata": {
        "id": "8ac16006"
      },
      "outputs": [],
      "source": [
        "# Download data from Hugging Face\n",
        "!pip install -q huggingface_hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "CWD = '.'\n",
        "DATA_PATH = f\"{CWD}/Classification\"\n",
        "REPO_ID = 'albarqouni/bild-dataset'\n",
        "SUBFOLDER = 'Classification'\n",
        "os.makedirs(CWD, exist_ok=True)\n",
        "\n",
        "# Download csv.zip\n",
        "csv_zip_path = hf_hub_download(\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        "    filename=\"csv.zip\",\n",
        "    subfolder=SUBFOLDER,\n",
        "    local_dir=CWD\n",
        ")\n",
        "\n",
        "# Download data_cxr8.zip\n",
        "data_zip_path = hf_hub_download(\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        "    filename=\"data_cxr8.zip\",\n",
        "    subfolder=SUBFOLDER,\n",
        "    local_dir=CWD\n",
        ")\n",
        "\n",
        "print(\"Download complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e8fc31f",
      "metadata": {
        "id": "4e8fc31f"
      },
      "outputs": [],
      "source": [
        "!unzip -q {DATA_PATH}/csv.zip -d {DATA_PATH}\n",
        "!unzip -q {DATA_PATH}/data_cxr8.zip -d {DATA_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">  RSNA dataset download is not included in the Hugging Face Classification dataset.\n",
        "If needed, add similar Hugging Face download logic here for RSNA or other datasets."
      ],
      "metadata": {
        "id": "JeGWQcggEIpT"
      },
      "id": "JeGWQcggEIpT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2 Downloading Pretrained Model Weights\n",
        "To save time and resources, we’ll use pretrained models (DenseNet121, EfficientNet, Swin Transformer).\n",
        "\n",
        "\n",
        "These weights are downloaded from Hugging Face and will be used for transfer learning.\n",
        "\n",
        "🔹 Why do we use pretrained models?\n",
        "\n",
        "- Training a deep network from scratch on a huge dataset (like 42GB chest X-rays) would take days/weeks and require lots of GPUs.\n",
        "\n",
        "- Instead, we use models that have already been pretrained on ImageNet (1M+ images).\n",
        "\n",
        "- We then fine-tune them on our medical dataset (transfer learning).\n",
        "\n",
        "- This makes training faster and often improves performance.\n",
        "\n",
        "🔹 Common Pretrained Models Used\n",
        "\n",
        "1. DenseNet121\n",
        "\n",
        "    - Dense connections between layers → improves gradient flow.\n",
        "\n",
        "    - Popular for medical imaging tasks (used in the NIH ChestX-ray paper itself).\n",
        "\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1Zr1-ni4pqjiZLHrFJNyXRGdwCFJp1Zrx\" width=\"600\">\n",
        "\n",
        "\n",
        "2. EfficientNet\n",
        "\n",
        "    - Balances model depth, width, and resolution efficiently.\n",
        "\n",
        "    - Often achieves better accuracy with fewer parameters.\n",
        "    <img src=\" https://drive.google.com/uc?export=view&id=173iLsgRvocshQRSG1DICALVakaJm-XxM\" width=\"600\">\n",
        "\n",
        "\n",
        "3. Swin Transformer\n",
        "\n",
        "    - A Vision Transformer (ViT) variant.\n",
        "\n",
        "    - Uses shifted windows for efficient self-attention.\n",
        "\n",
        "    - Very strong performance on classification & detection.\n",
        "   <img src=\"https://drive.google.com/uc?id=15Q75XaSSVRMHIg4GfUNNNAjAG61QWRJT\" width=\"500\">\n"
      ],
      "metadata": {
        "id": "MXgEQHXrXp8Q"
      },
      "id": "MXgEQHXrXp8Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RMTTjnkOeaPs",
      "metadata": {
        "id": "RMTTjnkOeaPs"
      },
      "outputs": [],
      "source": [
        "# Download pretrained model weights from Hugging Face\n",
        "\n",
        "# DenseNet121\n",
        "hf_hub_download(\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        "    filename=\"densenet121-classification.pth\",\n",
        "    subfolder=SUBFOLDER,\n",
        "    local_dir=CWD\n",
        ")\n",
        "# EfficientNet\n",
        "hf_hub_download(\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        "    filename=\"efficientnet-classification.pth\",\n",
        "    subfolder=SUBFOLDER,\n",
        "    local_dir=CWD\n",
        ")\n",
        "# Swin Transformer\n",
        "hf_hub_download(\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        "    filename=\"swintransformer-classification.pth\",\n",
        "    subfolder=SUBFOLDER,\n",
        "    local_dir=CWD\n",
        ")\n",
        "print(\"Model weights downloaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.3 Data Exploration\n",
        "\n",
        "Understanding the Data Structure\n",
        "\n",
        "After downloading and extracting the dataset, we expect the following structure inside the Classification/ folder:\n",
        "```\n",
        "Classification/\n",
        "  images/                              # Contains chest X-ray PNG images\n",
        "    00000005_003.png\n",
        "    00000005_006.png\n",
        "    00000005_007.png\n",
        "    ...\n",
        "  metadata.csv                         # Full metadata: image IDs, labels, patient info\n",
        "  metadata_filtered.csv                # Processed/filtered metadata\n",
        "  train_df.csv                         # Training set split\n",
        "  val_df.csv                           # Validation set split\n",
        "  test_df.csv                          # Test set split\n",
        "  train_val_list.txt                   # Combined train/val list\n",
        "  test_list.txt                        # Test image list\n",
        "  densenet121-classification.pth       # Pretrained DenseNet121 model weights\n",
        "  efficientnet-classification.pth      # Pretrained EfficientNet model weights\n",
        "  swintransformer-classification.pth   # Pretrained Swin Transformer model weights\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "UfU_19EebALr"
      },
      "id": "UfU_19EebALr"
    },
    {
      "cell_type": "markdown",
      "id": "oPha5-43tr6X",
      "metadata": {
        "id": "oPha5-43tr6X"
      },
      "source": [
        "### Load dataframe metadata\n",
        "\n",
        "A **dataframe** is a table of data, like a spreadsheet, that we can easily work with in Python using the pandas library. Here, we load the metadata for all our images. This metadata tells us important information about each image, such as which diseases are present, the patient ID, and more. Loading this information helps us organize and prepare our data for training and testing our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vffq_f-m6X8g",
      "metadata": {
        "id": "Vffq_f-m6X8g"
      },
      "outputs": [],
      "source": [
        "# Load and observe available data\n",
        "metadata_df = pd.read_csv(f'{DATA_PATH}/metadata.csv')\n",
        "metadata_df#.head() # Print the 5 fist rows of the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our dataset, we have two types of information:\n",
        "\n",
        "- A metadata file (metadata.csv) that lists more than 112,000 entries, one for each expected X-ray.\n",
        "\n",
        "- A folder of actual images, which contains only 24,502 files.\n",
        "\n",
        "Now the question is: do all metadata entries have a corresponding image file? Let’s visualize this relationship."
      ],
      "metadata": {
        "id": "kxMCwAB0R5Xk"
      },
      "id": "kxMCwAB0R5Xk"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9,5))\n",
        "\n",
        "# Big box = Metadata\n",
        "ax.add_patch(patches.Rectangle((0.1,0.2),0.6,0.6,fill=None,\n",
        "                               edgecolor=\"blue\",linewidth=2))\n",
        "ax.text(0.4,0.8,\"Metadata.csv\\n(112,120 entries)\",ha=\"center\",fontsize=11,color=\"blue\")\n",
        "\n",
        "# Smaller box = Images\n",
        "ax.add_patch(patches.Rectangle((0.5,0.3),0.35,0.4,fill=None,\n",
        "                               edgecolor=\"orange\",linewidth=2))\n",
        "ax.text(0.675,0.7,\"Images/\\n(24,502)\",ha=\"center\",fontsize=11,color=\"orange\")\n",
        "\n",
        "# Overlap area\n",
        "ax.add_patch(patches.Rectangle((0.5,0.3),0.2,0.4,fill=True,\n",
        "                               facecolor=\"lightgreen\",alpha=0.4,edgecolor=\"green\"))\n",
        "ax.text(0.6,0.5,\"Overlap\\n(24,502 usable)\",ha=\"center\",fontsize=11,color=\"green\")\n",
        "\n",
        "# Annotations\n",
        "ax.annotate(\"Only in Metadata\", xy=(0.2,0.5), xytext=(0.05,0.9),\n",
        "            arrowprops=dict(arrowstyle=\"->\",color=\"blue\"), fontsize=10, color=\"blue\")\n",
        "\n",
        "ax.annotate(\"Only in Images\", xy=(0.8,0.45), xytext=(0.9,0.2),\n",
        "            arrowprops=dict(arrowstyle=\"->\",color=\"orange\"), fontsize=10, color=\"orange\")\n",
        "\n",
        "ax.annotate(\"Both Metadata + Images\", xy=(0.58,0.45), xytext=(0.35,0.25),\n",
        "            arrowprops=dict(arrowstyle=\"->\",color=\"green\"), fontsize=10, color=\"green\")\n",
        "\n",
        "ax.axis(\"off\")\n",
        "plt.title(\"Relationship between Metadata and Images\", fontsize=13)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3ZwEIMR6Okb6"
      },
      "id": "3ZwEIMR6Okb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This view makes it even clearer:\n",
        "\n",
        "- Most of the data exists only in the metadata file, but without images, we can’t use them.\n",
        "\n",
        "- The overlap gives us the real working dataset we’ll use for training and evaluation: 24,502 chest X-rays.\n",
        "\n",
        "- The orange area for ‘Only in Images’ is empty, meaning every image in the folder has a metadata entry — which is good for consistency."
      ],
      "metadata": {
        "id": "Fq5QWbf7QRRb"
      },
      "id": "Fq5QWbf7QRRb"
    },
    {
      "cell_type": "markdown",
      "id": "DP7dICXbtzei",
      "metadata": {
        "id": "DP7dICXbtzei"
      },
      "source": [
        "Now we need to make sure that the information in our dataframe matches the images we actually downloaded. This step filters out any entries in the metadata that do not have a corresponding image file, so we only work with images that are available on our computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ITpC_G566lP_",
      "metadata": {
        "id": "ITpC_G566lP_"
      },
      "outputs": [],
      "source": [
        "imgs = glob(f'{DATA_PATH}/images/*')\n",
        "imgs_basename = [os.path.basename(i) for i in imgs]\n",
        "\n",
        "metadata_df = metadata_df.loc[metadata_df['Image Index'].isin(imgs_basename)]\n",
        "metadata_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot Patient Images\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Show a few random patient images\n",
        "def plot_random_images(metadata_df, data_path, n=6):\n",
        "    \"\"\"Plots n random images with their labels.\"\"\"\n",
        "    sample_df = metadata_df.sample(n)\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    for i, (_, row) in enumerate(sample_df.iterrows()):\n",
        "        img_path = os.path.join(data_path, \"images\", row[\"Image Index\"])\n",
        "        img = plt.imread(img_path)\n",
        "\n",
        "        plt.subplot(2, n//2, i+1)\n",
        "        plt.imshow(img, cmap=\"gray\")\n",
        "        plt.title(f\"Patient {row['Patient ID']}\\nLabel: {row['Finding Labels']}\", fontsize=9)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "plot_random_images(metadata_df, DATA_PATH, n=6)\n"
      ],
      "metadata": {
        "id": "ij9dk1gTfZwC"
      },
      "id": "ij9dk1gTfZwC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "DkfLApQltnLE",
      "metadata": {
        "id": "DkfLApQltnLE"
      },
      "source": [
        "##3. Data Splitting & Preparation\n",
        "\n",
        "### 3.1 Load patient splits\n",
        "\n",
        "To train and evaluate our model properly, we need to split our data into different groups:\n",
        "- **Training set:** Used to teach the model.\n",
        "- **Validation set:** Used to check how well the model is learning during training.\n",
        "- **Test set:** Used to see how well the model works on completely new data.\n",
        "\n",
        "In this step, we load lists of which images belong to each group. This helps us make sure that the model is tested on images it has never seen before, which is important for getting a fair measure of its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eipGr43oiUPw",
      "metadata": {
        "id": "eipGr43oiUPw"
      },
      "outputs": [],
      "source": [
        "train_val_patients = pd.read_csv(f'{DATA_PATH}/train_val_list.txt', header=None, names=['patientId'])\n",
        "test_patients = pd.read_csv(f'{DATA_PATH}/test_list.txt', header=None, names=['patientId'])\n",
        "\n",
        "print(f\"Number of patients in train/val set: {len(train_val_patients)}\")\n",
        "print(f\"Number of patients in test set: {len(test_patients)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E5vsy6EFi4pj",
      "metadata": {
        "id": "E5vsy6EFi4pj"
      },
      "source": [
        "The `.txt` files contain lists of image names that belong to the training/validation or test sets. To use these splits, we need to match the image names in these files with the information in our main database (`metadata.csv`). This way, we know which images and labels go into each group for training and testing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bJ6kj9ArLWPg",
      "metadata": {
        "id": "bJ6kj9ArLWPg"
      },
      "source": [
        "### 3.2 Handle targets\n",
        "\n",
        "In machine learning, a **target** is what we want the model to predict. For this project, the target is the disease label for each image. In this step, we prepare the target labels so that our model can learn to predict them. This may involve simplifying the labels or grouping them in a way that makes the problem easier to solve."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tS7yltl-MNbv",
      "metadata": {
        "id": "tS7yltl-MNbv"
      },
      "source": [
        "In the next step, we look at how many times each disease label appears in our data. Some diseases are very rare, which can make it hard for the model to learn about them. To keep things simple and make sure our model has enough examples to learn from, we will remove labels that appear less than 1,500 times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oxdVDqsIMcbC",
      "metadata": {
        "id": "oxdVDqsIMcbC"
      },
      "outputs": [],
      "source": [
        "label_counts = metadata_df['Finding Labels'].value_counts()\n",
        "label_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tqIz4i7rt-LV",
      "metadata": {
        "id": "tqIz4i7rt-LV"
      },
      "source": [
        "We remove rare labels (diseases that appear in fewer than 1,500 images) so that our model has enough examples to learn from. This helps the model focus on the most common diseases and improves its ability to make accurate predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YK7xNjgKb85a",
      "metadata": {
        "id": "YK7xNjgKb85a"
      },
      "source": [
        "After filtering out rare labels, we are left with the most common disease categories. The table below shows how many images belong to each label. This helps us understand the balance of our dataset and which diseases our model will learn to recognize."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eBW_rxY2uOhB",
      "metadata": {
        "id": "eBW_rxY2uOhB"
      },
      "source": [
        "First, we look at how many images there are for each disease label. This helps us see if some diseases are much more common than others, which can affect how well our model learns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22DM5KLAlaXb",
      "metadata": {
        "id": "22DM5KLAlaXb"
      },
      "outputs": [],
      "source": [
        "label_counts = metadata_df['Finding Labels'].value_counts()\n",
        "rare_labels = label_counts[label_counts < 1500].index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fo_nf5u0uaUy",
      "metadata": {
        "id": "fo_nf5u0uaUy"
      },
      "source": [
        "Now we update our data table (DataFrame) to remove any images with rare disease labels. This makes sure our model only sees images with the most common labels, which helps it learn better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NWjI8zgiubiV",
      "metadata": {
        "id": "NWjI8zgiubiV"
      },
      "outputs": [],
      "source": [
        "metadata_df_filtered = metadata_df[~metadata_df['Finding Labels'].isin(rare_labels)].copy()\n",
        "\n",
        "print(f\"Original shape: {metadata_df.shape}\")\n",
        "print(f\"Filtered shape: {metadata_df_filtered.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hw2tGIC7loOi",
      "metadata": {
        "id": "hw2tGIC7loOi"
      },
      "outputs": [],
      "source": [
        "metadata_df_filtered['Finding Labels'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jz9itQUZj4C8",
      "metadata": {
        "id": "jz9itQUZj4C8"
      },
      "source": [
        "To make our task easier, we will turn the problem into a **binary classification** problem. This means the model will learn to answer a simple question: Is this X-ray healthy or does it show signs of disease?\n",
        "\n",
        "- **Class 0 (Negative):** Images labeled as 'No Finding' (healthy)\n",
        "- **Class 1 (Positive):** Images with any disease label (pathology present)\n",
        "\n",
        "This approach is common in deep learning when starting out, because it is easier for the model to learn to distinguish between just two categories. The category we want the model to predict is called the **target class**. Here, you can also try focusing on a specific disease (like 'Effusion') or experiment with more classes to see how the model behaves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zNFARnXT0TOs",
      "metadata": {
        "id": "zNFARnXT0TOs"
      },
      "outputs": [],
      "source": [
        "keep = {\n",
        "    'No Finding', 'Effusion',\n",
        "}\n",
        "\n",
        "# split each cell into a list, then keep rows where at least one element is in `keep`\n",
        "df_filtered = metadata_df_filtered[\n",
        "    metadata_df_filtered['Finding Labels']\n",
        "      .str.split('|')                         # or .str.split(',') if comma‑separated\n",
        "      .apply(lambda labels: any(lbl in keep for lbl in labels))\n",
        "].copy()\n",
        "df_filtered['Finding Labels'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rFWbq74kvBvB",
      "metadata": {
        "id": "rFWbq74kvBvB"
      },
      "source": [
        "Now we create a new column called `Binary Label` in our data. This column will have a value of 0 for healthy images and 1 for images with any disease. This process is called **label encoding** and is very common in deep learning, because models work best with numbers instead of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Bi2PeFM03hr",
      "metadata": {
        "id": "9Bi2PeFM03hr"
      },
      "outputs": [],
      "source": [
        "df_filtered['Binary Label'] = (df_filtered['Finding Labels'] != 'No Finding').astype(int)\n",
        "df_filtered['Binary Label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MSbPWf5hvEgO",
      "metadata": {
        "id": "MSbPWf5hvEgO"
      },
      "source": [
        "We can further clean our dataset by selecting only one **view acquisition** type for our classifier. 'View acquisition' refers to the way the X-ray image was taken (for example, from the front or the side). Using only one type (like 'PA' for posteroanterior) helps the model learn more consistently, because all images will look similar in terms of orientation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28BaE1wQ0l3I",
      "metadata": {
        "id": "28BaE1wQ0l3I"
      },
      "outputs": [],
      "source": [
        "df_filtered = df_filtered[df_filtered[\"View Position\"] == 'PA']\n",
        "df_filtered['View Position'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5Bssc5DHnWkm",
      "metadata": {
        "id": "5Bssc5DHnWkm"
      },
      "source": [
        "### 3.3 Train / Test Split\n",
        "Now we use the lists of patient IDs to split our data into a **training set** (used to teach the model) and a **test set** (used to check how well the model works on new, unseen data). This is called a **train-test split** and is a key step in building reliable machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BuEJ-KhEnZHX",
      "metadata": {
        "id": "BuEJ-KhEnZHX"
      },
      "outputs": [],
      "source": [
        "# Split df_filtered based on patient IDs from the loaded lists\n",
        "train_val_df = df_filtered[df_filtered['Image Index'].isin(train_val_patients['patientId'])].copy()\n",
        "test_df = df_filtered[df_filtered['Image Index'].isin(test_patients['patientId'])].copy()\n",
        "\n",
        "print(f\"Train val shape: {train_val_df.shape}\")\n",
        "print(f\"Test set shape: {test_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Balance Classes & Prepare Subset\n",
        "\n",
        "Now we balance the dataset by limiting the number of examples per class to 3,000. This ensures that the model sees a similar number of positive (disease) and negative (healthy) examples during training. Balancing the classes is important because an unbalanced dataset can cause the model to become biased toward the majority class, reducing its ability to correctly predict the minority class."
      ],
      "metadata": {
        "id": "IVKWzwhpuQnI"
      },
      "id": "IVKWzwhpuQnI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77H7Yu-4xHmO",
      "metadata": {
        "id": "77H7Yu-4xHmO"
      },
      "outputs": [],
      "source": [
        "df = train_val_df.copy()\n",
        "pos = df[df['Binary Label'] == 1]\n",
        "neg = df[df['Binary Label'] == 0]\n",
        "\n",
        "# # sample up to x each\n",
        "n_samples = 3000\n",
        "pos = pos.sample(n=min(len(pos), n_samples), random_state=42)\n",
        "neg = neg.sample(n=min(len(neg), n_samples), random_state=42)\n",
        "\n",
        "subset = pd.concat([pos, neg]).reset_index(drop=True)\n",
        "print(\"Subset size:\", subset.shape)\n",
        "print(subset['Binary Label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Final Train/Validation Split\n",
        "\n",
        "Now we split the balanced subset into separate training and validation sets. The training set is used to teach the model, while the validation set is used to monitor the model’s learning during training and to tune hyperparameters. We use the stratify parameter to maintain class balance in both splits, ensuring that both sets contain similar proportions of positive and negative examples. This train-validation split is essential for building a reliable model and avoiding overfitting."
      ],
      "metadata": {
        "id": "DR3hoNNJvmYM"
      },
      "id": "DR3hoNNJvmYM"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "train_validation_df, test_df = train_test_split(\n",
        "    subset,\n",
        "    test_size=0.2,\n",
        "    stratify=subset['Binary Label'],\n",
        "    random_state=42\n",
        ")\n",
        "train_df, val_df = train_test_split(\n",
        "    train_validation_df,\n",
        "    test_size=0.1,\n",
        "    stratify=train_validation_df['Binary Label'],\n",
        "    random_state=42\n",
        ")\n",
        "print(\"Train:\", train_df.shape, \"Validation:\", val_df.shape)"
      ],
      "metadata": {
        "id": "xWVzC4w3vUnm"
      },
      "id": "xWVzC4w3vUnm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "F9Vt4KBlxGOW",
      "metadata": {
        "id": "F9Vt4KBlxGOW"
      },
      "source": [
        "### 3.6 Deep Learning Data Terminology\n",
        "\n",
        "Before we start building datasets and data loaders in PyTorch, it is important to understand these key concepts :\n",
        "\n",
        "- **Batch:** A batch is a small group of samples processed together by the model before updating its parameters. Using batches makes training faster and more stable.\n",
        "- **Epoch:** One epoch means the model has seen all the training data once. Training usually takes many epochs.\n",
        "- **DataLoader:** In PyTorch, a DataLoader helps us load data in batches, shuffle it, and use multiple CPU cores to speed up the process. This is essential for efficient deep learning training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BCJnQBP1wE03",
      "metadata": {
        "id": "BCJnQBP1wE03"
      },
      "source": [
        "## 4. Download Pretrained Model & Prepare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6ius3AZvwsR",
      "metadata": {
        "id": "f6ius3AZvwsR"
      },
      "source": [
        "\n",
        "This week, we delve into the power of deep models like CNNs, leveraging the PyTorch library as our framework. PyTorch provides the flexibility and tools necessary to explore and implement these complex architectures for challenging tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fUgU5u8OwvAF",
      "metadata": {
        "id": "fUgU5u8OwvAF"
      },
      "source": [
        "### 4.1 Define Custom Dataset in PyTorch\n",
        "Next we define our custom `ChestXrayDataset` using torch `Dataset` from `torch.utils.data`\n",
        "\n",
        "\n",
        "In PyTorch, datasets are represented as classes inheriting from torch.utils.data.Dataset. Here, we define a ChestXrayDataset class to handle image loading and preprocessing.\n",
        "\n",
        "\n",
        "This dataset class does three main things:\n",
        "\n",
        "-  Loads the X-ray images from a directory.\n",
        "\n",
        "-  Applies any preprocessing or transformations (resizing, normalization, augmentation) specified by transform.\n",
        "\n",
        "-  Returns the image and its corresponding label as a PyTorch tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jujnAUaMxBkF",
      "metadata": {
        "id": "jujnAUaMxBkF"
      },
      "outputs": [],
      "source": [
        "class ChestXrayDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['Image Index'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = torch.tensor(row['Binary Label'], dtype=torch.float32)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OKhmhPbJw4L0",
      "metadata": {
        "id": "OKhmhPbJw4L0"
      },
      "source": [
        "### 4.2 Image Transforms\n",
        "\n",
        "**Transforms** are changes we make to images as we load them. This can include resizing, flipping, rotating, or normalizing the images. When we do these changes randomly during training, it is called **data augmentation**. Data augmentation helps the model learn to recognize patterns in different situations, making it more robust and less likely to memorize the training data (a problem called overfitting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wG5zmY0Ow-8T",
      "metadata": {
        "id": "wG5zmY0Ow-8T"
      },
      "outputs": [],
      "source": [
        "mean = [0.485, 0.456, 0.406]\n",
        "std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "image_size_= 224\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((image_size_,image_size_)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xM4XoBfDzQub",
      "metadata": {
        "id": "xM4XoBfDzQub"
      },
      "source": [
        "Now the `val_transforms`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n0lqTxmPzUmL",
      "metadata": {
        "id": "n0lqTxmPzUmL"
      },
      "outputs": [],
      "source": [
        "val_transforms   = transforms.Compose([\n",
        "    transforms.Resize((image_size_,image_size_)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4xyIR_3MyqDg",
      "metadata": {
        "id": "4xyIR_3MyqDg"
      },
      "source": [
        "### 4.3 Dataloaders\n",
        "\n",
        "A **DataLoader** is a tool in PyTorch that helps us load data in small groups called **mini-batches**. Instead of giving the model one image at a time, we give it a batch of images. This makes training faster and helps the model learn more stable patterns. Dataloaders also make it easy to shuffle the data and use multiple CPU cores for loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e_dne34tyrOs",
      "metadata": {
        "id": "e_dne34tyrOs"
      },
      "outputs": [],
      "source": [
        "img_dir  = f'{DATA_PATH}/images'\n",
        "\n",
        "train_ds = ChestXrayDataset(train_df, img_dir, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=90, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaZ_U7XzzkGP",
      "metadata": {
        "id": "eaZ_U7XzzkGP"
      },
      "source": [
        "We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 32, *i.e.* each element in the dataloader iterable will return a batch of 32 features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nAy1Y-rlzmEp",
      "metadata": {
        "id": "nAy1Y-rlzmEp"
      },
      "outputs": [],
      "source": [
        "for X, y in train_loader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adTYoq-gz4Yi",
      "metadata": {
        "id": "adTYoq-gz4Yi"
      },
      "outputs": [],
      "source": [
        "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZR_-CnJYy-Kv",
      "metadata": {
        "id": "ZR_-CnJYy-Kv"
      },
      "outputs": [],
      "source": [
        "val_ds   = ChestXrayDataset(val_df,   img_dir, transform=val_transforms)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, num_workers=32, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WO8Sp67b0h_n",
      "metadata": {
        "id": "WO8Sp67b0h_n"
      },
      "source": [
        "## 4.4 Pretrained Models & Transfer Learning\n",
        "\n",
        "A **pretrained model** is a model that has already been trained on a large dataset (like ImageNet) and has learned useful features. The structure of the model is called its **architecture** (for example, DenseNet, ResNet, EfficientNet). Using a pretrained model and adapting it to our own data is called **transfer learning**. This is very helpful because it allows us to get good results even with smaller datasets and less training time.\n",
        "In 'torchvision.models' we can find many popular pretrained models and architectures."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "id": "vEEolVerMV5E"
      },
      "id": "vEEolVerMV5E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SPkgRRks0j5p",
      "metadata": {
        "id": "SPkgRRks0j5p"
      },
      "outputs": [],
      "source": [
        "torchvision.models.list_models()[::30]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0cc7bbb",
      "metadata": {
        "id": "a0cc7bbb"
      },
      "source": [
        "### 4.5  Understanding Model Layers\n",
        "\n",
        "When looking at a deep learning model, you will see several types of layers. Here is what to look for in each:\n",
        "\n",
        "- **Convolutional layers:** These are the building blocks of most image models. They scan the input image with small filters (sliding windows) to detect patterns like edges, shapes, or textures. The first convolutional layer takes the raw image (with 1 channel for grayscale or 3 for RGB) and produces feature maps.\n",
        "- **Normalization layers (BatchNorm):** These layers help the model train faster and more reliably by keeping the outputs of previous layers at a similar scale. Batch Normalization (BatchNorm) is the most common type. It makes training more stable and helps the model generalize better.\n",
        "- **Pooling layers:** Pooling reduces the size of the feature maps, making the model faster and helping it focus on the most important features. The most common is Max Pooling, which keeps only the largest value in each region.\n",
        "- **Activation functions:** After each convolution, the model uses an activation function (like ReLU) to introduce non-linearity. This helps the model learn complex patterns, not just straight lines.\n",
        "\n",
        "- **First layer:** This is usually a convolutional layer that takes the input image. Check its input dimension (number of channels, usually 1 for grayscale or 3 for RGB images).\n",
        "- **Second layer:** Often another convolutional, normalization, activation, or pooling layer, building on the features from the first.\n",
        "- **Second to last layer:** This is typically a feature layer just before the classifier. Its output dimension shows the number of features passed to the final classifier.\n",
        "- **Last layer:** This is the classifier or output layer. Its output dimension should match the number of classes (1 for binary classification).\n",
        "\n",
        "By examining these layers, you can understand how the model processes the input and what features are used for the final prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 Load Pretrained Model\n",
        "\n",
        "- We load DenseNet121 pretrained on ImageNet.\n",
        "\n",
        "- Replace the classifier with a single output unit for binary classification.\n",
        "\n",
        "- Printing first, second, second-to-last, and last layers gives insight into the model structure."
      ],
      "metadata": {
        "id": "E-Pz2iQT7FXr"
      },
      "id": "E-Pz2iQT7FXr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UzO-t4Fx0nXV",
      "metadata": {
        "id": "UzO-t4Fx0nXV"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = models.densenet121(pretrained=True)\n",
        "model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
        "model = model.to(device)\n",
        "\n",
        "# Print only the first and last two layer blocks\n",
        "layers = list(model.children())\n",
        "print('First layer block:')\n",
        "print(layers[0])\n",
        "print('\\n---')\n",
        "print('Second layer block:')\n",
        "print(layers[1])\n",
        "print('\\n...')\n",
        "print('Second to last layer block:')\n",
        "print(layers[-2])\n",
        "print('\\n---')\n",
        "print('Last layer block:')\n",
        "print(layers[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Train the Model (10 minutes)\n"
      ],
      "metadata": {
        "id": "lTlY4zP295rs"
      },
      "id": "lTlY4zP295rs"
    },
    {
      "cell_type": "markdown",
      "id": "qaEdGcOx0y9Q",
      "metadata": {
        "id": "qaEdGcOx0y9Q"
      },
      "source": [
        "### 5.1 Hyperparameters\n",
        "\n",
        "**Hyperparameters** are settings that you choose before training your model. They control how the learning process works. Common hyperparameters include:\n",
        "- **Number of epochs:** How many times the model sees the whole training set.\n",
        "- **Batch size:** How many samples are in each batch.\n",
        "- **Learning rate:** How big the steps are when updating the model's weights.\n",
        "\n",
        "Tuning hyperparameters is important because it can make a big difference in how well your model learns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CP3Y2XJR0znF",
      "metadata": {
        "id": "CP3Y2XJR0znF"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UTXB_k_Y08_f",
      "metadata": {
        "id": "UTXB_k_Y08_f"
      },
      "source": [
        "### 5.2 Optimization loop\n",
        "\n",
        "Training a deep learning model involves an **optimization loop**. Each time the model sees the whole training set, it completes one **epoch**. The process has two main parts:\n",
        "- **Train loop:** The model learns from the training data and updates its parameters.\n",
        "- **Validation loop:** The model is tested on validation data to see how well it is learning.\n",
        "\n",
        "A **loss function** measures how far the model's predictions are from the true answers. The goal of training is to minimize this loss. The optimization loop repeats for many epochs until the model performs well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AFH4boS51SCH",
      "metadata": {
        "id": "AFH4boS51SCH"
      },
      "source": [
        "Inside the training loop, the model learns by adjusting its parameters using **gradients**. Gradients show how much each parameter should change to reduce the loss. The process of calculating gradients and updating parameters is called **backpropagation**.\n",
        "\n",
        "- **optimizer.zero_grad():** Resets the gradients to zero before each batch.\n",
        "- **loss.backward():** Calculates the gradients using backpropagation.\n",
        "- **optimizer.step():** Updates the model's parameters using the gradients.\n",
        "- **Learning rate scheduler (like OneCycleLR):** Adjusts the learning rate during training to help the model learn better and faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UEz2Amsl09dM",
      "metadata": {
        "id": "UEz2Amsl09dM"
      },
      "outputs": [],
      "source": [
        "# Re-initialize the model after cleanup\n",
        "model = models.densenet121(pretrained=True)\n",
        "model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = OneCycleLR(optimizer, max_lr=learning_rate, epochs=epochs, steps_per_epoch=len(train_loader), total_steps=epochs * len(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0PlztayZ1JN7",
      "metadata": {
        "id": "0PlztayZ1JN7"
      },
      "outputs": [],
      "source": [
        "pos_frac = train_df['Binary Label'].mean()\n",
        "pos_weight = torch.tensor([(1 - pos_frac) / pos_frac]).to(device)\n",
        "\n",
        "criterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rlu-3UTk4j87",
      "metadata": {
        "id": "Rlu-3UTk4j87"
      },
      "source": [
        "### 5.3 Define Training & Validation Functions\n",
        "\n",
        "Next, we define our **training function** and **validation function**. The training function teaches the model using the training data, while the validation function checks how well the model is doing on data it hasn't seen before. Keeping these functions separate helps us monitor the model's progress and avoid overfitting (when the model memorizes the training data but doesn't generalize well to new data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6KZltOsh5EqE",
      "metadata": {
        "id": "6KZltOsh5EqE"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, loader, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for imgs, labels in tqdm(loader, desc=\"  Training\", leave=False):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs).squeeze(1)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        try:\n",
        "            scheduler.step()\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def val_loop(model, loader, criterion, auroc, device):\n",
        "    model.eval()\n",
        "    auroc.reset()\n",
        "    running_preds = []\n",
        "    running_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(loader, desc=\"  Validation\", leave=False):\n",
        "            imgs = imgs.to(device)\n",
        "            logits = model(imgs).squeeze(1)\n",
        "            probs = torch.sigmoid(logits)\n",
        "\n",
        "            preds = (probs > 0.5).int().cpu().numpy()\n",
        "            running_preds.extend(preds.tolist())\n",
        "            running_labels.extend(labels.int().tolist())\n",
        "\n",
        "            auroc.update(probs, labels.int().to(device))\n",
        "\n",
        "    acc = accuracy_score(running_labels, running_preds)\n",
        "    val_auroc = auroc.compute().item()\n",
        "    return acc, val_auroc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Load Dataset\n",
        "\n",
        "Define the train and validation datasets and dataloaders."
      ],
      "metadata": {
        "id": "Gkc2GXQ0Av_0"
      },
      "id": "Gkc2GXQ0Av_0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J38g7NXJ4kQq",
      "metadata": {
        "id": "J38g7NXJ4kQq"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_dir  = f'{DATA_PATH}/images'\n",
        "\n",
        "train_ds = ChestXrayDataset(train_df, img_dir, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=90, pin_memory=True)\n",
        "\n",
        "val_ds   = ChestXrayDataset(val_df,   img_dir, transform=val_transforms)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, num_workers=32, pin_memory=True)\n",
        "\n",
        "auroc = BinaryAUROC().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5 Train the DenseNet121 Model\n",
        "\n",
        "We now start the actual training loop for DenseNet121."
      ],
      "metadata": {
        "id": "P7eHpm6yA7tL"
      },
      "id": "P7eHpm6yA7tL"
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "    train_loss = train_loop(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    val_acc, val_auroc = val_loop(model, val_loader, criterion, auroc, device)\n",
        "\n",
        "    print(f\"  Train Loss: {train_loss:.4f}  |  Val Acc: {val_acc:.4f}  |  Val AUROC: {val_auroc:.4f}\")\n",
        "# ---- Save only the final model ----\n",
        "torch.save(model.state_dict(), \"DenseNet121_final.pth\")"
      ],
      "metadata": {
        "id": "SPGrRs1PA3CN"
      },
      "id": "SPGrRs1PA3CN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "G2gzcB-Z5_Xi",
      "metadata": {
        "id": "G2gzcB-Z5_Xi"
      },
      "source": [
        "## 6. Benchmarking model architectures (20 minutes)\n",
        "\n",
        "A **CNN architecture** is the specific design or structure of a convolutional neural network. Different architectures (like ResNet, DenseNet, EfficientNet, Swin Transformer) use different building blocks:\n",
        "- **Skip connections:** Allow information to skip layers, helping very deep networks learn better (used in ResNet).\n",
        "- **Dense connections:** Connect each layer to every other layer in a block, improving information flow (used in DenseNet).\n",
        "- **Normalization layers:** Help stabilize and speed up training by keeping the data flowing through the network at a similar scale.\n",
        "\n",
        "Trying different architectures is important because some may work better for your specific problem. In this section, you will train and compare several architectures to see which performs best on your data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LPuX8sCi6FvW",
      "metadata": {
        "id": "LPuX8sCi6FvW"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Q1.</b> In deep learning, different **model architectures** can have a big impact on performance. Complete the following cells to train and compare these models:\n",
        "    - EfficientNet\n",
        "    - Swin Transformer\n",
        "</div>\n",
        "\n",
        "Comparing different models helps you understand which design works best for your specific task and data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E_0Pwmcd97n8",
      "metadata": {
        "id": "E_0Pwmcd97n8"
      },
      "source": [
        "### EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yOEjyXbo-Dty",
      "metadata": {
        "id": "yOEjyXbo-Dty"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_enb0 = ... # COMPLETE\n",
        "model_enb0.classifier[1] = nn.Linear(1280, 1)\n",
        "\n",
        "model = ... # COMPLETE to device\n",
        "print(model)\n",
        "\n",
        "learning_rate = ... # COMPLETE\n",
        "batch_size = ... # COMPLETE\n",
        "epochs = ... # COMPLETE\n",
        "\n",
        "pos_frac = train_df['Binary Label'].mean()\n",
        "pos_weight = torch.tensor([(1 - pos_frac) / pos_frac]).to(device)\n",
        "criterion  = ... # COMPLETE\n",
        "\n",
        "optimizer = ... # COMPLETE\n",
        "scheduler = OneCycleLR(optimizer, max_lr=learning_rate, epochs=epochs, steps_per_epoch=len(train_loader), total_steps=epochs * len(train_loader)) # Explicitly set total_steps\n",
        "\n",
        "img_dir  = f'{DATA_PATH}/images'\n",
        "\n",
        "train_ds = ChestXrayDataset(train_df, img_dir, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=90, pin_memory=True)\n",
        "\n",
        "val_ds   = ChestXrayDataset(val_df,   img_dir, transform=val_transforms)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, num_workers=32, pin_memory=True)\n",
        "\n",
        "auroc = BinaryAUROC().to(device)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "    train_loss = train_loop(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    val_acc, val_auroc = val_loop(model, val_loader, criterion, auroc, device)\n",
        "\n",
        "    print(f\"  Train Loss: {train_loss:.4f}  |  Val Acc: {val_acc:.4f}  |  Val AUROC: {val_auroc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the trained model"
      ],
      "metadata": {
        "id": "lhH3M5yvNvis"
      },
      "id": "lhH3M5yvNvis"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), './efficientnet-classification.pth')\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "id": "JWSY3NDONxMu"
      },
      "id": "JWSY3NDONxMu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "P2PotfWt9_ef",
      "metadata": {
        "id": "P2PotfWt9_ef"
      },
      "source": [
        "### SwinTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UD-eEjS8_PWr",
      "metadata": {
        "id": "UD-eEjS8_PWr"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import swin_t, Swin_T_Weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_swin = ... # COMPLETE\n",
        "model_swin.head = nn.Linear(in_features=768, out_features=1, bias=True)\n",
        "model = ... # COMPLETE to device\n",
        "print(model)\n",
        "\n",
        "learning_rate = ... # COMPLETE\n",
        "batch_size = ... # COMPLETE\n",
        "epochs = ... # COMPLETE\n",
        "\n",
        "pos_frac = train_df['Binary Label'].mean()\n",
        "pos_weight = torch.tensor([(1 - pos_frac) / pos_frac]).to(device)\n",
        "criterion  = ... # COMPLETE\n",
        "\n",
        "optimizer = ... # COMPLETE\n",
        "scheduler = OneCycleLR(optimizer, max_lr=learning_rate, epochs=epochs, steps_per_epoch=len(train_loader), total_steps=epochs * len(train_loader)) # Explicitly set total_steps\n",
        "\n",
        "img_dir  = f'{DATA_PATH}/CXR8/images'\n",
        "\n",
        "train_ds = ChestXrayDataset(train_df, img_dir, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=90, pin_memory=True)\n",
        "\n",
        "val_ds   = ChestXrayDataset(val_df,   img_dir, transform=val_transforms)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, num_workers=32, pin_memory=True)\n",
        "\n",
        "auroc = BinaryAUROC().to(device)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "    train_loss = train_loop(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    val_acc, val_auroc = val_loop(model, val_loader, criterion, auroc, device)\n",
        "\n",
        "    print(f\"  Train Loss: {train_loss:.4f}  |  Val Acc: {val_acc:.4f}  |  Val AUROC: {val_auroc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Swin Transformer initially learned useful features (Val Acc ~0.83, AUROC ~0.86) but quickly collapsed. From epoch 3 onward, AUROC dropped to ~0.5 while accuracy reflected the majority class, indicating the model was no longer distinguishing classes. This behavior is caused by a high learning rate destabilizing pretrained weights and class imbalance in the dataset. AUROC is the better metric here, showing the model is effectively guessing randomly."
      ],
      "metadata": {
        "id": "MStBLzpEBI01"
      },
      "id": "MStBLzpEBI01"
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df['Binary Label'].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "XFsgDrLs4BVr"
      },
      "id": "XFsgDrLs4BVr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows a class imbalance in the dataset, with the majority class (0) being about twice the size of the minority class (1).\n",
        "\n",
        "We fix this by:\n",
        "\n",
        "- Using a WeightedRandomSampler to create balanced batches during training.\n",
        "\n",
        "- Applying a class-weighted loss (BCEWithLogitsLoss with pos_weight) to give more importance to the minority class.\n",
        "\n",
        "- Adjusting learning rate and batch size for more stable training."
      ],
      "metadata": {
        "id": "2CMWsg3LCHq9"
      },
      "id": "2CMWsg3LCHq9"
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import swin_t, Swin_T_Weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_swin = ... # COMPLETE\n",
        "model_swin.head = nn.Linear(in_features=768, out_features=1, bias=True)\n",
        "model = ... # COMPLETE to device\n",
        "print(model)\n",
        "\n",
        "learning_rate = ... # COMPLETE\n",
        "batch_size = ... # COMPLETE\n",
        "epochs = ... # COMPLETE\n",
        "\n",
        "pos_frac = train_df['Binary Label'].mean()\n",
        "pos_weight = torch.tensor([(1 - pos_frac) / pos_frac]).to(device)\n",
        "criterion  = ... # COMPLETE\n",
        "\n",
        "optimizer = ... # COMPLETE\n",
        "scheduler = OneCycleLR(optimizer, max_lr=learning_rate, epochs=epochs, steps_per_epoch=len(train_loader), total_steps=epochs * len(train_loader)) # Explicitly set total_steps\n",
        "\n",
        "img_dir  = f'{DATA_PATH}/CXR8/images'\n",
        "\n",
        "train_ds = ChestXrayDataset(train_df, img_dir, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=90, pin_memory=True)\n",
        "\n",
        "val_ds   = ChestXrayDataset(val_df,   img_dir, transform=val_transforms)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, num_workers=32, pin_memory=True)\n",
        "\n",
        "auroc = BinaryAUROC().to(device)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "    train_loss = train_loop(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    val_acc, val_auroc = val_loop(model, val_loader, criterion, auroc, device)\n",
        "\n",
        "    print(f\"  Train Loss: {train_loss:.4f}  |  Val Acc: {val_acc:.4f}  |  Val AUROC: {val_auroc:.4f}\")"
      ],
      "metadata": {
        "id": "3EOlmREr4rAr"
      },
      "id": "3EOlmREr4rAr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After these changes, the model achieves a high Val AUROC (~0.91), showing it can discriminate between classes effectively despite the imbalance."
      ],
      "metadata": {
        "id": "VxdaYvqLCTIN"
      },
      "id": "VxdaYvqLCTIN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the trained model"
      ],
      "metadata": {
        "id": "0WOK4QPTOAd5"
      },
      "id": "0WOK4QPTOAd5"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), './swintransformer-classification.pth')\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "id": "lE0lv7YhOBPt"
      },
      "id": "lE0lv7YhOBPt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Benchmarking Summary\n",
        "\n",
        "We trained three different architectures on our chest X-ray dataset: **DenseNet121**, **EfficientNet-B0**, and **Swin Transformer**. Key results:\n",
        "\n",
        "| Model            | Train Loss (final) | Val Acc (final) | Val AUROC (final) |\n",
        "|-----------------|-----------------|----------------|------------------|\n",
        "| DenseNet121      | 0.260           | 0.875          | 0.911            |\n",
        "| EfficientNet-B0  | 0.198           | 0.853          | 0.904            |\n",
        "| Swin Transformer | 0.435           | 0.834          | 0.915            |\n",
        "\n",
        "**Observations:**\n",
        "- DenseNet121 has high validation accuracy and AUROC.\n",
        "- EfficientNet is efficient with low train loss.\n",
        "- Swin Transformer captures global features but requires careful tuning.\n",
        "\n",
        "\n",
        "\n",
        "After completing the experiments, summarize the results:\n",
        "\n",
        "- Which loss function achieved the highest validation performance (e.g., best F1-score or AUROC) on the triage classification task?\n",
        "\n",
        "- Why do you think that loss function worked better than the others for this problem?\n",
        "\n",
        "- Did combining different losses (e.g., CrossEntropy + Focal) provide a “best of both worlds” effect, or did a simpler loss function perform just as well or even better?\n"
      ],
      "metadata": {
        "id": "cSPhS2LtDKv_"
      },
      "id": "cSPhS2LtDKv_"
    },
    {
      "cell_type": "markdown",
      "id": "2iEXO2Z46XIu",
      "metadata": {
        "id": "2iEXO2Z46XIu"
      },
      "source": [
        "## 7. Final Evaluation\n",
        "\n",
        "We have trained our models (DenseNet121, EfficientNet-B0, Swin Transformer) and used the validation set to guide hyperparameter tuning. Now, we evaluate them on the held-out test set, which the models have never seen before. This gives the most honest estimate of performance on new, unseen patients — the final verdict on model capability.\n",
        "\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Q2.</b> Compare the performance of the four models using appropriate metrics: <br>\n",
        "    - Plot the ROC curve <br>\n",
        "    - Accuracy score <br>\n",
        "    - Precision and Recall <br>\n",
        "    - F1-score <br>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wNVpLAryCLmx",
      "metadata": {
        "id": "wNVpLAryCLmx"
      },
      "source": [
        "Load the saved models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-dMPkiNwCNCW",
      "metadata": {
        "id": "-dMPkiNwCNCW"
      },
      "outputs": [],
      "source": [
        "model_densenet = ... # COMPLETE\n",
        "model_densenet.classifier = ... # COMPLETE\n",
        "\n",
        "model_enb0 = ... # COMPLETE\n",
        "model_enb0.classifier[1] = ... # COMPLETE\n",
        "\n",
        "model_swin = ... # COMPLETE\n",
        "model_swin.head = ... # COMPLETE\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_densenet.load_state_dict(torch.load('... # COMPLETE))\n",
        "model_enb0.load_state_dict(torch.load('.... # COMPLETE))\n",
        "model_swin.load_state_dict(torch.load('.... # COMPLETE))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Evaluate your models on the test set using the following metrics:\n",
        "\n",
        "- Accuracy: Fraction of correctly classified cases. Gives a quick overview but can be misleading with imbalanced classes.\n",
        "\n",
        "- AUROC (Area Under the ROC Curve): Measures the model’s ability to discriminate positive vs. negative classes; robust to class imbalance.\n",
        "\n",
        "- Precision: Of all predicted positives, how many are truly positive? High precision avoids unnecessary alarm.\n",
        "\n",
        "- Recall (Sensitivity): Of all true positives, how many were detected? High recall avoids missing urgent cases.\n",
        "\n",
        "- F1-score: Harmonic mean of precision and recall, balancing false positives and negatives.\n",
        "\n",
        ">Tip: For imbalanced medical datasets, AUROC and F1-score are usually more reliable than plain accuracy."
      ],
      "metadata": {
        "id": "VU4fPyIAb82T"
      },
      "id": "VU4fPyIAb82T"
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds     = ChestXrayDataset(test_df, img_dir, transform=val_transforms)\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=32,\n",
        "    num_workers=32,\n",
        "    pin_memory=True,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "model_list  = [model_densenet, model_enb0, model_swin]\n",
        "model_names = ['DenseNet 121', 'EfficientNet B0', 'Swin Transformer']\n",
        "\n",
        "all_preds_proba = {}\n",
        "all_labels      = None\n",
        "\n",
        "for model, name in zip(model_list, model_names):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    probs_list, labels_list = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X = X.to(device)\n",
        "            out = model(X)\n",
        "            probs = torch.sigmoid(out)\n",
        "            probs = probs.squeeze(1)\n",
        "            probs_list.extend(probs.cpu().numpy())\n",
        "            labels_list.extend(y.numpy())\n",
        "\n",
        "    preds_proba = np.array(probs_list)\n",
        "    labels      = np.array(labels_list)\n",
        "\n",
        "    all_preds_proba[name] = preds_proba\n",
        "    if all_labels is None:\n",
        "        all_labels = labels\n",
        "\n",
        "    preds_binary = (preds_proba > 0.5).astype(int)\n",
        "\n",
        "    acc     = ... # COMPLETE\n",
        "    prec    = ... # COMPLETE\n",
        "    rec     = ... # COMPLETE\n",
        "    f1      = ... # COMPLETE\n",
        "    roc_auc = ... # COMPLETE)\n",
        "\n",
        "... # COMPLETE WITH ROC Curve display, and other metrics\n"
      ],
      "metadata": {
        "id": "AZnueLq3frOH"
      },
      "id": "AZnueLq3frOH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclude: Which model gives the best performance and should be selected?"
      ],
      "metadata": {
        "id": "K8V5BczZHycx"
      },
      "id": "K8V5BczZHycx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part II: Quality Control\n",
        "\n",
        "Once a deep learning model is trained, it is crucial not only to evaluate its performance using metrics like accuracy, precision, or ROC-AUC, but also to understand why the model makes certain predictions. This is especially important in medical imaging, where trust and interpretability are critical.\n",
        "\n",
        "To achieve this, we use visual attribution methods that highlight the regions of the input image that most influenced the model's decision. In this project, we apply the following explainable AI techniques using the pytorch_grad_cam library:\n",
        "\n",
        "- Grad-CAM (Gradient-weighted Class Activation Mapping):\n",
        "Generates a heatmap that highlights the important regions of the image by computing the gradients of the target class with respect to the feature maps of the last convolutional layer.\n",
        "\n",
        "- Score-CAM:\n",
        "Unlike Grad-CAM, it does not rely on gradients. Instead, it evaluates the importance of each activation map by measuring how much it increases the score for the target class, making it more stable and sometimes more visually interpretable.\n",
        "\n",
        "- Grad-CAM++:\n",
        "An improved version of Grad-CAM that can better handle multiple occurrences of the target class in the same image and generally produces sharper localization maps.\n",
        "\n",
        "- LIME (Local Interpretable Model-agnostic Explanations):\n",
        "A model-agnostic method that perturbs the input image and observes the change in prediction, creating a weighted map of important superpixels.\n",
        "\n",
        "Importance of this step:\n",
        "\n",
        "- Provides interpretability, helping clinicians understand which regions of the image are influencing predictions.\n",
        "\n",
        "- Detects potential biases in the model.\n",
        "\n",
        "- Builds trust in AI-assisted diagnosis by making the model’s decision process transparent.\n",
        "\n",
        "- Supports error analysis, helping improve model design and dataset quality.\n",
        "\n",
        "By applying Grad-CAM, Score-CAM, Grad-CAM++, and LIME on our test set, we generate visual explanations for the model predictions and identify the most relevant regions in the X-ray images that contributed to each prediction. This step bridges the gap between model performance metrics and actionable insights for real-world medical applications."
      ],
      "metadata": {
        "id": "0WXKgwPGtjkY"
      },
      "id": "0WXKgwPGtjkY"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utility functions\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "def visualize_gradcam(\n",
        "    original_images: torch.Tensor,  # (N, C, H, W)\n",
        "    heatmaps:         np.ndarray,    # (N, H_cam, W_cam)\n",
        "    true_labels:      np.ndarray,    # (N,)\n",
        "    num_to_show:      int = 5,\n",
        "    mean:             float|list|tuple = 0.5,\n",
        "    std:              float|list|tuple = 0.2\n",
        "):\n",
        "    \"\"\"\n",
        "    - original_images: Tensor(N, C, H, W), normalized via (x-mean)/std\n",
        "    - heatmaps:        ndarray(N, H_cam, W_cam) in [0,1]\n",
        "    - true_labels:     ndarray(N,)\n",
        "    \"\"\"\n",
        "    N = min(num_to_show, original_images.shape[0])\n",
        "\n",
        "    # prepare mean/std arrays for un-normalization\n",
        "    if isinstance(mean, (list, tuple, np.ndarray)):\n",
        "        mean_arr = np.array(mean)[:, None, None]\n",
        "        std_arr  = np.array(std)[:,  None, None]\n",
        "    else:\n",
        "        mean_arr = mean\n",
        "        std_arr  = std\n",
        "\n",
        "    for i in range(N):\n",
        "        # 1) pull & un-normalize the i-th image\n",
        "        img = original_images[i].cpu().numpy()          # (C, H, W)\n",
        "        img = img * std_arr + mean_arr                  # broadcast over C,H,W\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        # 2) convert to H×W×C for plotting\n",
        "        img_hwc = np.transpose(img, (1, 2, 0))         # (H, W, C)\n",
        "        H, W, _ = img_hwc.shape\n",
        "\n",
        "        # 3) resize heatmap to match image size\n",
        "        hm = heatmaps[i]                                # (H_cam, W_cam)\n",
        "        hm_resized = cv2.resize(hm, (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # 4) overlay CAM\n",
        "        cam_overlay = show_cam_on_image(img_hwc, hm_resized, use_rgb=True)\n",
        "\n",
        "        # 5) plot side by side\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        ax1.imshow(img_hwc)\n",
        "        ax1.set_title(f\"Original (Label: {true_labels[i]})\")\n",
        "        ax1.axis('off')\n",
        "\n",
        "        ax2.imshow(cam_overlay)\n",
        "        ax2.set_title(\"GradCAM Overlay\")\n",
        "        ax2.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "E0Xburg6jaNB"
      },
      "id": "E0Xburg6jaNB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grad-CAM\n"
      ],
      "metadata": {
        "id": "SdCDPQ7qtopQ"
      },
      "id": "SdCDPQ7qtopQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ----- 1. List of models -----\n",
        "model_list  = [model_densenet, model_enb0, model_swin]\n",
        "model_names = ['DenseNet 121', 'EfficientNet B0']\n",
        "\n",
        "# ----- 2. Function to get last conv layer -----\n",
        "def get_last_conv(model):\n",
        "    last_name, last_conv = None, None\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            last_name, last_conv = name, module\n",
        "    return last_name, last_conv\n",
        "\n",
        "# ----- 3. Create test loader (already prepared) -----\n",
        "test_ds = ChestXrayDataset(test_df, img_dir, transform=val_transforms)\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=16,  # smaller batch for Grad-CAM\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# ----- 4. Loop through models -----\n",
        "for model, name in zip(model_list, model_names):\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # Get last conv layer\n",
        "    layer_name, target_layer = get_last_conv(model)\n",
        "    print(f\"Grad-CAM for {name}\")\n",
        "    print(f\"Target layer: {layer_name}\")\n",
        "\n",
        "    # Initialize Grad-CAM\n",
        "    cam = GradCAM(model=model, target_layers=[target_layer])\n",
        "\n",
        "\n",
        "    heatmaps, orig_images, true_labels = [], [], []\n",
        "\n",
        "    # ----- 5. Loop through test set -----\n",
        "    for X, y in test_loader:\n",
        "        X = X.to(device)\n",
        "        y = y.numpy()\n",
        "\n",
        "        # Forward pass and predictions\n",
        "        with torch.no_grad():\n",
        "            out = model(X)\n",
        "            if out.shape[1] == 2:\n",
        "                probs = torch.softmax(out, dim=1)[:, 1]\n",
        "            else:\n",
        "                probs = torch.sigmoid(out).squeeze(1)\n",
        "\n",
        "        preds_binary = (probs > 0.5).cpu().numpy()\n",
        "\n",
        "        # Keep only correctly predicted samples\n",
        "        correct_mask = (preds_binary == y)\n",
        "        if correct_mask.sum() == 0:\n",
        "            continue  # skip if no correct predictions in batch\n",
        "\n",
        "        X_correct = X[correct_mask]\n",
        "        y_correct = y[correct_mask]\n",
        "\n",
        "        # Compute Grad-CAM\n",
        "        hm_batch = cam(input_tensor=X_correct)\n",
        "        heatmaps.extend(hm_batch)\n",
        "        orig_images.extend(X_correct.cpu())\n",
        "        true_labels.extend(y_correct)\n",
        "\n",
        "    # ----- 6. Convert to arrays -----\n",
        "    orig_images = torch.stack(orig_images).detach().cpu().numpy()\n",
        "    heatmaps    = np.stack(heatmaps, axis=0)\n",
        "    true_labels = np.array(true_labels, dtype=int)\n",
        "\n",
        "    # ----- 7. Visualize -----\n",
        "    num_to_show = min(5, len(orig_images))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std  = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    for i in range(num_to_show):\n",
        "        orig = orig_images[i].transpose(1, 2, 0)  # CHW -> HWC\n",
        "        orig = std * orig + mean\n",
        "        orig = np.clip(orig, 0, 1)\n",
        "\n",
        "        cam_image = show_cam_on_image(orig, heatmaps[i], use_rgb=True)\n",
        "\n",
        "        plt.figure(figsize=(8,4))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.imshow(orig)\n",
        "        plt.title(f\"Original - Label: {true_labels[i]}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.imshow(cam_image)\n",
        "        plt.title(\"Grad-CAM\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "NULLohGeAavw"
      },
      "id": "NULLohGeAavw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Q3.</b> In the previous cell, we created the <code>GradCAM heatmaps</code>. Observe the code and the plots, and interpret how well the model is performing"
      ],
      "metadata": {
        "id": "Ji4fjeBut4wh"
      },
      "id": "Ji4fjeBut4wh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Score CAM"
      ],
      "metadata": {
        "id": "9YIVNTElt-tw"
      },
      "id": "9YIVNTElt-tw"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import ScoreCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# --- 1. Prepare test dataset and loader ---\n",
        "test_ds = ChestXrayDataset(test_df, img_dir, transform=val_transforms)\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=32,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# --- 1. Set device ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- 2. Prepare model list ---\n",
        "model_list  = [model_densenet, model_enb0, model_swin]\n",
        "model_names = ['DenseNet 121', 'EfficientNet B0']\n",
        "\n",
        "# --- 3. Function to get last conv layer ---\n",
        "def get_last_conv(model):\n",
        "    last_name, last_conv = None, None\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            last_name, last_conv = name, module\n",
        "    return last_name, last_conv\n",
        "\n",
        "# --- 4. Loop over models ---\n",
        "for model, name in zip(model_list, model_names):\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # Get target layer\n",
        "    _, target_layer = get_last_conv(model)\n",
        "    print(f\"Using Score-CAM on {name} layer: {target_layer}\")\n",
        "\n",
        "    cam = ScoreCAM(model=model, target_layers=[target_layer])\n",
        "\n",
        "    # Process a few images to save memory\n",
        "    inputs, labels = next(iter(test_loader))\n",
        "\n",
        "    for i in range(min(5, inputs.size(0))):  # one image at a time\n",
        "        input_img = inputs[i:i+1].to(device)  # batch size 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            heatmap = cam(input_tensor=input_img)[0]  # get single heatmap\n",
        "\n",
        "        # Original image for display\n",
        "        orig_img = input_img.cpu()[0].permute(1,2,0).numpy()\n",
        "        orig_img = (orig_img - orig_img.min()) / (orig_img.max() - orig_img.min())  # normalize 0-1\n",
        "\n",
        "        cam_img = show_cam_on_image(orig_img, heatmap, use_rgb=True)\n",
        "\n",
        "        plt.figure(figsize=(8,4))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.imshow(orig_img)\n",
        "        plt.title('Original')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.imshow(cam_img)\n",
        "        plt.title(f'{name} Score-CAM')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Free memory\n",
        "        del input_img, heatmap, cam_img\n",
        "        torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "Xfq3VBESlhCp"
      },
      "id": "Xfq3VBESlhCp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Q4.</b> In the previous cell, we created the <code>ScoreCAM heatmaps</code>. What remarks can you identify with respect to the <code>GradCAM</code>\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dGfNFAofuFIQ"
      },
      "id": "dGfNFAofuFIQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GradCAM++"
      ],
      "metadata": {
        "id": "M0QomLNtuIOh"
      },
      "id": "M0QomLNtuIOh"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAMPlusPlus\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "import torch.nn as nn\n",
        "\n",
        "# --- 1. Set device ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- 2. Move model to device and set eval mode ---\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# --- 3. Function to get last convolutional layer ---\n",
        "def get_last_conv(model):\n",
        "    last_name, last_conv = None, None\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            last_name, last_conv = name, module\n",
        "    return last_name, last_conv\n",
        "\n",
        "# --- 4. Get target layer ---\n",
        "_, target_layer = get_last_conv(model)\n",
        "print(f\"Using Grad-CAM++ on layer: {target_layer}\")\n",
        "\n",
        "# --- 5. Initialize GradCAM++ (no use_cuda argument) ---\n",
        "cam = GradCAMPlusPlus(model=model, target_layers=[target_layer])\n",
        "\n",
        "# --- 6. Parameters ---\n",
        "N = 10  # number of images to visualize\n",
        "mean = [0.5]  # modify according to your normalization\n",
        "std  = [0.2]\n",
        "\n",
        "heatmaps, orig_images, true_labels = [], [], []\n",
        "\n",
        "# --- 7. Process images individually ---\n",
        "processed = 0\n",
        "for inputs, labels in test_loader:\n",
        "    for i in range(inputs.size(0)):\n",
        "        if processed >= N:\n",
        "            break\n",
        "\n",
        "        input_img = inputs[i:i+1].to(device)\n",
        "        label = labels[i].item()\n",
        "\n",
        "        # GradCAM++ requires gradients for input\n",
        "        input_img.requires_grad_()\n",
        "\n",
        "        # Compute GradCAM++\n",
        "        hm = cam(input_tensor=input_img)\n",
        "\n",
        "        # Store results\n",
        "        heatmaps.append(hm[0])\n",
        "        orig_images.append(input_img.detach().cpu()[0])\n",
        "        true_labels.append(label)\n",
        "\n",
        "        del input_img, hm\n",
        "        torch.cuda.empty_cache()\n",
        "        processed += 1\n",
        "    if processed >= N:\n",
        "        break\n",
        "\n",
        "# --- 8. Convert lists to arrays/tensors ---\n",
        "orig_images = torch.stack(orig_images, dim=0)  # (N, C, H, W)\n",
        "heatmaps    = np.stack(heatmaps, axis=0)\n",
        "true_labels = np.array(true_labels, dtype=int)\n",
        "\n",
        "# --- 9. Visualization ---\n",
        "def visualize_gradcam(original_images, heatmaps, true_labels, num_to_show=10, mean=[0.5], std=[0.2]):\n",
        "    for i in range(num_to_show):\n",
        "        # Detach & permute\n",
        "        orig_img = original_images[i].permute(1,2,0).numpy()\n",
        "        orig_img = (orig_img * std[0]) + mean[0]\n",
        "        orig_img = np.clip(orig_img, 0, 1)\n",
        "\n",
        "        heatmap = heatmaps[i]\n",
        "\n",
        "        plt.figure(figsize=(8,4))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.imshow(orig_img)\n",
        "        plt.title(f'Original - Label: {true_labels[i]}')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        cam_img = show_cam_on_image(orig_img, heatmap, use_rgb=True)\n",
        "        plt.imshow(cam_img)\n",
        "        plt.title('Grad-CAM++')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# --- 10. Visualize ---\n",
        "visualize_gradcam(\n",
        "    original_images=orig_images,\n",
        "    heatmaps=heatmaps,\n",
        "    true_labels=true_labels,\n",
        "    num_to_show=N,\n",
        "    mean=mean,\n",
        "    std=std\n",
        ")\n"
      ],
      "metadata": {
        "id": "gmPmNgsspxqZ"
      },
      "id": "gmPmNgsspxqZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Q5.</b> In the previous cell, we created the <code>GradCAM++ heatmaps</code>. Can you spot the lesions in the image under the predicted class activation maps? Where do the difficulties arise?"
      ],
      "metadata": {
        "id": "MZV_c1LPuO4Y"
      },
      "id": "MZV_c1LPuO4Y"
    },
    {
      "cell_type": "markdown",
      "id": "rXOMzIexIo2L",
      "metadata": {
        "id": "rXOMzIexIo2L"
      },
      "source": [
        "# Conclusion and Final Thoughts\n",
        "\n",
        "Congratulations on completing this comprehensive, hands-on journey through medical image classification!\n",
        "\n",
        "Over the course of this notebook, you have successfully built, trained, and evaluated deep learning models from start to finish. You have gone beyond mere training—you have engaged in critical practices that transform a simple experiment into a robust, interpretable AI system.\n",
        "\n",
        "## Key Achievements\n",
        "\n",
        "- You successfully handled and prepared a real-world medical imaging dataset, including preprocessing and normalization.\n",
        "- You built custom PyTorch `Dataset` and `DataLoader` pipelines tailored for classification tasks.\n",
        "- You trained state-of-the-art models (DenseNet, EfficientNet, Swin Transformer) and evaluated them on unseen test data.\n",
        "- You implemented advanced evaluation metrics beyond simple accuracy, including precision, recall, F1-score, and AUROC.\n",
        "- You applied explainable AI techniques such as Grad-CAM, Grad-CAM++, and Score-CAM to interpret model predictions and identify the regions of the X-rays that influenced each decision.\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- **Data is foundational:** A thorough understanding of your dataset, including class balance and preprocessing, is essential for successful model training.  \n",
        "- **Evaluation is multi-dimensional:** Relying on a single metric is insufficient. Metrics like AUROC, F1-score, and visual inspection of correctly and incorrectly classified images give a complete picture of model performance.  \n",
        "- **Interpretability builds trust:** In high-stakes domains like medicine, it’s crucial to understand why models make their decisions. Techniques like Grad-CAM help clinicians trust AI-assisted predictions.\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "This notebook provides a strong foundation for further exploration. Here are some directions to extend your work:\n",
        "\n",
        "1. **Include Negative Samples:** Incorporate non-diseased cases to create a more comprehensive diagnostic model capable of both detection and classification.  \n",
        "2. **Experiment with Architectures:** Try alternative architectures or ensemble approaches to improve classification performance.  \n",
        "3. **Hyperparameter Optimization:** Systematically tune learning rates, batch sizes, and data augmentation strategies to improve metrics like AUROC and F1-score.  \n",
        "4. **3D Medical Imaging:** Extend these methods to 3D datasets (CT or MRI scans) to handle more complex structures and richer diagnostic information.  \n",
        "5. **Model Explainability:** Explore LIME or integrated gradients alongside Grad-CAM to provide multi-faceted interpretability for clinical decision support.\n",
        "\n",
        "By following these practices, you have not only trained accurate models but also built a framework for safe, interpretable, and clinically useful AI in medical imaging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 60 minutes"
      ],
      "metadata": {
        "id": "B5IdJmBHK0M6"
      },
      "id": "B5IdJmBHK0M6"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}