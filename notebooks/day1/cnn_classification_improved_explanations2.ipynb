{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "T6BeY23EjU0L"
   },
   "source": [
    "# Summer School on Biomedical Imaging with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "cgm9viPTP_dN"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/albarqounilab/BILD-Summer-School/blob/main/notebooks/day1/cnn_classification_improved_explanations2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "xY9NKCq8cO48"
   },
   "source": [
    "![alt_text](https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/helpers/notebook-banner.png)\n",
    "\n",
    "BILD 2025 is organized under the umbrella of the [Strategic Arab-German Network for Affordable and Democratized AI in Healthcare (SANAD)](https://albarqouni.github.io/funded/sanad/), uniting academic excellence and technological innovation across borders. This year’s edition is organized by the [Albarqouni Lab](https://albarqouni.github.io/) at the [University Hospital Bonn](https://www.ukbonn.de/) and the [University of Bonn](https://www.uni-bonn.de/en). We are proud to partner with leading institutions in the region—Lebanese American University, University of Tunis El Manar, and Duhok Polytechnic University — to deliver a truly international learning experience. Over five intensive days in Tunis, you will explore cutting-edge deep-learning techniques for medical imaging through expert lectures, hands-on labs, and collaborative case studies. Engage with peers and faculty from Germany, Lebanon, Iraq, and Tunisia as you develop practical skills in building and deploying AI models for real-world healthcare challenges. We look forward to an inspiring week of interdisciplinary exchange and the shared commitment to advancing affordable, life-saving AI in medicine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "ea5603fa-4f2d-483d-ba04-4aeed8ddf7c6"
   },
   "source": [
    "## Chest-X-Ray Classification [78 mins]\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This notebook will guide you step-by-step through practical exercises on using Convolutional Neural Networks (CNNs) for classifying and detecting diseases in chest X-ray images. If you are new to machine learning, PyTorch, or Python, don't worry—each section will explain what is happening and why it is important.\n",
    "\n",
    "We will use two real-world chest X-ray datasets to learn how to:\n",
    "- Recognize (classify) if a chest X-ray shows signs of disease or is healthy.\n",
    "- Find (detect) the location of disease in the image using bounding boxes.\n",
    "\n",
    "**Why do we do this?**\n",
    "- Medical images like X-rays are used by doctors to diagnose diseases. Automating this process with AI can help doctors make faster and more accurate decisions.\n",
    "\n",
    "**The datasets:**\n",
    "- **NIH ChestX-ray14:** Over 100,000 X-ray images labeled with 14 different diseases. Some images also have boxes showing where the disease is located.\n",
    "- **RSNA Pneumonia Detection Challenge:** About 30,000 X-rays with expert-drawn boxes around pneumonia, perfect for learning detection.\n",
    "\n",
    "**What will you learn?**\n",
    "1. **How to work with medical image datasets.**\n",
    "   - Loading images and reading their labels (what disease, if any, is present).\n",
    "   - Understanding the structure of the data and why we split it into training, validation, and test sets.\n",
    "2. **How to build and train a model to classify images.**\n",
    "   - Using powerful pre-trained models and adapting them to our problem.\n",
    "   - Measuring how well our model is doing and how to improve it.\n",
    "3. **How to detect disease locations in images.**\n",
    "   - Using models that can draw boxes around areas of interest.\n",
    "   - Evaluating how accurate these detections are.\n",
    "4. **How to report and interpret results.**\n",
    "   - Understanding key metrics like accuracy and ROC-AUC.\n",
    "   - Visualizing results to see what the model got right and wrong.\n",
    "\n",
    "By the end of this notebook, you will have hands-on experience with the full process of building, training, and evaluating deep learning models for medical image analysis, even if you are just starting out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "a6d9ccb1-08d3-4673-b513-39b15963f31c"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The [NIH ChestX-ray-14](https://nihcc.app.box.com/v/ChestXray-NIHCC) dataset is a large collection of chest X-ray images. Each image comes with information about the patient and labels that tell us which diseases (if any) are present. This dataset is widely used in medical AI research because it helps us train and test models to recognize diseases from X-ray images.\n",
    "\n",
    "**What does the dataset contain?**\n",
    "1. Over 100,000 chest X-ray images, each in PNG format. These are pictures of the inside of the chest, showing the lungs and heart.\n",
    "2. A metadata file (`Data_Entry_2017.csv`) that lists information about each image, such as:\n",
    "   - Which diseases are present (if any)\n",
    "   - Patient age and gender\n",
    "   - How the image was taken\n",
    "3. A file with bounding boxes (`BBox_List_2017.csv`) for about 1,000 images. These boxes show exactly where a disease is located in the image.\n",
    "4. Files that split the data into training and test sets. This is important because we want to train our model on some images and test it on others to see how well it works on new data.\n",
    "\n",
    "**Why do we use this dataset?**\n",
    "- It is large and diverse, which helps our model learn better.\n",
    "- It has real medical labels, making our project more realistic.\n",
    "- It allows us to practice both classification (is there a disease?) and detection (where is the disease?).\n",
    "\n",
    "In this notebook, we will use a smaller sample of this dataset and pre-trained models to make the exercises faster and easier to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install / update the huggingface hub CLI\n",
    "!pip install -q \"huggingface_hub[cli]\"\n",
    "\n",
    "# Download the dataset files via CLI\n",
    "!hf download albarqouni/bild-dataset --repo-type dataset \\\n",
    "    --include \"Classification/csv.zip\" \\\n",
    "    --local-dir ./\n",
    "\n",
    "!hf download albarqouni/bild-dataset --repo-type dataset \\\n",
    "    --include \"Classification/data_cxr8.zip\" \\\n",
    "    --local-dir ./\n",
    "\n",
    "# Unzip the downloaded files\n",
    "!unzip -q ./Classification/csv.zip -d ./Classification\n",
    "!unzip -q ./Classification/data_cxr8.zip -d ./Classification\n",
    "\n",
    "print(\"Download and extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "SkIFYQFwSdwm"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "oFnf-I9rRx6C"
   },
   "source": [
    "## Import essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!pip install pydicom -q\n",
    "import pydicom\n",
    "\n",
    "!pip install SimpleITK -q\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from glob import glob\n",
    "import time\n",
    "import cv2\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import tv_tensors\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pydicom # Added import for pydicom\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "!pip install torchmetrics -q\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "from huggingface_hub import hf_hub_download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "d7f6e4ce-4517-4a2a-80b5-dd002d282bd8"
   },
   "source": [
    "### Download\n",
    "\n",
    "Before we can work with the data, we need to download and unzip it. This means we are copying the files from the internet to our computer and making them ready to use.\n",
    "\n",
    "**Why do we do this?**\n",
    "- Machine learning models need data to learn from. Downloading the dataset gives us the images and labels we need for our project.\n",
    "- Unzipping extracts the files from a compressed format so we can access them easily in our code.\n",
    "\n",
    "**Instructions:**\n",
    "- If you have not downloaded the dataset yet, run the following cells to download and unzip the files.\n",
    "- If you already have the data, you can skip these steps by adding a `#` before the `!` in the code (this comments out the line so it won't run).\n",
    "- You can also change the `DATA_PATH` variable if you want to store the data in a different folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install / update huggingface hub CLI\n",
    "!pip install -q \"huggingface_hub[cli]\"\n",
    "\n",
    "# Download pretrained model weights via CLI\n",
    "!hf download albarqouni/bild-dataset --repo-type dataset \\\n",
    "    --include \"Classification/densenet121-classification.pth\" \\\n",
    "    --local-dir ./\n",
    "\n",
    "!hf download albarqouni/bild-dataset --repo-type dataset \\\n",
    "    --include \"Classification/efficientnet-classification.pth\" \\\n",
    "    --local-dir ./\n",
    "\n",
    "!hf download albarqouni/bild-dataset --repo-type dataset \\\n",
    "    --include \"Classification/swintransformer-classification.pth\" \\\n",
    "    --local-dir ./\n",
    "\n",
    "print(\"Model weights downloaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "DkfLApQltnLE"
   },
   "source": [
    "### Load patient splits\n",
    "\n",
    "To train and evaluate our model properly, we need to split our data into different groups:\n",
    "- **Training set:** Used to teach the model.\n",
    "- **Validation set:** Used to check how well the model is learning during training.\n",
    "- **Test set:** Used to see how well the model works on completely new data.\n",
    "\n",
    "In this step, we load lists of which images belong to each group. This helps us make sure that the model is tested on images it has never seen before, which is important for getting a fair measure of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=\"./Classification\"\n",
    "train_val_patients = pd.read_csv(f'{DATA_PATH}/train_val_list.txt', header=None, names=['patientId'])\n",
    "test_patients = pd.read_csv(f'{DATA_PATH}/test_list.txt', header=None, names=['patientId'])\n",
    "\n",
    "print(f\"Number of patients in train/val set: {len(train_val_patients)}\")\n",
    "print(f\"Number of patients in test set: {len(test_patients)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "E5vsy6EFi4pj"
   },
   "source": [
    "The `.txt` files contain lists of image names that belong to the training/validation or test sets. To use these splits, we need to match the image names in these files with the information in our main database (`metadata.csv`). This way, we know which images and labels go into each group for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "oPha5-43tr6X"
   },
   "source": [
    "### Load dataframe metadata\n",
    "\n",
    "A **dataframe** is a table of data, like a spreadsheet, that we can easily work with in Python using the pandas library. Here, we load the metadata for all our images. This metadata tells us important information about each image, such as which diseases are present, the patient ID, and more. Loading this information helps us organize and prepare our data for training and testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and observe available data\n",
    "metadata_df = pd.read_csv(f'{DATA_PATH}/metadata.csv')\n",
    "metadata_df#.head() # Print the 5 fist rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "DP7dICXbtzei"
   },
   "source": [
    "Now we need to make sure that the information in our dataframe matches the images we actually downloaded. This step filters out any entries in the metadata that do not have a corresponding image file, so we only work with images that are available on our computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = glob(f'{DATA_PATH}/images/*')\n",
    "imgs_basename = [os.path.basename(i) for i in imgs]\n",
    "\n",
    "metadata_df = metadata_df.loc[metadata_df['Image Index'].isin(imgs_basename)]\n",
    "metadata_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "bJ6kj9ArLWPg"
   },
   "source": [
    "### Handle targets\n",
    "\n",
    "In machine learning, a **target** is what we want the model to predict. For this project, the target is the disease label for each image. In this step, we prepare the target labels so that our model can learn to predict them. This may involve simplifying the labels or grouping them in a way that makes the problem easier to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "tS7yltl-MNbv"
   },
   "source": [
    "In the next step, we look at how many times each disease label appears in our data. Some diseases are very rare, which can make it hard for the model to learn about them. To keep things simple and make sure our model has enough examples to learn from, we will remove labels that appear less than 1,500 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = metadata_df['Finding Labels'].value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "tqIz4i7rt-LV"
   },
   "source": [
    "We remove rare labels (diseases that appear in fewer than 1,500 images) so that our model has enough examples to learn from. This helps the model focus on the most common diseases and improves its ability to make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "YK7xNjgKb85a"
   },
   "source": [
    "After filtering out rare labels, we are left with the most common disease categories. The table below shows how many images belong to each label. This helps us understand the balance of our dataset and which diseases our model will learn to recognize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "eBW_rxY2uOhB"
   },
   "source": [
    "First, we look at how many images there are for each disease label. This helps us see if some diseases are much more common than others, which can affect how well our model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = metadata_df['Finding Labels'].value_counts()\n",
    "rare_labels = label_counts[label_counts < 1500].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "fo_nf5u0uaUy"
   },
   "source": [
    "Now we update our data table (DataFrame) to remove any images with rare disease labels. This makes sure our model only sees images with the most common labels, which helps it learn better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df_filtered = metadata_df[~metadata_df['Finding Labels'].isin(rare_labels)].copy()\n",
    "\n",
    "print(f\"Original shape: {metadata_df.shape}\")\n",
    "print(f\"Filtered shape: {metadata_df_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df_filtered['Finding Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "jz9itQUZj4C8"
   },
   "source": [
    "To make our task easier, we will turn the problem into a **binary classification** problem. This means the model will learn to answer a simple question: Is this X-ray healthy or does it show signs of disease?\n",
    "\n",
    "- **Class 0 (Negative):** Images labeled as 'No Finding' (healthy)\n",
    "- **Class 1 (Positive):** Images with any disease label (pathology present)\n",
    "\n",
    "This approach is common in deep learning when starting out, because it is easier for the model to learn to distinguish between just two categories. The category we want the model to predict is called the **target class**. Here, you can also try focusing on a specific disease (like 'Effusion') or experiment with more classes to see how the model behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = {\n",
    "    'No Finding', 'Effusion',\n",
    "}\n",
    "\n",
    "# split each cell into a list, then keep rows where at least one element is in `keep`\n",
    "df_filtered = metadata_df_filtered[\n",
    "    metadata_df_filtered['Finding Labels']\n",
    "      .str.split('|')                         # or .str.split(',') if comma‑separated\n",
    "      .apply(lambda labels: any(lbl in keep for lbl in labels))\n",
    "].copy()\n",
    "df_filtered['Finding Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "rFWbq74kvBvB"
   },
   "source": [
    "Now we create a new column called `Binary Label` in our data. This column will have a value of 0 for healthy images and 1 for images with any disease. This process is called **label encoding** and is very common in deep learning, because models work best with numbers instead of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['Binary Label'] = (df_filtered['Finding Labels'] != 'No Finding').astype(int)\n",
    "df_filtered['Binary Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "MSbPWf5hvEgO"
   },
   "source": [
    "We can further clean our dataset by selecting only one **view acquisition** type for our classifier. 'View acquisition' refers to the way the X-ray image was taken (for example, from the front or the side). Using only one type (like 'PA' for posteroanterior) helps the model learn more consistently, because all images will look similar in terms of orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[df_filtered[\"View Position\"] == 'PA']\n",
    "df_filtered['View Position'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "5Bssc5DHnWkm"
   },
   "source": [
    "Now we use the lists of patient IDs to split our data into a **training set** (used to teach the model) and a **test set** (used to check how well the model works on new, unseen data). This is called a **train-test split** and is a key step in building reliable machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df_filtered based on patient IDs from the loaded lists\n",
    "train_val_df = df_filtered[df_filtered['Image Index'].isin(train_val_patients['patientId'])].copy()\n",
    "test_df = df_filtered[df_filtered['Image Index'].isin(test_patients['patientId'])].copy()\n",
    "\n",
    "print(f\"Train val shape: {train_val_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "id": "F9Vt4KBlxGOW"
   },
   "source": [
    "### Deep Learning Data Terminology\n",
    "\n",
    "- **Batch:** A batch is a small group of samples processed together by the model before updating its parameters. Using batches makes training faster and more stable.\n",
    "- **Epoch:** One epoch means the model has seen all the training data once. Training usually takes many epochs.\n",
    "- **DataLoader:** In PyTorch, a DataLoader helps us load data in batches, shuffle it, and use multiple CPU cores to speed up the process. This is essential for efficient deep learning training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_val_df.copy()\n",
    "pos = df[df['Binary Label'] == 1]\n",
    "neg = df[df['Binary Label'] == 0]\n",
    "\n",
    "# # sample up to x each\n",
    "n_samples = 3000\n",
    "pos = pos.sample(n=min(len(pos), n_samples), random_state=42)\n",
    "neg = neg.sample(n=min(len(neg), n_samples), random_state=42)\n",
    "\n",
    "subset = pd.concat([pos, neg]).reset_index(drop=True)\n",
    "print(\"Subset size:\", subset.shape)\n",
    "print(subset['Binary Label'].value_counts())\n",
    "\n",
    "# %%\n",
    "train_validation_df, test_df = train_test_split(\n",
    "    subset,\n",
    "    test_size=0.2,\n",
    "    stratify=subset['Binary Label'],\n",
    "    random_state=42\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_validation_df,\n",
    "    test_size=0.1,\n",
    "    stratify=train_validation_df['Binary Label'],\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Train:\", train_df.shape, \"Validation:\", val_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "BCJnQBP1wE03"
   },
   "source": [
    "### Download the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "id": "f6ius3AZvwsR"
   },
   "source": [
    "\n",
    "This week, we delve into the power of deep models like CNNs, leveraging the PyTorch library as our framework. PyTorch provides the flexibility and tools necessary to explore and implement these complex architectures for challenging tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "id": "fUgU5u8OwvAF"
   },
   "source": [
    "### Datasets in PyTorch\n",
    "Next we define our custom `ChestXrayDataset` using torch `Dataset` from `torch.utils.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row['Image Index'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(row['Binary Label'], dtype=torch.float32)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "id": "OKhmhPbJw4L0"
   },
   "source": [
    "**Transforms** are changes we make to images as we load them. This can include resizing, flipping, rotating, or normalizing the images. When we do these changes randomly during training, it is called **data augmentation**. Data augmentation helps the model learn to recognize patterns in different situations, making it more robust and less likely to memorize the training data (a problem called overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "image_size_= 224\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size_,image_size_)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "xM4XoBfDzQub"
   },
   "source": [
    "Now the `val_transforms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transforms   = transforms.Compose([\n",
    "    transforms.Resize((image_size_,image_size_)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "id": "4xyIR_3MyqDg"
   },
   "source": [
    "### Dataloaders\n",
    "\n",
    "A **DataLoader** is a tool in PyTorch that helps us load data in small groups called **mini-batches**. Instead of giving the model one image at a time, we give it a batch of images. This makes training faster and helps the model learn more stable patterns. Dataloaders also make it easy to shuffle the data and use multiple CPU cores for loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir  = f'{DATA_PATH}/images'\n",
    "\n",
    "train_ds = ChestXrayDataset(train_df, img_dir, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=90, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "eaZ_U7XzzkGP"
   },
   "source": [
    "We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 32, *i.e.* each element in the dataloader iterable will return a batch of 32 features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in train_loader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds   = ChestXrayDataset(val_df,   img_dir, transform=val_transforms)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, num_workers=32, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {
    "id": "WO8Sp67b0h_n"
   },
   "source": [
    "## Loading pretrained models with PyTorch\n",
    "\n",
    "A **pretrained model** is a model that has already been trained on a large dataset (like ImageNet) and has learned useful features. The structure of the model is called its **architecture** (for example, DenseNet, ResNet, EfficientNet). Using a pretrained model and adapting it to our own data is called **transfer learning**. This is very helpful because it allows us to get good results even with smaller datasets and less training time.\n",
    "In 'torchvision.models' we can find many popular pretrained models and architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.models.list_models()[::30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "id": "a0cc7bbb"
   },
   "source": [
    "#### Understanding Model Layers\n",
    "\n",
    "When looking at a deep learning model, you will see several types of layers. Here is what to look for in each:\n",
    "\n",
    "- **Convolutional layers:** These are the building blocks of most image models. They scan the input image with small filters (sliding windows) to detect patterns like edges, shapes, or textures. The first convolutional layer takes the raw image (with 1 channel for grayscale or 3 for RGB) and produces feature maps.\n",
    "- **Normalization layers (BatchNorm):** These layers help the model train faster and more reliably by keeping the outputs of previous layers at a similar scale. Batch Normalization (BatchNorm) is the most common type. It makes training more stable and helps the model generalize better.\n",
    "- **Pooling layers:** Pooling reduces the size of the feature maps, making the model faster and helping it focus on the most important features. The most common is Max Pooling, which keeps only the largest value in each region.\n",
    "- **Activation functions:** After each convolution, the model uses an activation function (like ReLU) to introduce non-linearity. This helps the model learn complex patterns, not just straight lines.\n",
    "\n",
    "- **First layer:** This is usually a convolutional layer that takes the input image. Check its input dimension (number of channels, usually 1 for grayscale or 3 for RGB images).\n",
    "- **Second layer:** Often another convolutional, normalization, activation, or pooling layer, building on the features from the first.\n",
    "- **Second to last layer:** This is typically a feature layer just before the classifier. Its output dimension shows the number of features passed to the final classifier.\n",
    "- **Last layer:** This is the classifier or output layer. Its output dimension should match the number of classes (1 for binary classification).\n",
    "\n",
    "By examining these layers, you can understand how the model processes the input and what features are used for the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.densenet121(pretrained=True)\n",
    "model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print only the first and last two layer blocks\n",
    "layers = list(model.children())\n",
    "print('First layer block:')\n",
    "print(layers[0])\n",
    "print('\\n---')\n",
    "print('Second layer block:')\n",
    "print(layers[1])\n",
    "print('\\n...')\n",
    "print('Second to last layer block:')\n",
    "print(layers[-2])\n",
    "print('\\n---')\n",
    "print('Last layer block:')\n",
    "print(layers[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {
    "id": "qaEdGcOx0y9Q"
   },
   "source": [
    "### Hyperparameters\n",
    "\n",
    "**Hyperparameters** are settings that you choose before training your model. They control how the learning process works. Common hyperparameters include:\n",
    "- **Number of epochs:** How many times the model sees the whole training set.\n",
    "- **Batch size:** How many samples are in each batch.\n",
    "- **Learning rate:** How big the steps are when updating the model's weights.\n",
    "\n",
    "Tuning hyperparameters is important because it can make a big difference in how well your model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {
    "id": "UTXB_k_Y08_f"
   },
   "source": [
    "### Optimization loop\n",
    "\n",
    "Training a deep learning model involves an **optimization loop**. Each time the model sees the whole training set, it completes one **epoch**. The process has two main parts:\n",
    "- **Train loop:** The model learns from the training data and updates its parameters.\n",
    "- **Validation loop:** The model is tested on validation data to see how well it is learning.\n",
    "\n",
    "A **loss function** measures how far the model's predictions are from the true answers. The goal of training is to minimize this loss. The optimization loop repeats for many epochs until the model performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {
    "id": "AFH4boS51SCH"
   },
   "source": [
    "Inside the training loop, the model learns by adjusting its parameters using **gradients**. Gradients show how much each parameter should change to reduce the loss. The process of calculating gradients and updating parameters is called **backpropagation**.\n",
    "\n",
    "- **optimizer.zero_grad():** Resets the gradients to zero before each batch.\n",
    "- **loss.backward():** Calculates the gradients using backpropagation.\n",
    "- **optimizer.step():** Updates the model's parameters using the gradients.\n",
    "- **Learning rate scheduler (like OneCycleLR):** Adjusts the learning rate during training to help the model learn better and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=learning_rate, epochs=epochs, steps_per_epoch=len(train_loader), total_steps=epochs * len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_frac = train_df['Binary Label'].mean()\n",
    "pos_weight = torch.tensor([(1 - pos_frac) / pos_frac]).to(device)\n",
    "\n",
    "criterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {
    "id": "Rlu-3UTk4j87"
   },
   "source": [
    "Next, we define our **training function** and **validation function**. The training function teaches the model using the training data, while the validation function checks how well the model is doing on data it hasn't seen before. Keeping these functions separate helps us monitor the model's progress and avoid overfitting (when the model memorizes the training data but doesn't generalize well to new data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels in tqdm(loader, desc=\"  Training\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs).squeeze(1)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        try:\n",
    "            scheduler.step()\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def val_loop(model, loader, criterion, auroc, device):\n",
    "    model.eval()\n",
    "    auroc.reset()\n",
    "    running_preds = []\n",
    "    running_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, desc=\"  Validation\", leave=False):\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs).squeeze(1)\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            preds = (probs > 0.5).int().cpu().numpy()\n",
    "            running_preds.extend(preds.tolist())\n",
    "            running_labels.extend(labels.int().tolist())\n",
    "\n",
    "            auroc.update(probs, labels.int().to(device))\n",
    "\n",
    "    acc = accuracy_score(running_labels, running_preds)\n",
    "    val_auroc = auroc.compute().item()\n",
    "    return acc, val_auroc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_dir  = f'{DATA_PATH}/images'\n",
    "\n",
    "train_ds = ChestXrayDataset(train_df, img_dir, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=90, pin_memory=True)\n",
    "\n",
    "val_ds   = ChestXrayDataset(val_df,   img_dir, transform=val_transforms)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, num_workers=32, pin_memory=True)\n",
    "\n",
    "auroc = BinaryAUROC().to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    train_loss = train_loop(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "    val_acc, val_auroc = val_loop(model, val_loader, criterion, auroc, device)\n",
    "\n",
    "    print(f\"  Train Loss: {train_loss:.4f}  |  Val Acc: {val_acc:.4f}  |  Val AUROC: {val_auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "del model   # delete model object\n",
    "del optimizer\n",
    "del scheduler\n",
    "del train_loader, val_loader, train_ds, val_ds\n",
    "torch.cuda.empty_cache()  # clears unused cached memory\n",
    "gc.collect()  # run garbage collector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {
    "id": "G2gzcB-Z5_Xi"
   },
   "source": [
    "### Benchmarking model architectures\n",
    "\n",
    "A **CNN architecture** is the specific design or structure of a convolutional neural network. Different architectures (like ResNet, DenseNet, EfficientNet, Swin Transformer) use different building blocks:\n",
    "- **Skip connections:** Allow information to skip layers, helping very deep networks learn better (used in ResNet).\n",
    "- **Dense connections:** Connect each layer to every other layer in a block, improving information flow (used in DenseNet).\n",
    "- **Normalization layers:** Help stabilize and speed up training by keeping the data flowing through the network at a similar scale.\n",
    "\n",
    "Trying different architectures is important because some may work better for your specific problem. In this section, you will train and compare several architectures to see which performs best on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {
    "id": "LPuX8sCi6FvW"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Q3.</b> In deep learning, different **model architectures** can have a big impact on performance. Complete the following cells to train and compare these models:\n",
    "    - EfficientNet\n",
    "    - Swin Transformer\n",
    "</div>\n",
    "\n",
    "Comparing different models helps you understand which design works best for your specific task and data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {
    "id": "E_0Pwmcd97n8"
   },
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_enb0 = ... # COMPLETE\n",
    "model_enb0.classifier[1] = nn.Linear(1280, 1)\n",
    "\n",
    "model = ... # COMPLETE to device\n",
    "\n",
    "learning_rate = ... # COMPLETE\n",
    "batch_size = ... # COMPLETE\n",
    "epochs = ... # COMPLETE\n",
    "\n",
    "pos_frac = train_df['Binary Label'].mean()\n",
    "pos_weight = torch.tensor([(1 - pos_frac) / pos_frac]).to(device)\n",
    "criterion  = ... # COMPLETE\n",
    "\n",
    "optimizer = ... # COMPLETE\n",
    "scheduler = OneCycleLR(optimizer, max_lr=learning_rate, epochs=epochs, steps_per_epoch=len(train_loader), total_steps=epochs * len(train_loader)) # Explicitly set total_steps\n",
    "\n",
    "img_dir  = f'{DATA_PATH}/images'\n",
    "\n",
    "train_ds = ChestXrayDataset(train_df, img_dir, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=90, pin_memory=True)\n",
    "\n",
    "val_ds   = ChestXrayDataset(val_df,   img_dir, transform=val_transforms)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, num_workers=32, pin_memory=True)\n",
    "\n",
    "auroc = BinaryAUROC().to(device)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    train_loss = train_loop(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "    val_acc, val_auroc = val_loop(model, val_loader, criterion, auroc, device)\n",
    "\n",
    "    print(f\"  Train Loss: {train_loss:.4f}  |  Val Acc: {val_acc:.4f}  |  Val AUROC: {val_auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# After finishing training a model\n",
    "del model   # delete model object\n",
    "del optimizer\n",
    "del scheduler\n",
    "del train_loader, val_loader, train_ds, val_ds\n",
    "torch.cuda.empty_cache()  # clears unused cached memory\n",
    "gc.collect()  # run garbage collector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {
    "id": "P2PotfWt9_ef"
   },
   "source": [
    "### SwinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import swin_t, Swin_T_Weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_swin = ... # COMPLETE\n",
    "model_swin.head = nn.Linear(in_features=768, out_features=1, bias=True)\n",
    "model = ... # COMPLETE to device\n",
    "print(model)\n",
    "\n",
    "learning_rate = ... # COMPLETE\n",
    "batch_size = ... # COMPLETE\n",
    "epochs = ... # COMPLETE\n",
    "\n",
    "pos_frac = train_df['Binary Label'].mean()\n",
    "pos_weight = torch.tensor([(1 - pos_frac) / pos_frac]).to(device)\n",
    "criterion  = ... # COMPLETE\n",
    "\n",
    "optimizer = ... # COMPLETE\n",
    "scheduler = OneCycleLR(optimizer, max_lr=learning_rate, epochs=epochs, steps_per_epoch=len(train_loader), total_steps=epochs * len(train_loader)) # Explicitly set total_steps\n",
    "\n",
    "img_dir  = f'{DATA_PATH}/images'\n",
    "\n",
    "train_ds = ChestXrayDataset(train_df, img_dir, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=90, pin_memory=True)\n",
    "\n",
    "val_ds   = ChestXrayDataset(val_df,   img_dir, transform=val_transforms)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, num_workers=32, pin_memory=True)\n",
    "\n",
    "auroc = BinaryAUROC().to(device)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    train_loss = train_loop(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "    val_acc, val_auroc = val_loop(model, val_loader, criterion, auroc, device)\n",
    "\n",
    "    print(f\"  Train Loss: {train_loss:.4f}  |  Val Acc: {val_acc:.4f}  |  Val AUROC: {val_auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# After finishing training a model\n",
    "del model   # delete model object\n",
    "del optimizer\n",
    "del scheduler\n",
    "del train_loader, val_loader, train_ds, val_ds\n",
    "torch.cuda.empty_cache()  # clears unused cached memory\n",
    "gc.collect()  # run garbage collector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {
    "id": "2iEXO2Z46XIu"
   },
   "source": [
    "#### Metrics\n",
    "\n",
    "After training, we need to measure how well our models perform. In deep learning, we use different **metrics** to evaluate models:\n",
    "- **Accuracy:** The percentage of correct predictions.\n",
    "- **Precision:** Answers the question: \"Of all the pixels the model labeled as tumor, what fraction were actually tumor?\" High precision means the model makes few false positive errors. Clinically, this translates to not raising false alarms or suggesting unnecessary biopsies. (TP/(TP+FP))\n",
    "- **Recall:** Answers the question: \"Of all the pixels that were actually tumor, what fraction did the model correctly identify?\" High recall means the model makes few false negative errors. This is often critically important in medicine, as it relates to not missing a diagnosis. (TP/(TP+FN))\n",
    "- **F1-score:** Answers the question: “How well does the model balance precision and recall?” It is the harmonic mean of the two, ensuring that a model must perform well on both dimensions rather than excelling in only one.\n",
    "- **ROC curve:** A plot that shows how well the model separates healthy from diseased images at different thresholds.\n",
    "\n",
    "Using multiple metrics gives a more complete picture of model performance, especially when the data is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {
    "id": "3q_sCE9k6azN"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Q4.</b> Compare the performance of the different models using these metrics:\n",
    "    - Plot the **ROC curve**\n",
    "    - Accuracy score\n",
    "    - Precision and Recall\n",
    "    - F1-score\n",
    "</div>\n",
    "\n",
    "Comparing models with these metrics helps you choose the best one for your task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {
    "id": "wNVpLAryCLmx"
   },
   "source": [
    "Load the saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_densenet = ... # COMPLETE\n",
    "model_densenet.classifier = ... # COMPLETE\n",
    "\n",
    "model_enb0 = ... # COMPLETE\n",
    "model_enb0.classifier[1] = ... # COMPLETE\n",
    "\n",
    "model_swin = ... # COMPLETE\n",
    "model_swin.head = ... # COMPLETE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_densenet.load_state_dict(torch.load('...' # COMPLETE))\n",
    "model_enb0.load_state_dict(torch.load('...' # COMPLETE))\n",
    "model_swin.load_state_dict(torch.load('...' # COMPLETE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {
    "id": "cBlyZIx2HKEI"
   },
   "source": [
    "Now lets compute the metrics and plot all models: (8mins)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds     = ChestXrayDataset(test_df, img_dir, transform=val_transforms)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=32,\n",
    "    num_workers=32,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model_list  = [model_densenet, model_enb0, model_swin]\n",
    "model_names = ['DenseNet 121', 'EfficientNet B0', 'Swin Transformer']\n",
    "\n",
    "all_preds_proba = {}\n",
    "all_labels      = None\n",
    "\n",
    "for model, name in zip(model_list, model_names):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    probs_list, labels_list = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X = X.to(device)\n",
    "            out = model(X)\n",
    "            probs = torch.sigmoid(out)\n",
    "            probs = probs.squeeze(1)\n",
    "            probs_list.extend(probs.cpu().numpy())\n",
    "            labels_list.extend(y.numpy())\n",
    "\n",
    "    preds_proba = np.array(probs_list)\n",
    "    labels      = np.array(labels_list)\n",
    "\n",
    "    all_preds_proba[name] = preds_proba\n",
    "    if all_labels is None:\n",
    "        all_labels = labels\n",
    "\n",
    "    preds_binary = (preds_proba > 0.5).astype(int)\n",
    "\n",
    "    acc     = ... # COMPLETE\n",
    "    prec    = ... # COMPLETE\n",
    "    rec     = ... # COMPLETE\n",
    "    f1      = ... # COMPLETE\n",
    "    roc_auc = ... # COMPLETE)\n",
    "\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"  Accuracy : {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall   : {rec:.4f}\")\n",
    "    print(f\"  F1-score : {f1:.4f}\")\n",
    "    print(f\"  ROC AUC  : {roc_auc:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "... # COMPLETE WITH ROC Curve display, and other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {
    "id": "rXOMzIexIo2L"
   },
   "source": [
    "### Summary: Key Deep Learning Terms\n",
    "\n",
    "- **Dataset:** The collection of images and labels we use to train and test our model.\n",
    "- **Label:** The answer we want the model to predict (e.g., healthy or diseased).\n",
    "- **Model architecture:** The design or structure of the neural network.\n",
    "- **Training:** Teaching the model using known data.\n",
    "- **Validation:** Checking the model's performance during training.\n",
    "- **Test set:** Data the model has never seen, used to measure final performance.\n",
    "- **Batch/Epoch:** Groups of data and full passes through the dataset.\n",
    "- **Loss function:** Measures how wrong the model's predictions are.\n",
    "- **Optimizer:** Algorithm that updates the model's parameters to reduce loss.\n",
    "- **Metric:** A way to measure how well the model is doing.\n",
    "\n",
    "Reflect on these terms as you work through the notebook—they are the foundation of deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {
    "id": "igNcHyGckBTg"
   },
   "source": [
    "# Advanced Topic: Quality Control  \n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {
    "id": "Zfop5zPjkEks"
   },
   "source": [
    "In medical AI, accuracy alone is not enough. A reliable system must also quantify its uncertainty. We will now explore Quality Control techniques to ensure predictions are interpretable and safe.\n",
    "\n",
    "For chest-X-ray **classification**, we’ll use **Grad-CAM**, **Score-CAM**, **Grad-CAM++ and LIME** to turn model predictions into **heatmaps** that highlight the image regions driving each decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {
    "id": "bvZYzemSkL7v"
   },
   "source": [
    "###  Grad-CAM (Gradient-weighted Class Activation Mapping)\n",
    "- Uses the **gradients of the target class** flowing into the last convolutional layer.  \n",
    "- Produces **coarse heatmaps** that highlight the most important regions influencing the prediction.  \n",
    "- Helps verify whether the model is focusing on the lungs rather than irrelevant areas.  \n",
    "\n",
    "---\n",
    "###  Grad-CAM++\n",
    "- An **improvement over Grad-CAM**.  \n",
    "- Better at handling **multiple occurrences** of the same pathology (e.g., lesions in different lung areas).  \n",
    "\n",
    "---\n",
    "### Score-CAM\n",
    "- Does **not rely on gradients**.  \n",
    "- Uses the **model’s confidence scores** to generate explanations.  \n",
    "- Avoids problems like noisy or vanishing gradients.  \n",
    "\n",
    "---\n",
    "\n",
    "### LIME (Local Interpretable Model-agnostic Explanations)\n",
    "\n",
    "- Perturbs the input image (e.g., hides or alters superpixels) and observes how predictions change.  \n",
    "- Produces **superpixel-based explanations**, showing which regions increase or decrease the prediction probability.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/jacobgil/pytorch-grad-cam.git\n",
    "!pip install lime -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {
    "id": "kbTBiWVShoEp"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_conv(model):\n",
    "    last_name, last_conv = None, None\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            last_name, last_conv = name, module\n",
    "    return last_name, last_conv\n",
    "\n",
    "name, target_layer = get_last_conv(model_densenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {
    "id": "IfViKsEOeBCi"
   },
   "source": [
    "Implement GradCAM, ScoreCAM, and GradCAM++ from the pytorch_grad_cam library on the test_loader using the loaded model and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds     = ChestXrayDataset(test_df, img_dir, transform=val_transforms)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=32,\n",
    "    num_workers=32,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradcam(\n",
    "    original_images: torch.Tensor,  # (N, C, H, W)\n",
    "    heatmaps:         np.ndarray,    # (N, H_cam, W_cam)\n",
    "    true_labels:      np.ndarray,    # (N,)\n",
    "    num_to_show:      int = 5,\n",
    "    mean:             float|list|tuple = 0.5,\n",
    "    std:              float|list|tuple = 0.2,\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    - original_images: Tensor(N, C, H, W), normalized via (x-mean)/std\n",
    "    - heatmaps:        ndarray(N, H_cam, W_cam) in [0,1]\n",
    "    - true_labels:     ndarray(N,)\n",
    "    \"\"\"\n",
    "    N = min(num_to_show, original_images.shape[0])\n",
    "\n",
    "    # prepare mean/std arrays for un-normalization\n",
    "    if isinstance(mean, (list, tuple, np.ndarray)):\n",
    "        mean_arr = np.array(mean)[:, None, None]\n",
    "        std_arr  = np.array(std)[:,  None, None]\n",
    "    else:\n",
    "        mean_arr = mean\n",
    "        std_arr  = std\n",
    "\n",
    "    for i in range(N):\n",
    "        # 1) pull & un-normalize the i-th image\n",
    "        img = original_images[i].cpu().numpy()          # (C, H, W)\n",
    "        img = img * std_arr + mean_arr                  # broadcast over C,H,W\n",
    "        img = np.clip(img, 0, 1)\n",
    "\n",
    "        # 2) convert to H×W×C for plotting\n",
    "        img_hwc = np.transpose(img, (1, 2, 0))         # (H, W, C)\n",
    "        H, W, _ = img_hwc.shape\n",
    "\n",
    "        # 3) resize heatmap to match image size\n",
    "        hm = heatmaps[i]                                # (H_cam, W_cam)\n",
    "        hm_resized = cv2.resize(hm, (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # 4) overlay CAM\n",
    "        cam_overlay = show_cam_on_image(img_hwc, hm_resized, use_rgb=True)\n",
    "\n",
    "        # 5) plot side by side\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax1.imshow(img_hwc)\n",
    "        ax1.set_title(f\"Original (Label: {true_labels[i]})\")\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2.imshow(cam_overlay)\n",
    "        ax2.set_title(\"GradCAM Overlay\")\n",
    "        ax2.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {
    "id": "tk3-vW4YhrIv"
   },
   "source": [
    "### Grad-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {
    "id": "y7VNpzHwX-PK"
   },
   "source": [
    "#### Deep Dive: How Grad-CAM Works\n",
    "\n",
    "Grad-CAM creates its heatmap by combining two key pieces of information from the model:\n",
    "\n",
    "- **Feature Maps from a Convolutional Layer**: Deep inside the network, convolutional layers produce feature maps that highlight abstract patterns like textures, edges, and shapes. The final convolutional layers capture the most high-level, class-specific information.\n",
    "\n",
    "- **Gradients**: It calculates the gradient (the importance signal) of the model's final prediction score with respect to each feature map. A high gradient for a particular feature map means that map was very influential in the final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {
    "id": "Ba7YCLFIusYz"
   },
   "source": [
    "Visualize the original images and the generated heatmaps from GradCAM, ScoreCAM, and GradCAM++ for a subset of the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {
    "id": "vBFRJKmp8-sy"
   },
   "source": [
    "GradCAM computes heatmaps using gradients of the target class with respect to the feature maps of the last convolutional layer.\n",
    "\n",
    "Produces a heatmap that highlights regions that most strongly influence the model’s prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Delete variables that occupy RAM\n",
    "del orig_images, heatmaps, true_labels, hm_batch, inputs, labels\n",
    "gc.collect()\n",
    "\n",
    "# If using GPU, clear CUDA cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {
    "id": "Ml-A5iQ7uf7f"
   },
   "source": [
    "### Score CAM [6mins]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {
    "id": "8gKXUhvMYLsv"
   },
   "source": [
    "#### Deep Dive: How Score-CAM Works\n",
    "\n",
    "Score-CAM avoids using gradients and instead relies on forward passes to assess importance:\n",
    "\n",
    "- **Perturbation-Based Activation**: Each activation map from the target convolutional layer is normalized and used as a mask on the input image.\n",
    "\n",
    "- **Forward Pass Evaluation**: The masked image is passed through the model, and the change in the target class score determines the importance of that activation map.\n",
    "\n",
    "- **Heatmap Construction**: The final heatmap is a weighted combination of activation maps, highlighting the regions that most influence the model’s prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_densenet.to(device).eval()\n",
    "cam = ScoreCAM(model=model_densenet, target_layers=[target_layer])\n",
    "\n",
    "orig_images_sc = []\n",
    "heatmaps_sc    = []\n",
    "true_labels    = []\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    hm_batch = cam(input_tensor=inputs)\n",
    "    heatmaps_sc.extend(hm_batch)\n",
    "\n",
    "    orig_images_sc.extend(inputs.cpu())\n",
    "    true_labels.extend(labels.numpy())\n",
    "\n",
    "orig_images_sc = torch.stack(orig_images_sc, dim=0)\n",
    "heatmaps_sc    = np.stack(heatmaps_sc, axis=0)\n",
    "true_labels    = np.array(true_labels, dtype=int)\n",
    "\n",
    "N = 10\n",
    "visualize_gradcam(\n",
    "    original_images=orig_images_sc,\n",
    "    heatmaps=heatmaps_sc,\n",
    "    true_labels=true_labels,\n",
    "    num_to_show=N,\n",
    "    mean=[0.5],\n",
    "    std =[0.2],\n",
    "    title=\"ScoreCAM Overlay\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {
    "id": "20453Kti9HlM"
   },
   "source": [
    "ScoreCAM masks the feature maps of the target layer one by one, runs them through the network, and measures the increase in predicted score for the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete large objects\n",
    "del orig_images_sc\n",
    "del heatmaps_sc\n",
    "del true_labels\n",
    "del inputs, hm_batch\n",
    "# Empty the cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Run garbage collection to free up Python memory\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {
    "id": "lfQR8m9r9OEK"
   },
   "source": [
    "### GradCAM++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {
    "id": "j31K6LpkYbMP"
   },
   "source": [
    "#### Deep Dive: How Grad-CAM++ Works\n",
    "\n",
    "Grad-CAM++ builds on Grad-CAM but improves the localization of small or multiple objects:\n",
    "\n",
    "- **Weighted Combination of Feature Maps**: Instead of a simple global average of gradients, Grad-CAM++ uses a weighted sum that accounts for pixel-wise contributions.\n",
    "\n",
    "- **Better Fine-Grained Localization**: This allows the heatmap to more precisely highlight smaller regions that are critical to the prediction, especially when multiple objects or details matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAMPlusPlus\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_densenet.to(device).eval()\n",
    "cam = GradCAMPlusPlus(model=model_densenet, target_layers=[target_layer])\n",
    "\n",
    "heatmaps    = []\n",
    "orig_images = []\n",
    "true_labels = []\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    hm_batch = cam(input_tensor=inputs)\n",
    "    heatmaps.extend(hm_batch)\n",
    "\n",
    "    orig_images.extend(inputs.cpu())\n",
    "    true_labels.extend(labels.numpy())\n",
    "\n",
    "orig_images = torch.stack(orig_images, dim=0)\n",
    "heatmaps    = np.stack(heatmaps,    axis=0)\n",
    "true_labels = np.array(true_labels, dtype=int)\n",
    "\n",
    "visualize_gradcam(\n",
    "    original_images=orig_images,\n",
    "    heatmaps=heatmaps,\n",
    "    true_labels=true_labels,\n",
    "    num_to_show=N,\n",
    "    mean=mean,\n",
    "    std=std\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {
    "id": "-6cIlDqy-Opw"
   },
   "source": [
    "GradCAMPlusPlus is an improved version of Grad-CAM.\n",
    "\n",
    "It better captures multiple important regions in an image (not just the strongest gradient).\n",
    "\n",
    "Especially useful when:\n",
    "\n",
    "- An image has multiple objects of the same class.\n",
    "\n",
    "- You want finer-grained localization than standard Grad-CAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete large objects\n",
    "del orig_images\n",
    "del heatmaps\n",
    "del true_labels\n",
    "del inputs, hm_batch\n",
    "# Empty the cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Run garbage collection to free up Python memory\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {
    "id": "iDebDqYu_j4c"
   },
   "source": [
    "### LIME [5mins]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {
    "id": "KrhgViu5YlVU"
   },
   "source": [
    "#### Deep Dive: How LIME Works\n",
    "\n",
    "LIME provides explanations by approximating the model locally with interpretable models:\n",
    "\n",
    "- **Superpixel Segmentation**: The input image is divided into superpixels—small contiguous regions of similar color or texture.\n",
    "\n",
    "- **Perturbation and Prediction**: LIME creates many perturbed versions of the image by hiding or altering superpixels and records the model’s predictions on these variants.\n",
    "\n",
    "- **Linear Approximation**: It fits a simple interpretable model on the perturbed dataset to determine which superpixels most strongly influence the prediction.\n",
    "\n",
    "- **Explanation**: The highlighted superpixels are those that contributed most to the model’s decision for the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title helpers\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (a) model → probability bridge for LIME\n",
    "def batch_predict(np_imgs: list[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Accepts a list of H×W×C RGB images in [0,255] (dtype uint8).\n",
    "    Returns an (N, 2) array of class probabilities for LIME.\n",
    "    \"\"\"\n",
    "    model_densenet.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = torch.stack(\n",
    "            [\n",
    "                val_transforms(                     # same transforms you used at eval time\n",
    "                    to_pil_image(img.astype(np.uint8))\n",
    "                )\n",
    "                for img in np_imgs\n",
    "            ],\n",
    "            dim=0,\n",
    "        ).to(device)\n",
    "\n",
    "        logits = model_densenet(batch)              # (N, 1)\n",
    "        probs_pos = torch.sigmoid(logits)  # (N, 1)  – P(class = 1)\n",
    "        probs_neg = 1 - probs_pos          # (N, 1)  – P(class = 0)\n",
    "        probs = torch.cat([probs_neg, probs_pos], dim=1)  # (N, 2)\n",
    "\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# (b) tensor → uint8 numpy (unnormalised) for LIME visualisation\n",
    "def tensor_to_uint8(img_tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    img_tensor: (3, H, W) – normalised\n",
    "    Returns  H×W×3 uint8 image in RGB.\n",
    "    \"\"\"\n",
    "    img = img_tensor.cpu().clone().numpy()\n",
    "    img = img * np.array(std)[:, None, None] + np.array(mean)[:, None, None]\n",
    "    img = np.clip(img, 0, 1)\n",
    "    img = (np.transpose(img, (1, 2, 0)) * 255).astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {
    "id": "hMuT2FDCAb7M"
   },
   "source": [
    "Instantiate LIME explainer LimeImageExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer(random_state=42)\n",
    "\n",
    "num_samples_to_show = 5      # how many images you want to inspect\n",
    "lime_samples        = 1000   # neighbourhood size – higher = slower but smoother\n",
    "top_labels          = [1]    # we care about the “positive” class (index 1)\n",
    "\n",
    "for i in range(num_samples_to_show):\n",
    "    img_tensor, true_label = test_ds[i]\n",
    "    img_uint8              = tensor_to_uint8(img_tensor)\n",
    "\n",
    "    # LIME explanation\n",
    "    explanation = explainer.explain_instance(\n",
    "        image=img_uint8,\n",
    "        classifier_fn=batch_predict,\n",
    "        labels=top_labels,\n",
    "        hide_color=0,\n",
    "        num_samples=lime_samples\n",
    "    )\n",
    "\n",
    "    lime_img, lime_mask = explanation.get_image_and_mask(\n",
    "        label=1,\n",
    "        positive_only=False,\n",
    "        hide_rest=False,\n",
    "        num_features=8,\n",
    "        min_weight=0.0\n",
    "    )\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    ax1.imshow(img_uint8)\n",
    "    ax1.set_title(f\"Original  –  label = {int(true_label)}\")\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2.imshow(mark_boundaries(lime_img / 255.0, lime_mask))\n",
    "    ax2.set_title(\"LIME explanation\")\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {
    "id": "dmeznhgyJOrJ"
   },
   "source": [
    "LIME divides the image into superpixels (small contiguous regions of similar color/texture).\n",
    "\n",
    "It then perturbs the image by hiding or altering these superpixels and observes how the model’s prediction changes.\n",
    "\n",
    "The highlighted superpixels are those that most strongly influenced the model’s prediction for the class you’re explaining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {
    "id": "ZSUyFNA1KKSi"
   },
   "source": [
    "### Conclusion\n",
    "By applying multiple quality control methods, we can better understand what drives the model’s predictions:\n",
    "\n",
    "- **GradCAM / GradCAM++ / ScoreCAM**: Highlight the regions of the image that contributed most to the model’s decision. These methods provide a coarse-to-fine visualization of attention, showing where the model “looks” when predicting a class. GradCAM++ tends to emphasize smaller, more precise areas compared to GradCAM. ScoreCAM avoids gradient computation and often produces smoother, less noisy heatmaps.\n",
    "\n",
    "- **LIME**: Highlights superpixels that most strongly influence the prediction by perturbing them and observing changes in output. Unlike CAM-based methods, LIME works in a model-agnostic way and can provide complementary explanations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
