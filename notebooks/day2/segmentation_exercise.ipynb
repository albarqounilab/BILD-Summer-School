{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bef0e6e1",
   "metadata": {
    "id": "bef0e6e1"
   },
   "source": [
    "\n",
    "# Image Segmentation with Deep Learning\n",
    "\n",
    "Welcome to the **BILD 2025 Summer School** hands-on session on medical image segmentation!  \n",
    "This notebook will guide you through training, evaluating, and **comparing deep learning models** for brain tumor segmentation.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/albarqounilab/BILD-Summer-School/blob/main/notebooks/day2/segmentation_exercise.ipynb)\n",
    "![banner](https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/helpers/notebook-banner.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Today's Goals\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- **Understand & Prepare Medical Imaging Data:** Load and process MRI scans into a deep learning pipeline.\n",
    "- **Train a Segmentation Model:** Fit a U-Net baseline to identify tumor regions.\n",
    "- **Master Segmentation Metrics:** Use and interpret **Dice** and **IoU** during evaluation.\n",
    "- **Compare Architectures:** Benchmark losses/variants and understand trade-offs.\n",
    "- **Explore Foundation Models:** Run **MedSAM** (prompt-based) and compare to U-Net.\n",
    "\n",
    "**Objectives:** You'll see how AI can delineate structures (like your manual contouring) but faster and consistently. You'll also apply core DL skills to a high-impact real-world task.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use the [The Brain Resection Multimodal Imaging Database](https://www.cancerimagingarchive.net/collection/remind/) [1]. The Brain Resection Multimodal Imaging Database (ReMIND) contains pre- and intra-operative brain MRI collected on 114 consecutive patients who were surgically treated with image-guided tumor resection between 2018 and 2022.\n",
    "\n",
    "[1] Juvekar, P., Dorent, R., Kögl, F., Torio, E., Barr, C., Rigolo, L., Galvin, C., Jowkar, N., Kazi, A., Haouchine, N., Cheema, H., Navab, N., Pieper, S., Wells, W. M., Bi, W. L., Golby, A., Frisken, S., & Kapur, T. (2023). The Brain Resection Multimodal Imaging Database (ReMIND) (Version 1) [dataset]. The Cancer Imaging Archive. [https://doi.org/10.7937/3RAG-D070](https://doi.org/10.7937/3RAG-D070)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1188c1d7",
   "metadata": {
    "id": "1188c1d7"
   },
   "source": [
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "We install and import required libraries. Run this once per new environment.\n",
    "\n",
    "> **Note:** The cell will install packages (internet required). If you're offline, skip installation and ensure the environment already has the packages.\n",
    "\n",
    "### Import libraries (3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tbtu63oG0dDI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbtu63oG0dDI",
    "outputId": "c6b8d5ca-c145-4be9-ef3e-b45bd0775a0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m36.7/52.6 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m^C\n"
     ]
    }
   ],
   "source": [
    "# Install the required libraries for our notebook\n",
    "# segmentation-models-pytorch is a great library with pre-built segmentation model architectures.\n",
    "# albumentations is a powerful library for data augmentation.\n",
    "!pip install -U segmentation-models-pytorch albumentations SimpleITK -q\n",
    "\n",
    "# Library Imports\n",
    "# Standard libraries\n",
    "import os\n",
    "# This helps manage GPU allocation and suppresses common warnings for a cleaner notebook.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Use the first available GPU\n",
    "import sys\n",
    "import warnings\n",
    "from glob import glob\n",
    "import math\n",
    "import time\n",
    "import gc # Garbage collector to clean RAM\n",
    "\n",
    "# Data handling and processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# Image processing and visualization\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Deep Learning with PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "# Environment Configuration\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# This ensures that SimpleITK does not print excessive warnings.\n",
    "sitk.ProcessObject.SetGlobalWarningDisplay(False)\n",
    "\n",
    "# Setup reproducibility\n",
    "import random\n",
    "RANDOM_SEED   = 42\n",
    "def set_seed(seed=RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries imported and environment configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3351e7",
   "metadata": {
    "id": "7a3351e7"
   },
   "source": [
    "\n",
    "## 2. The Dataset: Brain Tumor MRIs (ReMIND-style)\n",
    "\n",
    "We’ll use a **ReMIND** dataset structure with MRI volumes and tumor masks.  \n",
    "If downloads fail or data aren’t present, we’ll provide a **synthetic fallback** so the notebook always runs.\n",
    "\n",
    "### 2.1 Downloading the Data (2 minutes)\n",
    "> ~1.8GB; may take a few minutes. If you're have the local data, skip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qo4-R_31cENd",
   "metadata": {
    "id": "Qo4-R_31cENd"
   },
   "outputs": [],
   "source": [
    "# !pip -q install -U \"huggingface_hub[cli]\" -q\n",
    "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Segmentation/data.zip\" --local-dir ./\n",
    "# !hf download albarqouni/bild-dataset --repo-type dataset --include \"Segmentation/fcn_segmentation020_dice0.5188.pt\" --local-dir ./\n",
    "!unzip -q ./Segmentation/data.zip -d ./\n",
    "\n",
    "# If you want to download the trained model checkpoints (uncomment) | For low resource environments\n",
    "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Segmentation/checkpoints.zip\" --local-dir ./\n",
    "!unzip Segmentation/checkpoints.zip -d ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c0395",
   "metadata": {
    "id": "e77c0395"
   },
   "source": [
    "\n",
    "Understanding the Data Structure\n",
    "\n",
    "We expect:\n",
    "```\n",
    "data/ReMIND/\n",
    "  sub-001/\n",
    "    anat/T1w*                           # image volume (DICOM folder)\n",
    "    seg/ReMIND-001-preop-SEG-tumor-*.nrrd  # binary mask\n",
    "  sub-002/...\n",
    "  ReMIND_metadata.csv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MKRqB4MY2Nin",
   "metadata": {
    "id": "MKRqB4MY2Nin"
   },
   "source": [
    "### 2.2. Data Handling Utilities\n",
    "\n",
    "These helper functions form the backbone of our data loading pipeline. They are designed to be robust and handle the common complexities of medical imaging data formats and the crucial need for spatial alignment.\n",
    "\n",
    "-   **`load_dicom_series_to_3d_image(path)`**: Medical scans like MRIs are often stored as a folder containing hundreds of 2D DICOM files, one for each slice. This function uses SimpleITK's powerful `ImageSeriesReader` to intelligently read all slices in a given folder and stack them into a single, coherent 3D volume.\n",
    "\n",
    "-   **`reconstruct_mask_in_image_space(mask, reference)`**: This is arguably the most critical function for data integrity. A segmentation mask and its corresponding image might have different origins, orientations, or spacing. This function uses resampling to transform the `mask` so that it perfectly aligns with the physical coordinate system of the `reference` image. It uses **Nearest Neighbor interpolation** to ensure that the discrete mask labels (e.g., 0 for background, 1 for tumor) are preserved without creating new, intermediate values (like 0.5).\n",
    "\n",
    "-   **`load_volume_any(path)`**: A versatile wrapper that abstracts away the file format. It checks if the provided path is a directory (and assumes it's a DICOM series) or a single file (like a NIfTI `.nii` or NRRD `.nrrd`) and uses the correct loading method. It returns the 3D image data as a NumPy array and the physical spacing of the pixels.\n",
    "\n",
    "-   **`normalize_zero_one(array)`**: This function handles contrast normalization. Instead of a simple min-max scaling which is sensitive to extreme outliers (e.g., imaging artifacts), it uses the **1st and 99th percentiles** as the floor and ceiling. This makes the normalization more robust and typically produces better visual contrast for deep learning models.\n",
    "\n",
    "-   **`discover_patients(path)`**: This utility acts like a web scraper for your file system. It automatically scans a data directory, looks for folders named `sub-*`, and within each, verifies the existence of both an image file (`anat/T1w*`) and a corresponding segmentation file (`seg/ReMIND*.nrrd`). It returns a list of all patient cases that have valid data pairs, which we can then use to build our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09b7f1",
   "metadata": {
    "id": "8d09b7f1"
   },
   "outputs": [],
   "source": [
    "# Utility functions for I/O and alignment\n",
    "\n",
    "def load_dicom_series_to_3d_image(p): return sitk.ReadImage(sitk.ImageSeriesReader().GetGDCMSeriesFileNames(p))\n",
    "def reconstruct_mask_in_image_space(m, r): return sitk.Resample(m, r, sitk.Transform(), sitk.sitkNearestNeighbor, 0, m.GetPixelID())\n",
    "def load_volume_any(p): return load_dicom_series_to_3d_image(p) if os.path.isdir(p) else sitk.ReadImage(p)\n",
    "\n",
    "def normalize_zero_one(x):\n",
    "    a, b = np.percentile(x, 1), np.percentile(x, 99)\n",
    "    return np.clip((x - a) / (b - a + 1e-6), 0.0, 1.0)\n",
    "\n",
    "\n",
    "def discover_patients(p):\n",
    "    pats = []\n",
    "    for sub in sorted(glob(os.path.join(p, \"sub-*\"))):\n",
    "        idx = os.path.basename(sub).split(\"-\")[-1]\n",
    "        imgs = glob(os.path.join(sub, \"anat\", \"T1w*\"))\n",
    "        segs = glob(os.path.join(sub, \"seg\", f\"ReMIND-{idx}-preop-SEG-tumor-*.nrrd\"))\n",
    "        if imgs and segs: pats.append({'id': idx, 'img_path': imgs[0], 'seg_path': segs[0]})\n",
    "    return pats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb2879",
   "metadata": {
    "id": "c5cb2879"
   },
   "source": [
    "\n",
    "### 2.3 Load Clinical Metadata & Clean\n",
    "\n",
    "Real-world datasets are rarely perfect. We begin by loading the clinical metadata CSV file, which contains information about each patient case, such as the tumor's `Histopathology`. We then perform two key cleaning steps to ensure our dataset is robust and well-behaved for training a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027fa82",
   "metadata": {
    "id": "1027fa82"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/ReMIND\"\n",
    "clinical_df = pd.read_csv(f\"{DATA_PATH}/ReMIND_metadata.csv\")\n",
    "clinical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IWpRA2Fc4PFT",
   "metadata": {
    "id": "IWpRA2Fc4PFT"
   },
   "outputs": [],
   "source": [
    "# First, let's inspect the distribution of tumor types\n",
    "print(\"Original Histopathology Distribution:\\n\")\n",
    "print(clinical_df[\"Histopathology\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m2qP--Q04ZkO",
   "metadata": {
    "id": "m2qP--Q04ZkO"
   },
   "outputs": [],
   "source": [
    "# Data Cleaning Step 1: Group Rare Classes\n",
    "hist_counts = clinical_df[\"Histopathology\"].value_counts()\n",
    "# Identify all tumor types that appear 2 or fewer times\n",
    "to_other = hist_counts[hist_counts <= 2].index\n",
    "clinical_df[\"Histopathology\"] = clinical_df[\"Histopathology\"].replace(to_other, \"Other\")\n",
    "print(\"\\nCleaned Histopathology Distribution:\\n\")\n",
    "print(clinical_df[\"Histopathology\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88sYodPk4vvj",
   "metadata": {
    "id": "88sYodPk4vvj"
   },
   "source": [
    "#### Why do we group infrequent histopathology types into 'Other'?\n",
    "\n",
    "When we first inspect the distribution of tumor types using `value_counts()`, we can see that some categories, like \"Atypical Meningioma\" or \"Metastatic carcinoma\", may have only one or two examples in the entire dataset. This severe class imbalance poses several significant problems for training a reliable machine learning model:\n",
    "\n",
    "1.  **Impossible Stratification**: When we split our data into training, validation, and test sets, we use stratification to ensure each set has a similar distribution of classes. If a class has only one sample, it's impossible to distribute it across all three sets. This can lead to sets where certain tumor types are completely absent, making it impossible to train or evaluate the model's performance on them.\n",
    "\n",
    "2.  **Inability to Learn Generalizable Patterns**: A deep learning model cannot learn the general visual characteristics of a \"Metastasis\" tumor from a single example. Instead of learning what makes a metastasis look like a metastasis, it will simply memorize the specific features of that one unique image.\n",
    "\n",
    "3.  **Risk of Overfitting**: The model will likely overfit to the few examples of rare classes it sees, leading to poor performance when it encounters a new, unseen example of that same class in the real world.\n",
    "\n",
    "By grouping these extremely rare instances into a single, more substantial **'Other'** category, we create a more balanced and statistically stable dataset. This pragmatic step ensures that our data splits are meaningful and that the model can learn from groups that are large enough to contain generalizable patterns. It is a common and necessary practice in applied machine learning to handle the long-tail distributions often found in real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IdNNaG6FFx1y",
   "metadata": {
    "id": "IdNNaG6FFx1y"
   },
   "source": [
    "#### Data Cleaning Step: Exclude Problematic Cases\n",
    "\n",
    "Before training any model, it is crucial to perform **visual quality control** on the dataset. While automated scripts can load and process thousands of images, they cannot replace the expert human eye in identifying subtle yet critical issues.\n",
    "\n",
    "**\"Garbage In, Garbage Out\"**: This is a fundamental principle in machine learning. If we train our model on poor-quality or incorrectly labeled data, it will learn incorrect patterns, and its predictions will be unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50xlW2ypK33m",
   "metadata": {
    "cellView": "form",
    "id": "50xlW2ypK33m"
   },
   "outputs": [],
   "source": [
    "#@title Plot patients\n",
    "def plot_n_patients(clinical_df, n=5):\n",
    "    plotted = 0\n",
    "    for _, row in clinical_df.iterrows():\n",
    "        if plotted >= n:\n",
    "            break\n",
    "\n",
    "        patient   = row['Case Number']\n",
    "        hist_type = row['Histopathology']\n",
    "        idx = str(patient).zfill(3)\n",
    "        img_dir  = glob(f\"{DATA_PATH}/sub-{idx}/anat/T1w*\")\n",
    "        seg_path = glob(f\"{DATA_PATH}/sub-{idx}/seg/ReMIND-{idx}-preop-SEG-tumor-*.nrrd\")\n",
    "\n",
    "        if not img_dir or not seg_path:\n",
    "            print(f\"Data not found for patient {idx}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        img_sitk = load_dicom_series_to_3d_image(img_dir[0])\n",
    "        img_np   = sitk.GetArrayFromImage(img_sitk)\n",
    "\n",
    "        mask_sitk = sitk.ReadImage(seg_path[0])\n",
    "        mask_sitk = reconstruct_mask_in_image_space(mask_sitk, img_sitk)\n",
    "        mask_np   = sitk.GetArrayFromImage(mask_sitk).astype(np.uint8)\n",
    "\n",
    "        slices_with_tumor = [i for i, mask_slice in enumerate(mask_np) if mask_slice.any()]\n",
    "        if not slices_with_tumor:\n",
    "            print(f\"No tumor slices found for patient {idx}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        slice_to_plot_idx = slices_with_tumor[len(slices_with_tumor) // 2]\n",
    "        img_slice  = img_np[slice_to_plot_idx].astype(np.float32)\n",
    "        mask_slice = mask_np[slice_to_plot_idx]\n",
    "\n",
    "        plot_img_t  = A.Compose([A.Resize(512, 512, interpolation=0), A.Normalize(0.0, 1.0), ToTensorV2()])(image=img_slice)[\"image\"].float()\n",
    "        plot_mask_t = A.Compose([A.Resize(512, 512, interpolation=0), ToTensorV2(transpose_mask=True)])(image=img_slice, mask=mask_slice)[\"mask\"]\n",
    "\n",
    "        img_arr  = plot_img_t.squeeze(0).numpy()\n",
    "        mask_arr = plot_mask_t.squeeze().numpy()\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        ax[0].imshow(img_arr, cmap=\"gray\"); ax[0].set_title(f\"{hist_type} - Patient {idx} - Original Image\"); ax[0].axis(\"off\")\n",
    "        ax[1].imshow(mask_arr, cmap=\"gray\"); ax[1].set_title(f\"{hist_type} - Patient {idx} - Mask\"); ax[1].axis(\"off\")\n",
    "        ax[2].imshow(img_arr, cmap=\"gray\")\n",
    "        ax[2].imshow(np.ma.masked_where(mask_arr != 1, mask_arr), cmap=\"Reds\", alpha=0.6, vmin=0, vmax=1)\n",
    "        ax[2].contour(mask_arr, levels=[0.5], colors=\"red\", linewidths=1)\n",
    "        ax[2].set_title(f\"{hist_type} - Patient {idx} - Overlay\"); ax[2].axis(\"off\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "        plotted += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ef38f",
   "metadata": {
    "id": "a31ef38f"
   },
   "outputs": [],
   "source": [
    "print(\"Plotting all examples with tumors:\")\n",
    "number_patients=5 #@param {type: \"number\"}\n",
    "plot_n_patients(clinical_df, n=number_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nvm4RSwmL5Sy",
   "metadata": {
    "id": "nvm4RSwmL5Sy"
   },
   "source": [
    "\n",
    "Here we will predefined list `excluded_patients` is used to remove specific cases from our dataset. This list is not arbitrary; it is the result of a careful, manual review process where a domain expert (like a radiologist or medical imaging scientist) inspected the data and identified cases with problems such as:\n",
    "\n",
    "-   **Severe Imaging Artifacts**: Issues like motion artifacts, signal dropout, or aliasing can corrupt the image, making it impossible for a model (or even a human) to make a confident assessment.\n",
    "-   **Incorrect or Ambiguous Annotations**: The ground truth segmentation mask might be misaligned, inaccurate, or the boundary of the pathology might be inherently ambiguous.\n",
    "-   **Atypical Anatomy or Co-occurring Pathologies**: Sometimes a case might be an extreme outlier that would confuse the model more than it would help it learn a general pattern.\n",
    "\n",
    "By explicitly removing these known problematic cases, we improve the overall quality of our training data. This leads to a more robust model that learns from clean, representative examples, ultimately resulting in better and more trustworthy performance on new, unseen data. This curation step is a hallmark of high-quality, professional machine learning projects, especially in high-stakes fields like medicine.\n",
    "\n",
    "```\n",
    "excluded_patients = [7, 16, 4, ..., 114]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oes6ILZ9FvyN",
   "metadata": {
    "id": "oes6ILZ9FvyN"
   },
   "outputs": [],
   "source": [
    "excluded_patients = [7, 16, 4, 19, 22, 23, 27, 35, 41, 43, 47, 60, 76, 92, 95, 114]\n",
    "clinical_df = clinical_df[~clinical_df[\"Case Number\"].isin(excluded_patients)].reset_index(drop=True)\n",
    "print(f\"Clinical metadata loaded and cleaned. Final patient count: {len(clinical_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0c140",
   "metadata": {
    "id": "55b0c140"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>A Critical Note on Data Exclusion:</b>\n",
    "The exclusion of patient data is a significant decision that must be handled with extreme care, especially in clinical applications. The rationale for removing any case should be transparent and based on well-defined clinical or technical criteria (e.g., severe imaging artifacts, confirmed annotation errors).\n",
    "\n",
    "Warning: Modifying the dataset by excluding samples directly impacts the integrity of your results. Excluded data must never be used in the final test set. Furthermore, altering the standard public dataset can make direct comparisons to state-of-the-art benchmarks invalid. Always document and justify any exclusions thoroughly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d486076",
   "metadata": {
    "id": "9d486076"
   },
   "source": [
    "\n",
    "### 2.4 The `SegmentationDataset` Class\n",
    "\n",
    "This class is the heart of our data pipeline, acting as the crucial bridge between the raw medical imaging files on our disk and the structured format PyTorch requires for training.\n",
    "\n",
    "Think of this class as a meticulous digital assistant. Its main job is to go through every patient's 3D MRI scan, find every single 2D slice that contains a piece of the tumor, and prepare it for the model to learn from. This ensures we focus only on the relevant information.\n",
    "\n",
    "This class inherits from PyTorch's `torch.utils.data.Dataset`. Its key responsibilities are:\n",
    "1.  **In `__init__`**: It scans the data directory, indexes all valid tumor slices across all specified patients, and sets up our data augmentation pipeline using the powerful [Albumentations](https://albumentations.ai/docs/) library.\n",
    "2.  **In `__getitem__`**: It retrieves a single image-mask pair by its index and applies the necessary transformations (like resizing, flipping, and converting to a tensor) on the fly.\n",
    "\n",
    "\n",
    "\n",
    "> Data Augmentation with Albumentations: We use the albumentations library because it is incredibly fast and can transform both an image and its corresponding mask simultaneously, ensuring that geometric changes (like rotations or flips) are applied identically to both. This is crucial for segmentation tasks.\n",
    "\n",
    "> In a nutshell: The Dataset class is a flexible blueprint for telling PyTorch how to access and prepare each individual piece of your data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kvNb3Qgg-_aQ",
   "metadata": {
    "id": "kvNb3Qgg-_aQ"
   },
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, patient_df: pd.DataFrame, data_path: str, size: int = 512, augment: bool = True):\n",
    "        self.data_path, self.size, self.slices = data_path, size, []\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(size, size, interpolation=1),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.6),\n",
    "            A.Normalize(mean=0.0, std=1.0), ToTensorV2(),\n",
    "        ]) if augment else A.Compose([\n",
    "            A.Resize(size, size, interpolation=1), A.Normalize(mean=0.0, std=1.0), ToTensorV2(),\n",
    "        ])\n",
    "        start_time = time.time()\n",
    "        patients = discover_patients(data_path)\n",
    "        valid_ids = set(patient_df[\"Case Number\"].astype(str).str.zfill(3))\n",
    "        patients_to_process = [p for p in patients if p['id'] in valid_ids]\n",
    "        # print(\"Filtered patients without available mask:\",len(clinical_df) - len(patients_to_process))\n",
    "        print(f\"Indexing {len(patients_to_process)} patients...\")\n",
    "        for pat in tqdm(patients_to_process, desc=\"Processing Patients\"):\n",
    "          ref_img = load_volume_any(pat['img_path'])\n",
    "          mask = sitk.ReadImage(pat['seg_path'])\n",
    "          mask_resampled = reconstruct_mask_in_image_space(mask, ref_img)\n",
    "          img_np = normalize_zero_one(sitk.GetArrayFromImage(ref_img).astype(np.float32))\n",
    "          mask_np = sitk.GetArrayFromImage(mask_resampled).astype(np.uint8)\n",
    "          for k in range(img_np.shape[0]):\n",
    "              if mask_np[k].any(): self.slices.append((img_np[k], mask_np[k]))\n",
    "        print(f\"\\nDataset ready in {time.time() - start_time:.2f}s.\")\n",
    "\n",
    "    def __len__(self): return len(self.slices)\n",
    "    def __getitem__(self, idx):\n",
    "        img, msk = self.slices[idx]; t=self.transform(image=img, mask=msk); return t[\"image\"], t[\"mask\"].unsqueeze(0).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fXX8HgBZEemZ",
   "metadata": {
    "id": "fXX8HgBZEemZ"
   },
   "source": [
    "\n",
    "> 15 minutes in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ki49bKrl73Zv",
   "metadata": {
    "id": "ki49bKrl73Zv"
   },
   "outputs": [],
   "source": [
    "seg_dataset = SegmentationDataset(clinical_df, DATA_PATH, augment=True)\n",
    "print(\"\\nTotal tumor slices indexed:\", len(seg_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9lYAnBGJ_jDN",
   "metadata": {
    "cellView": "form",
    "id": "9lYAnBGJ_jDN"
   },
   "outputs": [],
   "source": [
    "#@title Sanity Check: visualize index slices form our dataset\n",
    "slice_index = 60  #@param {type: \"number\"}\n",
    "\n",
    "if len(seg_dataset) > 0:\n",
    "    img_t, msk_t = seg_dataset[min(slice_index,len(seg_dataset)-1)]\n",
    "    img_np, msk_np = img_t.squeeze().numpy(), msk_t.squeeze().numpy()\n",
    "    fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(18,6))\n",
    "    fig.suptitle(f\"Index slice {slice_index}\\n\")\n",
    "    # Input MRI\n",
    "    ax1.imshow(img_np,cmap=\"gray\"); ax1.set_title(\"Input MRI\"); ax1.axis(\"off\")\n",
    "    # Ground Truth\n",
    "    ax2.imshow(msk_np,cmap=\"gray\"); ax2.set_title(\"Ground Truth\"); ax2.axis(\"off\")\n",
    "    # Overlay\n",
    "    ax3.imshow(img_np,cmap=\"gray\");ax3.contour(msk_np, colors=\"red\", linewidths=0.5);\n",
    "    ax3.imshow(np.ma.masked_where(msk_np != 1, msk_np),cmap=\"Reds\", alpha=0.2, vmin=0, vmax=1)\n",
    "    ax3.set_title(\"Overlay\"); ax3.axis(\"off\"); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LSOLMAs0FMxE",
   "metadata": {
    "id": "LSOLMAs0FMxE"
   },
   "outputs": [],
   "source": [
    "#@title Free up system RAM so we can continue with the rest of the notebook\n",
    "try:\n",
    "  del seg_dataset\n",
    "  gc.collect()\n",
    "  print(\"SegmentationDataset variable deleted and garbage collection performed.\")\n",
    "except:\n",
    "  print(\"Already cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c906b6d",
   "metadata": {
    "id": "6c906b6d"
   },
   "source": [
    "\n",
    "## 3. Data Splitting & Preparation\n",
    "\n",
    "To build a model that we can trust, we must evaluate it on data it has never seen before. It's like teaching a student: you wouldn't test them on the exact same questions they studied in their textbook. This principle of separating learning data from testing data is fundamental to all of machine learning.\n",
    "\n",
    "We will divide our dataset into three distinct, independent sets:\n",
    "\n",
    "1.  **Training Set (70% of patients)**: This is the bulk of our data, our \"textbook.\" The model will learn exclusively from these images to identify the patterns and features associated with brain tumors.\n",
    "2.  **Validation Set (10% of patients)**: This is our \"practice exam.\" During the training process, we use this set to check the model's performance on unseen data. It helps us answer crucial questions like, \"Is the model starting to overfit (memorize) the training data?\" and allows us to tune our model's settings (hyperparameters).\n",
    "3.  **Test Set (20% of patients)**: This is the \"final, proctored exam.\" This set is kept completely separate and is used only *once* at the very end of our project to get a final, unbiased measure of the model's performance in a real-world scenario.\n",
    "\n",
    "---\n",
    "\n",
    "#### The Golden Rule in Medical AI: Split by Patient\n",
    "\n",
    "How we perform this split is critically important. Instead of randomly splitting all our 2D slices, we must split by **patient**.\n",
    "\n",
    "Slices from the same patient are highly correlated—they share the same unique anatomy, are from the same scanner, and may have similar imaging artifacts. If we were to put some slices from Patient A into the training set and others into the test set, the model might learn to recognize Patient A's specific anatomy rather than the general features of a tumor. This critical error, known as **data leakage**, would lead to an artificially inflated performance score. The model would seem to perform well because it's \"cheating\" by recognizing patients it has already seen. By splitting at the patient level, we ensure our model is truly tested on its ability to generalize to new, unseen individuals.\n",
    "\n",
    "#### The Refinement: Stratification\n",
    "\n",
    "To make our splits even more robust, we use **stratification**. Imagine if, by pure random chance, all patients with a \"Meningioma\" tumor ended up in our test set. Our model would never have learned to identify them during training, leading to a guaranteed failure. Stratification prevents this by ensuring that the proportion of each tumor type (`Histopathology`) is roughly the same across the training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z5SSakpBRPh6",
   "metadata": {
    "id": "z5SSakpBRPh6"
   },
   "outputs": [],
   "source": [
    "# Stratified patient-wise split (if metadata exists), else random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "patient_data = (clinical_df[[\"Case Number\",\"Histopathology\"]]\n",
    "                .drop_duplicates()\n",
    "                .set_index(\"Case Number\"))\n",
    "patient_list = patient_data.index.to_numpy()\n",
    "patient_labels = patient_data[\"Histopathology\"].to_numpy()\n",
    "print(f\"Total patients: {len(patient_list)}\")\n",
    "print(f\"Total labels: {len(patient_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4K5dUGCkRTCe",
   "metadata": {
    "id": "4K5dUGCkRTCe"
   },
   "source": [
    "Now we have a unique list of patients and their corresponding labels for stratification. Let's perform the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2910b96e",
   "metadata": {
    "id": "2910b96e"
   },
   "outputs": [],
   "source": [
    "pt_train, pt_temp, lb_train, lb_temp = train_test_split(\n",
    "    patient_list, patient_labels, test_size=0.3, stratify=patient_labels, random_state=RANDOM_SEED\n",
    ")\n",
    "pt_val, pt_test, _, _ = train_test_split(\n",
    "    pt_temp, lb_temp, test_size=(2/3.0), stratify=lb_temp, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "train_df = clinical_df[clinical_df[\"Case Number\"].isin(pt_train)]\n",
    "val_df   = clinical_df[clinical_df[\"Case Number\"].isin(pt_val)]\n",
    "test_df  = clinical_df[clinical_df[\"Case Number\"].isin(pt_test)]\n",
    "\n",
    "print(\"Split sizes (patients if available):\")\n",
    "if \"Case Number\" in clinical_df and len(clinical_df)>0:\n",
    "    print(\"Train:\", len(pt_train), \" Val:\", len(pt_val), \" Test:\", len(pt_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h5wxTLoaQq_9",
   "metadata": {
    "id": "h5wxTLoaQq_9"
   },
   "source": [
    "\n",
    "#### The Final Step: The `DataLoader` (2 minutes)\n",
    "\n",
    "Once we have our lists of patients for each set, we create `DataLoader` objects. A `DataLoader` is a powerful PyTorch utility that acts as an efficient pipeline, feeding data to the GPU. It automatically handles:\n",
    "-   **Batching**: Grouping multiple images together to be processed at once.\n",
    "-   **Shuffling**: Randomizing the order of the training data each epoch to improve learning.\n",
    "-   **Parallelization**: Using multiple CPU cores to prepare the next batch of data while the GPU is busy, preventing bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f6279",
   "metadata": {
    "id": "8f7f6279"
   },
   "outputs": [],
   "source": [
    "# Build datasets/loaders\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "train_ds = SegmentationDataset(train_df, DATA_PATH, size=512, augment=True)\n",
    "val_ds   = SegmentationDataset(val_df,   DATA_PATH, size=512, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"DataLoaders ready. Train: {len(train_ds)}, Validation: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VL7GIRRgOaqD",
   "metadata": {
    "id": "VL7GIRRgOaqD"
   },
   "source": [
    "\n",
    "> **Summary**:\n",
    ">\n",
    "> **What is Data Leakage?** Data leakage occurs when information from outside the training set is used to create the model. In medical imaging, this is a particularly insidious problem. Slices from the same patient are highly correlated; they share the same anatomy, are scanned on the same machine (with the same potential artifacts), and have the same underlying pathology.\n",
    ">\n",
    "> **Why is it so bad?** If you randomly split slices (instead of patients), you might have slice #5 of Patient-A in your training set and slice #10 of Patient-A in your test set. The model might learn to recognize features unique to Patient-A's anatomy or scan quality, rather than the general features of the tumor. This leads to an artificially high score on your test set because the model is effectively \"cheating\" by recognizing the patients it has already seen. When deployed in the real world on a new patient, its performance would be drastically lower.\n",
    ">\n",
    "> **The Rule:** Always split by the independent unit of interest, which in clinical applications is almost always the **patient**.\n",
    ">\n",
    "> **In a nutshell:** Splitting by patient ensures your model is evaluated on its ability to analyze new patients, not its ability to remember patients it saw during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PTmiYpHOE3JM",
   "metadata": {
    "id": "PTmiYpHOE3JM"
   },
   "source": [
    "> 25 minutes in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3aa86",
   "metadata": {
    "id": "aca3aa86"
   },
   "source": [
    "\n",
    "## 4. Modeling and Training: Teaching the AI to See\n",
    "\n",
    "With our data meticulously prepared and loaded, we have arrived at the core of our project: building and training the neural network. This is where the learning happens. We will teach our model to distinguish between tumor and non-tumor tissue by showing it thousands of labeled examples from our training set.\n",
    "\n",
    "Our chosen architecture is the **U-Net**, a model renowned for its success in biomedical image segmentation. We will enhance it further by using a **pre-trained encoder**, a powerful technique known as **transfer learning**.\n",
    "\n",
    "---\n",
    "> ### Deep Dive: The U-Net Architecture and Transfer Learning\n",
    ">\n",
    "> The **U-Net**, introduced in 2015 for biomedical image segmentation, is an elegant and powerful convolutional neural network (CNN) architecture. Its name comes from its distinctive U-shape. To understand its power, let's break it down into its three key components:\n",
    ">\n",
    "><img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"70%\">\n",
    ">\n",
    ">Ronneberger, O., Fischer, P., Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In: Navab, N., Hornegger, J., Wells, W., Frangi, A. (eds) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. MICCAI 2015 https://arxiv.org/pdf/1505.04597\n",
    ">\n",
    "> 1.  **The Encoder (or Contracting Path):** This is a standard CNN that progressively downsamples the input image. With each downsampling step (typically a max-pooling operation), the network's spatial resolution decreases, but its \"semantic\" understanding of the image increases. It learns to identify *what* is in the image at a high level (e.g., \"there is tumor-like texture in this quadrant\"). Think of it as zooming out to get the general context.\n",
    ">\n",
    "> 2.  **The Decoder (or Expansive Path):** This path takes the low-resolution, high-context feature map from the encoder and progressively upsamples it. Its job is to recover the spatial information and precisely localize the features identified by the encoder. It answers the question of *where* exactly the features are, ultimately creating the detailed, pixel-by-pixel segmentation mask. Think of it as zooming back in to carefully draw the outlines.\n",
    ">\n",
    "> 3.  **Skip Connections:** This is the U-Net's most critical innovation. Skip connections create a direct link, a \"shortcut,\" between the encoder and decoder at the same spatial level. This allows the decoder to access the high-resolution feature maps from the early layers of the encoder. Why is this so important? The encoder, in its quest for context, loses fine-grained spatial detail. The skip connections allow the decoder to \"remember\" these details (like precise edges), enabling it to produce much sharper and more accurate segmentations.\n",
    ">\n",
    "> **The Power of Transfer Learning**: When we instantiate our U-Net, we set `encoder_weights=\"imagenet\"`. This means the encoder part of our U-Net isn't initialized with random numbers. Instead, it's pre-loaded with knowledge learned from training on the **ImageNet** dataset—a massive collection of over a million everyday photographs. While MRIs are very different from photos, this pre-training has taught the encoder to recognize fundamental visual patterns like edges, gradients, textures, and shapes. This gives our model a significant head start, leading to faster training and often better final performance, which is especially valuable when working with smaller medical datasets.\n",
    ">\n",
    "> **In a nutshell:** The U-Net combines a \"what\" pathway (encoder) with a \"where\" pathway (decoder), using skip connections to preserve detail. Transfer learning gives it a head start by pre-loading it with basic visual knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ac7b2",
   "metadata": {
    "id": "877ac7b2"
   },
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet50\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=1,\n",
    "    classes=1,\n",
    ").to(device)\n",
    "\n",
    "print(model.__class__.__name__, \"ready on\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x5EzcE4U9K9s",
   "metadata": {
    "id": "x5EzcE4U9K9s"
   },
   "source": [
    "### Guiding the Learning: Loss Functions vs. Metrics\n",
    "\n",
    "\n",
    "\n",
    "-   **Metric**: This is the \"final grade\" on a report card. It's a human-interpretable score, like the **Dice Coefficient**, that we use to judge the model's performance. It doesn't need to be differentiable and is not used for training, only for evaluation.\n",
    "\n",
    "### The Training Loop: The Engine Room\n",
    "\n",
    "The code that follows will define our training and validation loops. This process is orchestrated by a few key components:\n",
    "-   **Optimizer (`AdamW`)**: The engine that implements the learning process, deciding exactly how to adjust the model's weights based on the loss function's feedback.\n",
    "-   **Learning Rate Scheduler (`CosineAnnealingLR`)**: A strategy to adjust the learning rate (the size of the adjustment steps) during training. It typically starts with larger steps and gradually decreases them to allow the model to fine-tune its performance.\n",
    "-   **Checkpointing**: The crucial practice of saving the model's state whenever its performance on the validation set improves. This ensures we don't lose our best model.\n",
    "\n",
    "Now, you will put these concepts into practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160df9f5",
   "metadata": {
    "id": "160df9f5"
   },
   "source": [
    "### 4.1 Metrics\n",
    "\n",
    "This is the \"final grade\" on a report card. It's a human-interpretable score, like the Dice Coefficient, that we use to judge the model's performance. It doesn't need to be differentiable and is not used for training, only for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1muIiFbDZZjl",
   "metadata": {
    "id": "1muIiFbDZZjl"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/DiceScore.png\" width=\"60%\">\n",
    "\n",
    "**Dice** ≈ overlap between predicted and true mask (0-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626885c9",
   "metadata": {
    "id": "626885c9"
   },
   "outputs": [],
   "source": [
    "def dice_coefficient(logits, targets, smooth=1.0):\n",
    "    preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "    preds_flat = preds.view(-1)\n",
    "    targets_flat = targets.view(-1)\n",
    "    intersection = (preds_flat * targets_flat).sum()\n",
    "    union = preds_flat.sum() + targets_flat.sum()\n",
    "    return ((2. * intersection + smooth) / (union + smooth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kVtqzqanbOxI",
   "metadata": {
    "id": "kVtqzqanbOxI"
   },
   "source": [
    "> The Role of the `smooth` Parameter in Dice Calculations. When implementing the Dice coefficient and Dice loss, you will notice a small but crucial parameter often called `smooth` or `epsilon` (ε).\n",
    "\n",
    "This `smooth` term serves two vital purposes:\n",
    "\n",
    "**Preventing Division by Zero (Numerical Stability)**\n",
    "\n",
    "The primary role of the `smooth` parameter is to ensure numerical stability. Consider the scenario where both the ground truth mask and the model's prediction are completely empty for a given image (i.e., there is no tumor, and the model correctly predicts no tumor).\n",
    "\n",
    "In this case:\n",
    "-   The **intersection** (predicted tumor pixels AND true tumor pixels) would be **0**.\n",
    "-   The **union** (all predicted tumor pixels OR all true tumor pixels) would also be **0**.\n",
    "\n",
    "The Dice formula would become `(2 * 0) / (0 + 0)`, which results in a `0 / 0` division. This is an undefined mathematical operation and would cause the program to crash or return `NaN` (Not a Number).\n",
    "\n",
    "By adding a small `smooth` value to both the numerator and the denominator, we prevent this from ever happening. The formula becomes `(2 * 0 + smooth) / (0 + 0 + smooth)`, which simplifies to `smooth / smooth = 1.0`. This is a sensible result: if the model perfectly predicts an empty mask, it should get a perfect Dice score of 1.0.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TaEqAzebZ0ZH",
   "metadata": {
    "id": "TaEqAzebZ0ZH"
   },
   "source": [
    "### 4.2 Loss Function\n",
    "\n",
    "To teach our model, we need two key types of feedback:\n",
    "\n",
    "-   **Loss Function**: This is the \"teacher's correction.\" It's a differentiable function that calculates a penalty score based on how wrong the model's prediction is. The model uses this score during backpropagation to adjust its internal weights and improve. We will experiment with three different loss functions to see which \"teaching style\" is most effective:\n",
    "    1.  **BCEWithLogitsLoss**: A stable, standard loss that treats each pixel as a simple binary classification problem (tumor or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3kfdtQjm5E-",
   "metadata": {
    "id": "a3kfdtQjm5E-"
   },
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/BCEWithLogitsLoss.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L5D2is_mnIGv",
   "metadata": {
    "id": "L5D2is_mnIGv"
   },
   "outputs": [],
   "source": [
    "# Standard BCE Loss with logits\n",
    "bce_loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lPNMKuEsm0VF",
   "metadata": {
    "id": "lPNMKuEsm0VF"
   },
   "source": [
    "2.  **DiceLoss**: Directly optimizes the Dice Score, our main goal. It's excellent for imbalanced datasets where the tumor is a small part of the image.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/dice_loss.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V1_6JaiWm7ih",
   "metadata": {
    "id": "V1_6JaiWm7ih"
   },
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        p = probs.view(-1); t = targets.view(-1)\n",
    "        inter = (p * t).sum()\n",
    "        dice  = (2*inter + self.smooth) / (p.sum() + t.sum() + self.smooth)\n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yQwv3KSt3q1R",
   "metadata": {
    "id": "yQwv3KSt3q1R"
   },
   "source": [
    "**Smoothing the Loss Landscape for Better Training**\n",
    "\n",
    "When used within the `DiceLoss` function, the `smooth` parameter has a secondary, more subtle benefit. It helps to \"smooth out\" the loss landscape, especially in the early stages of training.\n",
    "\n",
    "Without the smoothing term, the gradient of the Dice loss can be very sharp and unstable when predictions are poor. By adding `smooth`, we ensure that the loss function is better-behaved, providing more stable and reliable gradients for the optimizer to work with. This can lead to a more stable and efficient training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SkiqkMjxnS4H",
   "metadata": {
    "id": "SkiqkMjxnS4H"
   },
   "source": [
    "3.  **BCEDiceLoss**: A hybrid that combines the stability of BCE with the direct metric optimization of Dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zv90iCYOm9Bm",
   "metadata": {
    "id": "zv90iCYOm9Bm"
   },
   "outputs": [],
   "source": [
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "    def forward(self, logits, targets):\n",
    "        return 0.5*self.bce(logits, targets) + 0.5*self.dice(logits, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DqiidS_fm1o6",
   "metadata": {
    "id": "DqiidS_fm1o6"
   },
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/6c6bb94415c9377fc563efb3570a7a69bb1182f8/images/BCEDiceloss.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-UAgvYI4M3v",
   "metadata": {
    "id": "h-UAgvYI4M3v"
   },
   "source": [
    "\n",
    "We’ll later compare **BCEWithLogitsLoss**, **DiceLoss**, and **BCEDiceLoss**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fv6krbNDG5Qq",
   "metadata": {
    "id": "fv6krbNDG5Qq"
   },
   "source": [
    "> 35 minutes in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wGT7IT47pqj",
   "metadata": {
    "id": "8wGT7IT47pqj"
   },
   "source": [
    "### 4.3 Training and Validation Loops\n",
    "\n",
    "Now we define the core functions that will drive the training process. The `train_epoch` function, which iterates through the training data and updates the model's weights, has been provided for you.\n",
    "\n",
    "Your first programming task is to implement its crucial counterpart: the validation loop.\n",
    "\n",
    "### Q1: Implement the Validation Loop\n",
    "\n",
    "**The Purpose of Validation:**\n",
    "While the training loop teaches the model, the validation loop acts as its \"practice exam.\" Its purpose is to provide an unbiased assessment of how well the model is generalizing to data it has not been trained on. We run this check at the end of every epoch to monitor our progress and make key decisions, such as saving our best-performing model.\n",
    "\n",
    "**Your Task:**\n",
    "Write a `validate(model, loader, criterion, device)` that returns mean loss and mean Dice. This function should:\n",
    "1.  **Set the model to evaluation mode** by calling `model.eval()`. This is crucial as it deactivates layers like `Dropout` and normalizes behavior in layers like `BatchNorm`, ensuring consistent and repeatable predictions.\n",
    "2.  **Disable gradient calculations** using a `with torch.no_grad():` block. Since we are only evaluating and not learning, we don't need to compute gradients, which saves significant memory and computation time.\n",
    "3.  **Iterate** through every batch in the provided validation `loader`.\n",
    "4.  For each batch, **calculate the validation loss** using the `criterion` and the **Dice coefficient** using our helper function.\n",
    "5.  **Accumulate** these scores and, at the end, return the **average** validation loss and average Dice score across all batches.\n",
    "\n",
    "This function will be the cornerstone of our ability to track, evaluate, and compare our training experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f066c90",
   "metadata": {
    "id": "7f066c90"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for imgs, masks in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, masks)\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total += loss.item()\n",
    "    return total / max(len(loader), 1)\n",
    "\n",
    "# SOLUTION for Q1\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses, dices = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            ...\n",
    "    return ...\n",
    "\n",
    "print(\"Training/Validation helpers ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb1937",
   "metadata": {
    "id": "b7eb1937"
   },
   "source": [
    "\n",
    "### 4.4 Training Experiments\n",
    "\n",
    "Now that we have our training and validation machinery in place, we can conduct a systematic experiment to answer a critical question in deep learning: **Does the choice of loss function matter?**\n",
    "\n",
    "The loss function is the signal we use to \"teach\" our model. A different loss function represents a different teaching style, emphasizing different aspects of the prediction error.\n",
    "\n",
    "### Q2: Which Loss Function is Best?\n",
    "\n",
    "**Your Task:**\n",
    "Your task is to run the experiment defined in the code block below. We will train our U-Net model three separate times from scratch. Each training run will be identical in every way (same architecture, same data, same learning rate) *except* for the loss function used:\n",
    "1.  **Experiment 1**: Using `BCEWithLogitsLoss`\n",
    "2.  **Experiment 2**: Using `DiceLoss`\n",
    "3.  **Experiment 3**: Using the hybrid `BCEDiceLoss`\n",
    "\n",
    "To keep our code clean, the entire training process is encapsulated within a helper function called `run_experiment`. For each experiment, this function will:\n",
    "-   Initialize a fresh U-Net model.\n",
    "-   Set up the optimizer and learning rate scheduler.\n",
    "-   Run the training and validation loop for the specified number of `EPOCHS`.\n",
    "-   Perform **checkpointing**: it will monitor the validation Dice score at the end of each epoch and save the model's weights (`.pt` file) to a unique directory *only* if the performance has improved. This ensures we capture the best version of each model.\n",
    "\n",
    "This process will take some time, as it involves three full training runs. As the code executes, observe the `val_dice` output for each epoch and see how each \"teaching style\" guides the model's learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JDN1_b_pIIwo",
   "metadata": {
    "id": "JDN1_b_pIIwo"
   },
   "source": [
    "#### Training (7 minutes per model | 1.5 minutes per epoch)\n",
    "\n",
    "> Training takes around 23 minutes total. If you are under low resource constrains, you could load the pre-trained weights provided inside `./checkpoints_{loss}` folders.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647b227",
   "metadata": {
    "id": "5647b227"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "def run_experiment(loss_name, criterion):\n",
    "    torch.cuda.empty_cache()\n",
    "    model = smp.Unet(\"resnet50\", encoder_weights=\"imagenet\", in_channels=1, classes=1).to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    sch = CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=1e-6)\n",
    "    scaler = torch.cuda.amp.GradScaler() if (torch.cuda.is_available() and device.type=='cuda') else None\n",
    "\n",
    "    best_d = 0.0\n",
    "    ckpt_dir = f\"./checkpoints_{loss_name}\"; os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        print(f\"Epoch {ep}/{EPOCHS} [{loss_name}] LR={opt.param_groups[0]['lr']:.1e}\")\n",
    "        tr = train_epoch(model, train_loader, criterion, opt, scaler, device)\n",
    "        vl, vd = validate(model, val_loader, criterion, device)\n",
    "        print(f\"  train_loss={tr:.4f}  val_loss={vl:.4f}  val_dice={vd:.4f}\")\n",
    "        if vd > best_d:\n",
    "            best_d = vd\n",
    "            path = os.path.join(ckpt_dir, \"best.pt\")\n",
    "            torch.save(model.state_dict(), path)\n",
    "            print(\"Saved best to\", path)\n",
    "        sch.step()\n",
    "    return best_d, os.path.join(ckpt_dir, \"best.pt\")\n",
    "\n",
    "results = {}\n",
    "# SOLUTION FOR Q2\n",
    "for name, criterion in [\n",
    "    (\"...\",...),\n",
    "    (\"...\",...),\n",
    "    (\"...\",...)]:\n",
    "    ... = run_experiment(name, criterion)\n",
    "    results[name] = {\"best_val_dice\": ... \"ckpt\": ...}\n",
    "# ----\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: Dice={v['best_val_dice']:.4f}  -> {v['ckpt']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hz1-th_nTrR0",
   "metadata": {
    "cellView": "form",
    "id": "hz1-th_nTrR0"
   },
   "outputs": [],
   "source": [
    "#@title cleanup free RAM and GPU cache\n",
    "import gc, torch, matplotlib.pyplot as plt\n",
    "\n",
    "for n in (\"model\", \"opt\", \"test_ds\",\"train_loader\", \"train_ds\", \"val_ds\"):\n",
    "    if n in globals():\n",
    "        del globals()[n]\n",
    "\n",
    "plt.close(\"all\")\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    try: torch.cuda.ipc_collect()\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YW5XyS9098mS",
   "metadata": {
    "id": "YW5XyS9098mS"
   },
   "source": [
    "After the experiments are complete, compare the final summary.\n",
    "- Which loss function achieved the highest validation Dice score on this task? - Why do you think that might be?\n",
    "- Does the stability of BCE combined with the directness of Dice (in the `BCEDiceLoss`) offer a \"best of both worlds\" advantage, or did a simpler approach win out?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0yxkilKsHtPv",
   "metadata": {
    "id": "0yxkilKsHtPv"
   },
   "source": [
    "> 45 minutes in (without counting 23 minutes of training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c5faf",
   "metadata": {
    "id": "f83c5faf"
   },
   "source": [
    "## 5. Final Evaluation and Quality Control\n",
    "\n",
    "We have trained our models and used the validation set to guide our experiments. Now, we reach the most critical stage of evaluation: **the final exam**.\n",
    "\n",
    "Up to this point, the **test set** has been kept completely locked away. Our model has never seen a single image from it. This is by design. The performance on this held-out set represents our most honest and reliable estimate of how the model will perform on new, unseen patients in a real-world setting. It is the final verdict on our model's capabilities.\n",
    "\n",
    "### Beyond a Single Score: A Complete Diagnostic Report\n",
    "\n",
    "While the Dice score has been our primary guide, a single metric rarely tells the whole story. To truly understand our model's strengths and weaknesses, we need a more complete diagnostic report. We will therefore evaluate it using a suite of metrics:\n",
    "\n",
    "-   **Dice Coefficient & IoU (Intersection over Union)**: Both measure the overlap between the prediction and the ground truth. IoU is generally a stricter metric than Dice, meaning it will yield a lower score for the same level of performance.\n",
    "-   **Precision**: Answers the question: *\"Of all the pixels the model labeled as tumor, what fraction were actually tumor?\"* High precision means the model makes few **false positive** errors. Clinically, this translates to not raising false alarms or suggesting unnecessary biopsies.\n",
    "-   **Recall (or Sensitivity)**: Answers the question: \"*Of all the pixels that were actually tumor, what fraction did the model correctly identify?*\" High recall means the model makes few **false negative** errors. This is often critically important in medicine, as it relates to not missing a diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hSxyushu-7df",
   "metadata": {
    "id": "hSxyushu-7df"
   },
   "source": [
    "\n",
    "### Q3 & Q4: Implementing and Running the Final Evaluation\n",
    "\n",
    "To get this complete picture, your next tasks are to build the tools and run the final analysis.\n",
    "\n",
    "**Q3: Implement Evaluation Metrics**\n",
    "Your first task is to complete the helper functions `compute_iou` and `compute_precision_recall`. These will be the tools you use to generate the model's \"diagnostic report.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bljC3T97AGNt",
   "metadata": {
    "id": "bljC3T97AGNt"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "def compute_iou(logits, targets, threshold=0.5, eps=1e-7):\n",
    "    preds = (torch.sigmoid(logits) > threshold).float()\n",
    "    t = (targets > 0.5).float()\n",
    "    inter= ...\n",
    "    union= ...\n",
    "    iou = ...\n",
    "    return iou.item()\n",
    "\n",
    "\n",
    "def compute_precision_recall(logits, targets, threshold=0.5, eps=1e-7):\n",
    "    preds = (torch.sigmoid(logits) > threshold).float()\n",
    "    t = (targets > 0.5).float()\n",
    "    TP = ...\n",
    "    FP = ...\n",
    "    FN = ...\n",
    "    precision = ...\n",
    "    recall    = ...\n",
    "    return precision.item(), recall.item()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe879e4",
   "metadata": {
    "id": "8fe879e4"
   },
   "outputs": [],
   "source": [
    "# SOLUTION for Q3\n",
    "def compute_iou(logits, targets, threshold=0.5, eps=1e-7):\n",
    "  ...\n",
    "\n",
    "def compute_precision_recall(logits, targets, threshold=0.5, eps=1e-7):\n",
    "  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xd9vta54_UoK",
   "metadata": {
    "id": "xd9vta54_UoK"
   },
   "source": [
    "\n",
    "**Q4: Run the Final Test**\n",
    "Your second task is to use these new metrics in a final evaluation loop. The process will be:\n",
    "1.  Identify which of our three experimental models (trained with BCE, Dice, or BCEDice loss) performed best on the validation set.\n",
    "2.  Load the saved checkpoint (`best.pt` file) for that single best model.\n",
    "3.  Run this model on the **test loader** and compute all four metrics: Dice, IoU, Precision, and Recall.\n",
    "\n",
    "This will give us the definitive performance summary for our project.\n",
    "\n",
    "We’ll compute **Dice**, **IoU**, **Precision**, and **Recall** for the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GN4XdUnuSm0k",
   "metadata": {
    "id": "GN4XdUnuSm0k"
   },
   "outputs": [],
   "source": [
    "test_ds  = SegmentationDataset(test_df, DATA_PATH, size=512, augment=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "print(f\"DataLoaders ready. Test: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OBRAllMAAEGP",
   "metadata": {
    "id": "OBRAllMAAEGP"
   },
   "outputs": [],
   "source": [
    "# SOLUTION for Q4\n",
    "try:\n",
    "  # Pick best of all automatically\n",
    "  best_key = max(results.keys(), key=lambda k: results[k]['best_val_dice'])\n",
    "  best_ckpt = # COMPLETE get results checkpoint for the best_key\n",
    "  print(\"Using best model:\", best_key, \"->\", best_ckpt)\n",
    "except:\n",
    "  best_checkpoint = './checkpoints_BCE/best.pt'  #@param {type: \"string\"}\n",
    "  best_ckpt = best_checkpoint # Or define it yourself from the directory\n",
    "  print(\"Using best model:\", best_ckpt)\n",
    "\n",
    "final_model = smp.Unet(... #COMPLETE definition of resnet50 Unet).to(device)\n",
    "final_model.load_state_dict(torch.load(best_ckpt, map_location=device))\n",
    "final_model.eval()\n",
    "\n",
    "dice_list, iou_list, prec_list, rec_list = [], [], [], []\n",
    "\n",
    "threshold_iou = 0.5 #@param {type: \"slider\", min: 0, max: 1, step:0.01}\n",
    "threshold_precision_recall = 0.5 #@param {type: \"slider\", min: 0, max: 1, step:0.01}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, masks in tqdm(test_loader, desc=\"Testing\"):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        logits = final_model(imgs)\n",
    "        dice_list.append(...) # Complete metrics\n",
    "        iou_list.append(...) # Complete metrics\n",
    "        p, r = compute_precision_recall(...) # Complete metrics\n",
    "        prec_list.append(p); rec_list.append(r)\n",
    "\n",
    "print(\"\\n--- Test Results ---\")\n",
    "print(f\"Dice:     {np.mean(dice_list):.4f}\")\n",
    "print(f\"IoU:      {np.mean(iou_list):.4f}\")\n",
    "print(f\"Precision:{np.mean(prec_list):.4f}\")\n",
    "print(f\"Recall:   {np.mean(rec_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7d330",
   "metadata": {
    "id": "f5b7d330"
   },
   "source": [
    "### 5.1 Visualizing Predictions: The \"Eye Test\" for AI\n",
    "\n",
    "Quantitative metrics like the Dice score are essential for summarizing our model's performance, but they don't tell the whole story. A model might achieve a respectable Dice score of 0.85, but is it making clinically acceptable errors? Does it consistently fail on small tumors? Does it struggle with blurry boundaries? To answer these crucial \"why\" and \"how\" questions, we must perform a **visual quality control**—the \"eye test\" for our AI model.\n",
    "\n",
    "By visualizing our model's predictions on the test set, we can gain invaluable insights that numbers alone cannot provide. In this section, our goal is to move beyond averages and look at individual cases to understand the model's behavior in detail.\n",
    "\n",
    "---\n",
    "\n",
    "#### Finding Failure Modes: Learning from Mistakes\n",
    "\n",
    "The most informative cases are often the ones where the model performed poorly. A model's mistakes are a rich source of information, revealing its inherent biases and weaknesses. By examining the **worst predictions** (those with the lowest Dice scores), we can identify specific patterns of failure. For example, we might discover that our model consistently:\n",
    "-   Misses very small lesions entirely.\n",
    "-   Under-segments tumors with fuzzy, infiltrative borders.\n",
    "-   Is confused by imaging artifacts or unusual patient anatomy.\n",
    "\n",
    "Conversely, by looking at the **best predictions**, we can confirm what the model has learned to do well and build confidence in its capabilities under ideal conditions.\n",
    "\n",
    "In the following code block, we will implement a function that automatically finds the best and worst predictions from our test set and plots them side-by-side with the input image and the ground truth mask.\n",
    "\n",
    "As you look at the visualizations, ask yourself:\n",
    "-   In the \"worst\" cases, what do the failures have in common? What kind of errors is the model making (e.g., false positives, false negatives)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ljDhKxHyCoJD",
   "metadata": {
    "id": "ljDhKxHyCoJD"
   },
   "outputs": [],
   "source": [
    "def visualize_best_and_worst(model, dataset, n=3, device=\"cuda\", contour_width=1.2):\n",
    "    model.eval(); results = []\n",
    "    for i in tqdm(range(len(dataset)), desc=\"Gathering predictions\"):\n",
    "        img_t, msk_t = dataset[i]\n",
    "        with torch.no_grad():\n",
    "            logits = model(img_t.unsqueeze(0).to(device))\n",
    "            pred_t = (torch.sigmoid(logits) > 0.5).float()\n",
    "            dice = dice_coefficient(pred_t, msk_t.to(device)).item()  # Dice on binarized pred\n",
    "        results.append({\n",
    "            'dice': dice,\n",
    "            'img': img_t.squeeze().numpy(),\n",
    "            'msk': msk_t.squeeze().numpy(),\n",
    "            'pred': pred_t.cpu().squeeze().numpy()\n",
    "        })\n",
    "    results.sort(key=lambda x: x['dice'])\n",
    "    def plot(cases, title):\n",
    "        fig, axes = plt.subplots(len(cases), 4, figsize=(20, 4*len(cases)))\n",
    "        fig.suptitle(title, fontsize=20, y=1.02)\n",
    "        for i, case in enumerate(cases):\n",
    "            ax = axes if len(cases) == 1 else axes[i]\n",
    "            ax[0].imshow(case['img'], cmap='gray'); ax[0].set_title(\"Input\"); ax[0].axis('off')\n",
    "            ax[1].imshow(case['msk'], cmap='gray'); ax[1].set_title(\"Ground Truth\"); ax[1].axis('off')\n",
    "            ax[2].imshow(case['pred'], cmap='gray'); ax[2].set_title(f\"Prediction\\nDice:{case['dice']:.3f}\"); ax[2].axis('off')\n",
    "            # Overlay: GT in green, Pred in red (with contours)\n",
    "            gt = case['msk'] > 0.5\n",
    "            pr = case['pred'] > 0.5\n",
    "            ax[3].imshow(case['img'], cmap='gray')\n",
    "            ax[3].imshow(np.ma.masked_where(~gt, gt), cmap='Greens', alpha=0.35, vmin=0, vmax=1)\n",
    "            ax[3].imshow(np.ma.masked_where(~pr, pr), cmap='Reds',   alpha=0.35, vmin=0, vmax=1)\n",
    "            # Contours at the 0.5 boundary\n",
    "            ax[3].contour(gt.astype(float), levels=[0.5], colors='lime', linewidths=contour_width)\n",
    "            ax[3].contour(pr.astype(float), levels=[0.5], colors='red',  linewidths=contour_width)\n",
    "            ax[3].set_title(\"Overlay (GT=green, Pred=red + contours)\"); ax[3].axis('off')\n",
    "        plt.tight_layout(rect=[0,0,1,0.97]); plt.show()\n",
    "    plot(results[:n],  \"Worst Predictions\")\n",
    "    plot(results[-n:], \"Best Predictions\")\n",
    "\n",
    "visualize_best_and_worst(final_model, test_ds, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UWvbcXSkWTmY",
   "metadata": {
    "id": "UWvbcXSkWTmY"
   },
   "source": [
    "> 60 minutes in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FkzkFszt7Oef",
   "metadata": {
    "id": "FkzkFszt7Oef"
   },
   "source": [
    "# Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hs-QqtvDsFpG",
   "metadata": {
    "id": "Hs-QqtvDsFpG"
   },
   "source": [
    "Reimport libraries (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u6HKJIxCrqn5",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6HKJIxCrqn5",
    "outputId": "23afc3e0-94be-40e1-a0ce-7505f8e897a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hLibraries imported and environment configured successfully!\n"
     ]
    }
   ],
   "source": [
    "#@title re-import libraries\n",
    "# Install the required libraries for our notebook\n",
    "# segmentation-models-pytorch is a great library with pre-built segmentation model architectures.\n",
    "# albumentations is a powerful library for data augmentation.\n",
    "!pip install -U segmentation-models-pytorch albumentations SimpleITK -q\n",
    "\n",
    "# Library Imports\n",
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from glob import glob\n",
    "import math\n",
    "import time\n",
    "import gc # Garbage collector to clean RAM\n",
    "\n",
    "# Data handling and processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# Image processing and visualization\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Deep Learning with PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "# --- Environment Configuration ---\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# This ensures that SimpleITK does not print excessive warnings.\n",
    "sitk.ProcessObject.SetGlobalWarningDisplay(False)\n",
    "\n",
    "# Setup reproducibility\n",
    "import random\n",
    "RANDOM_SEED   = 42\n",
    "def set_seed(seed=RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries imported and environment configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tUE8MBysrkEO",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUE8MBysrkEO",
    "outputId": "e8940b64-9aed-408f-85a9-16bf46f71955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Histopathology Distribution:\n",
      "\n",
      "Histopathology\n",
      "Astrocytoma          33\n",
      "Glioblastoma         31\n",
      "Other                26\n",
      "Oligodendroglioma    24\n",
      "Name: count, dtype: int64\n",
      "Clinical metadata loaded and cleaned. Final patient count: 98\n",
      "Total patients: 98\n",
      "Total labels: 98\n",
      "Split sizes (patients if available):\n",
      "Train: 68  Val: 10  Test: 20\n",
      "Indexing 18 patients...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Patients: 100%|██████████| 18/18 [00:14<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset ready in 14.89s.\n",
      "DataLoaders ready. Test: 673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title re-load model\n",
    "\n",
    "# Utility functions for I/O and alignment\n",
    "\n",
    "def load_dicom_series_to_3d_image(p): return sitk.ReadImage(sitk.ImageSeriesReader().GetGDCMSeriesFileNames(p))\n",
    "def reconstruct_mask_in_image_space(m, r): return sitk.Resample(m, r, sitk.Transform(), sitk.sitkNearestNeighbor, 0, m.GetPixelID())\n",
    "def load_volume_any(p): return load_dicom_series_to_3d_image(p) if os.path.isdir(p) else sitk.ReadImage(p)\n",
    "\n",
    "def normalize_zero_one(x):\n",
    "    a, b = np.percentile(x, 1), np.percentile(x, 99)\n",
    "    return np.clip((x - a) / (b - a + 1e-6), 0.0, 1.0)\n",
    "\n",
    "\n",
    "def discover_patients(p):\n",
    "    pats = []\n",
    "    for sub in sorted(glob(os.path.join(p, \"sub-*\"))):\n",
    "        idx = os.path.basename(sub).split(\"-\")[-1]\n",
    "        imgs = glob(os.path.join(sub, \"anat\", \"T1w*\"))\n",
    "        segs = glob(os.path.join(sub, \"seg\", f\"ReMIND-{idx}-preop-SEG-tumor-*.nrrd\"))\n",
    "        if imgs and segs: pats.append({'id': idx, 'img_path': imgs[0], 'seg_path': segs[0]})\n",
    "    return pats\n",
    "\n",
    "DATA_PATH = \"data/ReMIND\"\n",
    "clinical_df = pd.read_csv(f\"{DATA_PATH}/ReMIND_metadata.csv\")\n",
    "clinical_df\n",
    "# Data Cleaning Step 1: Group Rare Classes\n",
    "hist_counts = clinical_df[\"Histopathology\"].value_counts()\n",
    "# Identify all tumor types that appear 2 or fewer times\n",
    "to_other = hist_counts[hist_counts <= 2].index\n",
    "clinical_df[\"Histopathology\"] = clinical_df[\"Histopathology\"].replace(to_other, \"Other\")\n",
    "print(\"\\nCleaned Histopathology Distribution:\\n\")\n",
    "print(clinical_df[\"Histopathology\"].value_counts())\n",
    "excluded_patients = [7, 16, 4, 19, 22, 23, 27, 35, 41, 43, 47, 60, 76, 92, 95, 114]\n",
    "clinical_df = clinical_df[~clinical_df[\"Case Number\"].isin(excluded_patients)].reset_index(drop=True)\n",
    "print(f\"Clinical metadata loaded and cleaned. Final patient count: {len(clinical_df)}\")\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, patient_df: pd.DataFrame, data_path: str, size: int = 512, augment: bool = True):\n",
    "        self.data_path, self.size, self.slices = data_path, size, []\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(size, size, interpolation=1),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.6),\n",
    "            A.Normalize(mean=0.0, std=1.0), ToTensorV2(),\n",
    "        ]) if augment else A.Compose([\n",
    "            A.Resize(size, size, interpolation=1), A.Normalize(mean=0.0, std=1.0), ToTensorV2(),\n",
    "        ])\n",
    "        start_time = time.time()\n",
    "        patients = discover_patients(data_path)\n",
    "        valid_ids = set(patient_df[\"Case Number\"].astype(str).str.zfill(3))\n",
    "        patients_to_process = [p for p in patients if p['id'] in valid_ids]\n",
    "        # print(\"Filtered patients without available mask:\",len(clinical_df) - len(patients_to_process))\n",
    "        print(f\"Indexing {len(patients_to_process)} patients...\")\n",
    "        for pat in tqdm(patients_to_process, desc=\"Processing Patients\"):\n",
    "          ref_img = load_volume_any(pat['img_path'])\n",
    "          mask = sitk.ReadImage(pat['seg_path'])\n",
    "          mask_resampled = reconstruct_mask_in_image_space(mask, ref_img)\n",
    "          img_np = normalize_zero_one(sitk.GetArrayFromImage(ref_img).astype(np.float32))\n",
    "          mask_np = sitk.GetArrayFromImage(mask_resampled).astype(np.uint8)\n",
    "          for k in range(img_np.shape[0]):\n",
    "              if mask_np[k].any(): self.slices.append((img_np[k], mask_np[k]))\n",
    "        print(f\"\\nDataset ready in {time.time() - start_time:.2f}s.\")\n",
    "\n",
    "    def __len__(self): return len(self.slices)\n",
    "    def __getitem__(self, idx):\n",
    "        img, msk = self.slices[idx]; t=self.transform(image=img, mask=msk); return t[\"image\"], t[\"mask\"].unsqueeze(0).float()\n",
    "\n",
    "# Stratified patient-wise split (if metadata exists), else random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "patient_data = (clinical_df[[\"Case Number\",\"Histopathology\"]]\n",
    "                .drop_duplicates()\n",
    "                .set_index(\"Case Number\"))\n",
    "patient_list = patient_data.index.to_numpy()\n",
    "patient_labels = patient_data[\"Histopathology\"].to_numpy()\n",
    "print(f\"Total patients: {len(patient_list)}\")\n",
    "print(f\"Total labels: {len(patient_labels)}\")\n",
    "\n",
    "pt_train, pt_temp, lb_train, lb_temp = train_test_split(\n",
    "    patient_list, patient_labels, test_size=0.3, stratify=patient_labels, random_state=RANDOM_SEED\n",
    ")\n",
    "pt_val, pt_test, _, _ = train_test_split(\n",
    "    pt_temp, lb_temp, test_size=(2/3.0), stratify=lb_temp, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "train_df = clinical_df[clinical_df[\"Case Number\"].isin(pt_train)]\n",
    "val_df   = clinical_df[clinical_df[\"Case Number\"].isin(pt_val)]\n",
    "test_df  = clinical_df[clinical_df[\"Case Number\"].isin(pt_test)]\n",
    "\n",
    "print(\"Split sizes (patients if available):\")\n",
    "if \"Case Number\" in clinical_df and len(clinical_df)>0:\n",
    "    print(\"Train:\", len(pt_train), \" Val:\", len(pt_val), \" Test:\", len(pt_test))\n",
    "\n",
    "# Build datasets/loaders\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# train_ds = SegmentationDataset(train_df, DATA_PATH, size=512, augment=True)\n",
    "# val_ds   = SegmentationDataset(val_df,   DATA_PATH, size=512, augment=False)\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "# val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# print(f\"DataLoaders ready. Train: {len(train_ds)}, Validation: {len(val_ds)}\")\n",
    "\n",
    "def dice_coefficient(logits, targets, smooth=1.0):\n",
    "    preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "    preds_flat = preds.view(-1)\n",
    "    targets_flat = targets.view(-1)\n",
    "    intersection = (preds_flat * targets_flat).sum()\n",
    "    union = preds_flat.sum() + targets_flat.sum()\n",
    "    return ((2. * intersection + smooth) / (union + smooth))\n",
    "# Standard BCE Loss with logits\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        p = probs.view(-1); t = targets.view(-1)\n",
    "        inter = (p * t).sum()\n",
    "        dice  = (2*inter + self.smooth) / (p.sum() + t.sum() + self.smooth)\n",
    "        return 1 - dice\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "    def forward(self, logits, targets):\n",
    "        return 0.5*self.bce(logits, targets) + 0.5*self.dice(logits, targets)\n",
    "# SOLUTION for Q3\n",
    "def compute_iou(logits, targets, threshold=0.5, eps=1e-7):\n",
    "    preds = (torch.sigmoid(logits) > threshold).float()\n",
    "    t = (targets > 0.5).float()\n",
    "    inter = (preds * t).sum()\n",
    "    union = preds.sum() + t.sum() - inter\n",
    "    iou = ((inter + eps) / (union + eps))\n",
    "    return iou.item()\n",
    "\n",
    "def compute_precision_recall(logits, targets, threshold=0.5, eps=1e-7):\n",
    "    preds = (torch.sigmoid(logits) > threshold).float()\n",
    "    t = (targets > 0.5).float()\n",
    "    TP = (preds * t).sum()\n",
    "    FP = (preds * (1 - t)).sum()\n",
    "    FN = ((1 - preds) * t).sum()\n",
    "    precision = (TP + eps) / (TP + FP + eps)\n",
    "    recall    = (TP + eps) / (TP + FN + eps)\n",
    "    return precision.item(), recall.item()\n",
    "\n",
    "test_ds  = SegmentationDataset(test_df,  DATA_PATH, size=512, augment=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "print(f\"DataLoaders ready. Test: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RsZjuCVFHGHV",
   "metadata": {
    "id": "RsZjuCVFHGHV"
   },
   "source": [
    "## 6. Advanced Topic I: Quality Control via Uncertainty\n",
    "\n",
    "A good model knows what it knows, but a *great* model knows what it *doesn't* know. We will now use **Monte Carlo (MC) Dropout** to estimate our model's uncertainty.\n",
    "\n",
    "> ### Deep Dive: Understanding and Quantifying Uncertainty\n",
    ">\n",
    "> In machine learning, not all errors are equal. Uncertainty quantification aims to distinguish between two fundamental types of uncertainty:\n",
    "> - **Aleatoric Uncertainty (Data Uncertainty):** This is uncertainty inherent in the data itself. It's caused by noise, measurement errors, or ambiguity. For example, a blurry part of an MRI scan has high aleatoric uncertainty. This type of uncertainty *cannot* be reduced by collecting more data.\n",
    "> - **Epistemic Uncertainty (Model Uncertainty):** This reflects the model's own ignorance about the data. It's high in regions of the input space where the model has seen little or no training data. For example, a very rare type of tumor might cause high epistemic uncertainty. This type of uncertainty *can* be reduced by training the model on more diverse data.\n",
    ">\n",
    "> **Monte Carlo (MC) Dropout:** This is a clever and computationally cheap technique to estimate *epistemic* uncertainty. The key idea, proposed by Gal & Ghahramani (2016), is to keep the `Dropout` layers (normally only active during training) turned ON during inference.\n",
    "> 1. We perform `N` stochastic forward passes on the same input image. Each pass is different because dropout randomly \"switches off\" different neurons.\n",
    "> 2. This process generates `N` different prediction masks. It's like getting `N` second opinions from slightly different \"sub-networks\" within our main model.\n",
    "> 3. We then compute statistics across these `N` masks:\n",
    ">     - The **mean** of the predictions gives us a robust, ensemble-like segmentation.\n",
    ">     - The **variance** of the predictions serves as our uncertainty map. If the sub-networks disagree on a region, the variance will be high, signaling high model uncertainty.\n",
    ">\n",
    "> **In a nutshell:** MC Dropout simulates an ensemble of models by using dropout at test time, allowing us to measure model disagreement (variance) as a proxy for its confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tceH9pQbHYZn",
   "metadata": {
    "id": "tceH9pQbHYZn"
   },
   "source": [
    "### 6.1. Model with Dropout and MC Inference (7 minutes)\n",
    "\n",
    "We'll define a special U-Net with a `Dropout` layer at its bottleneck. By keeping this layer active during inference and running the model multiple times, we get a distribution of predictions. The **variance** of this distribution is our uncertainty map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "REfbm-LnKKXg",
   "metadata": {
    "id": "REfbm-LnKKXg"
   },
   "outputs": [],
   "source": [
    "# Define a U-Net variant that has a dropout layer we can activate for MC-Dropout\n",
    "class UnetWithBottleneckDropout(smp.Unet):\n",
    "    def __init__(self, dropout_rate=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.bottleneck_dropout = nn.Dropout2d(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        # Apply dropout to the deepest feature map (the bottleneck)\n",
    "        features[-1] = self.bottleneck_dropout(features[-1])\n",
    "        decoder_output = self.decoder(features)\n",
    "        return self.segmentation_head(decoder_output)\n",
    "\n",
    "def enable_dropout(model):\n",
    "    \"\"\"Activates dropout layers for inference.\"\"\"\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__.startswith('Dropout'):\n",
    "            m.train()\n",
    "\n",
    "# Load our best fine-tuned model's weights into this new architecture\n",
    "final_model = UnetWithBottleneckDropout(\n",
    "    encoder_name=\"resnet50\", encoder_weights=None, in_channels=1, classes=1\n",
    ").to(device)\n",
    "final_model.load_state_dict(torch.load(best_checkpoint, map_location=device))\n",
    "print(\"Uncertainty-capable model loaded with best weights.\")\n",
    "\n",
    "# MC Dropout Inference Function\n",
    "def perform_mc_inference(model, loader, device, num_samples=25):\n",
    "    \"\"\"Performs MC Dropout on an entire dataset and returns detailed results.\"\"\"\n",
    "    model.eval()\n",
    "    enable_dropout(model) # Keep dropout active\n",
    "\n",
    "    mc_results = []\n",
    "    for img_t, msk_t in tqdm(loader, desc=\"MC Inference on Test Set\"):\n",
    "        img_t, msk_t = img_t.to(device), msk_t.to(device)\n",
    "\n",
    "        stochastic_preds = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_samples):\n",
    "                stochastic_preds.append(torch.sigmoid(model(img_t)))\n",
    "\n",
    "        # Stack predictions and compute mean and variance\n",
    "        predictions_stack = torch.stack(stochastic_preds)\n",
    "        mean_pred = predictions_stack.mean(dim=0)\n",
    "        variance_map = predictions_stack.var(dim=0)\n",
    "\n",
    "        # Store all relevant data, moving to CPU to save GPU memory\n",
    "        mc_results.append({\n",
    "            'img': img_t.cpu(), 'mask': msk_t.cpu(),\n",
    "            'mean_pred': mean_pred.cpu(), 'variance_map': variance_map.cpu(),\n",
    "            'avg_variance': variance_map.mean().item() # A single score for sorting\n",
    "        })\n",
    "    return mc_results\n",
    "\n",
    "# Run MC inference on the test set (this will take time)\n",
    "test_loader_single_batch = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "mc_results = perform_mc_inference(final_model, test_loader_single_batch, device)\n",
    "\n",
    "# Sort cases from most uncertain to least uncertain\n",
    "mc_results.sort(key=lambda x: x['avg_variance'], reverse=True)\n",
    "print(f\"\\nMC Dropout complete. Most uncertain case has avg variance: {mc_results[0]['avg_variance']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5RdTZqk7MPQQ",
   "metadata": {
    "id": "5RdTZqk7MPQQ"
   },
   "source": [
    "### 6.2. Finding and Analyzing Difficult Cases with Uncertainty\n",
    "\n",
    "Now we can use our uncertainty metric to automatically find the cases the model was least confident about.\n",
    "\n",
    "**Q5 Analysis**: After running the uncertainty analysis, look at the most uncertain cases. Do regions of high uncertainty (hot colors) correspond to tumor boundaries, complex anatomy, or areas where the model made a clear mistake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "myYnxjKEP2BU",
   "metadata": {
    "cellView": "form",
    "id": "myYnxjKEP2BU"
   },
   "outputs": [],
   "source": [
    "#@title Helper function to plot uncertain cases\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plot_uncertain_cases(\n",
    "    results, num_cases=10, thr=0.5, overlay_alpha=0.40, unc_alpha=0.45,\n",
    "    var_mode=\"pred\", band=(0.4, 0.6), var_clip=(1, 99),\n",
    "    contour_width=1.5, add_pred_contour=True, compute_dice=True,\n",
    "    sort_by_uncertainty=True,\n",
    "    shuffle=True,           # randomize the order shown\n",
    "    seed=None,               # set for reproducibility\n",
    "    top_k=None               # take from top-K most uncertain before shuffling\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # keep your existing to_np + (optional) sorting\n",
    "    def to_np(x):\n",
    "        x = x.detach().cpu().numpy() if hasattr(x, \"detach\") else np.asarray(x)\n",
    "        while x.ndim > 2: x = x[0]\n",
    "        return x\n",
    "\n",
    "    if sort_by_uncertainty and results and ('avg_variance' in results[0]):\n",
    "        results = sorted(results, key=lambda c: c.get('avg_variance', 0.0), reverse=True)\n",
    "\n",
    "    # choose pool (top_k) and optionally shuffle\n",
    "    pool = results[:top_k] if (top_k is not None) else results\n",
    "    if shuffle:\n",
    "        pool = pool.copy()\n",
    "        rng.shuffle(pool)\n",
    "\n",
    "    print(f\"--- Displaying {min(num_cases, len(pool))} Most Uncertain (pool={len(pool)}; \"\n",
    "          f\"{'shuffled' if shuffle else 'ordered'}) ---\")\n",
    "\n",
    "    # iterate over the (maybe shuffled) pool\n",
    "    for i in range(min(num_cases, len(pool))):\n",
    "        case = pool[i]\n",
    "        # case    = results[i]\n",
    "        img     = to_np(case['img'])\n",
    "        gt      = (to_np(case['mask']) > 0.5)\n",
    "        mean_p  = to_np(case['mean_pred'])\n",
    "        var_map = to_np(case['variance_map'])\n",
    "        pred    = (mean_p > thr)\n",
    "\n",
    "        dice = None\n",
    "        if compute_dice:\n",
    "            inter = np.logical_and(pred, gt).sum()\n",
    "            denom = pred.sum() + gt.sum()\n",
    "            dice  = (2.0 * inter) / (denom + 1e-8)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "        st = f\"Case #{i+1} Most Uncertain\"\n",
    "        if 'avg_variance' in case:\n",
    "            st += f\" | Avg Var: {case['avg_variance']:.6f}\"\n",
    "        fig.suptitle(st, fontsize=16)\n",
    "\n",
    "        # Original\n",
    "        ax = axes[0]\n",
    "        ax.imshow(img, cmap='gray'); ax.set_title('Original MRI'); ax.axis('off')\n",
    "\n",
    "        # Segmentation overlay\n",
    "        ax = axes[1]\n",
    "        ax.imshow(img, cmap='gray')\n",
    "\n",
    "        red = np.zeros((*pred.shape, 4), float)\n",
    "        red[..., 0] = pred.astype(float)\n",
    "        red[..., 3] = pred.astype(float) * overlay_alpha\n",
    "        ax.imshow(red)\n",
    "\n",
    "        if gt.any():\n",
    "            for c in measure.find_contours(gt.astype(float), 0.5):\n",
    "                ax.plot(c[:, 1], c[:, 0], color='lime', linewidth=contour_width)\n",
    "        if add_pred_contour and pred.any():\n",
    "            for c in measure.find_contours(pred.astype(float), 0.5):\n",
    "                ax.plot(c[:, 1], c[:, 0], color='red', linewidth=contour_width, linestyle='--')\n",
    "\n",
    "        title = 'Segmentation Overlay'\n",
    "        if dice is not None:\n",
    "            title += f'\\nDice = {dice:.4f}'\n",
    "        ax.set_title(title)\n",
    "        # Explicit handles (avoid single-arg legend parsing issue)\n",
    "        legend_handles = [\n",
    "            Line2D([], [], color='lime', lw=contour_width, label='GT'),\n",
    "            Line2D([], [], color='red',  lw=contour_width, ls='--', label='Pred')\n",
    "        ]\n",
    "        ax.legend(handles=legend_handles, loc='lower right', frameon=True)\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Uncertainty overlay (variance), masked + percentile clip\n",
    "        ax = axes[2]\n",
    "        ax.imshow(img, cmap='gray')\n",
    "\n",
    "        if var_mode == \"pred\":\n",
    "            mask = pred\n",
    "        elif var_mode == \"union\":\n",
    "            mask = np.logical_or(pred, gt)\n",
    "        elif var_mode == \"band\":\n",
    "            lb, ub = band\n",
    "            mask = (mean_p >= lb) & (mean_p <= ub)\n",
    "        else:  # 'all'\n",
    "            mask = np.ones_like(var_map, dtype=bool)\n",
    "\n",
    "        safe_var = np.nan_to_num(var_map, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        mvar = np.ma.masked_where(~mask, safe_var)\n",
    "        if mask.any():\n",
    "            vmin, vmax = np.nanpercentile(safe_var[mask], var_clip)\n",
    "        else:\n",
    "            vmin, vmax = np.nanpercentile(safe_var, var_clip)\n",
    "\n",
    "        im3 = ax.imshow(mvar, cmap='hot', alpha=unc_alpha, vmin=vmin, vmax=vmax)\n",
    "        ax.set_title('Uncertainty Overlay'); ax.axis('off')\n",
    "        cb = fig.colorbar(im3, ax=ax, fraction=0.046, pad=0.04); cb.set_label('Variance')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.93]); plt.show()\n",
    "\n",
    "# Simple version\n",
    "def plot_uncertain_cases_simple(results, num_cases=5):\n",
    "    \"\"\"Plots the most uncertain cases from MC-Dropout results.\"\"\"\n",
    "    print(f\"--- Displaying the {num_cases} Most Uncertain Predictions ---\")\n",
    "    for i in range(min(num_cases, len(results))):\n",
    "        case = results[i]\n",
    "        img, mask = case['img'][0, 0], case['mask'][0, 0]\n",
    "        mean_p, var_map = case['mean_pred'][0, 0], case['variance_map'][0, 0]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(22, 6))\n",
    "        fig.suptitle(f\"Case #{i+1} Most Uncertain | Avg Variance: {case['avg_variance']:.6f}\", fontsize=16)\n",
    "\n",
    "        axes[0].imshow(img, cmap='gray'); axes[0].set_title('Input MRI'); axes[0].axis('off')\n",
    "        axes[1].imshow(mask, cmap='gray'); axes[1].set_title('Ground Truth'); axes[1].axis('off')\n",
    "\n",
    "        im2 = axes[2].imshow(mean_p, cmap='viridis'); axes[2].set_title('Mean Prediction'); axes[2].axis('off')\n",
    "        fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "        im3 = axes[3].imshow(var_map, cmap='hot'); axes[3].set_title('Uncertainty (Variance)'); axes[3].axis('off')\n",
    "        fig.colorbar(im3, ax=axes[3], fraction=0.046, pad=0.04)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UikrrWlvP5mT",
   "metadata": {
    "id": "UikrrWlvP5mT"
   },
   "outputs": [],
   "source": [
    "plot_uncertain_cases(mc_results, num_cases=5, shuffle=True, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TDeTyn9SdM1a",
   "metadata": {
    "id": "TDeTyn9SdM1a"
   },
   "source": [
    "**Write your analysis here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wYqpjyTHMoau",
   "metadata": {
    "id": "wYqpjyTHMoau"
   },
   "source": [
    "\n",
    "### 6.3. Final Check: Does Uncertainty Correlate with Error?\n",
    "\n",
    "Our hypothesis is that high uncertainty should correlate with low performance (i.e., low Dice score). Let's test this directly by plotting average variance against the Dice score for every sample in the test set. A negative correlation would validate our uncertainty metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JqIFQjzbMp5Q",
   "metadata": {
    "id": "JqIFQjzbMp5Q"
   },
   "outputs": [],
   "source": [
    "# Calculate Dice score for each case in our MC results\n",
    "def dice_numpy(pred, target, smooth=1e-6):\n",
    "    p_flat = (pred > 0.5).flatten()\n",
    "    t_flat = (target > 0.5).flatten()\n",
    "    intersection = (p_flat & t_flat).sum()\n",
    "    union = p_flat.sum() + t_flat.sum()\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "for case in mc_results:\n",
    "    case['dice_score'] = dice_numpy(case['mean_pred'].numpy(), case['mask'].numpy())\n",
    "\n",
    "# Extract data for plotting\n",
    "variances = [case['avg_variance'] for case in mc_results]\n",
    "dice_scores = [case['dice_score'] for case in mc_results]\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "correlation = np.corrcoef(variances, dice_scores)[0, 1]\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(x=variances, y=dice_scores, scatter_kws={'alpha':0.5})\n",
    "plt.title(f'Uncertainty vs. Model Performance (Correlation: {correlation:.3f})', fontsize=16)\n",
    "plt.xlabel('Average Variance (Uncertainty)')\n",
    "plt.ylabel('Dice Score (Performance)')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHLLH2hHMwKs",
   "metadata": {
    "id": "ZHLLH2hHMwKs"
   },
   "source": [
    "\n",
    "The negative correlation confirms it: **higher uncertainty is a strong indicator of a poorer segmentation**. We can now confidently use this metric to flag cases for review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xerWpJ4SWffr",
   "metadata": {
    "id": "xerWpJ4SWffr"
   },
   "source": [
    "> 75 minutes in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n0u6o1OdSBei",
   "metadata": {
    "id": "n0u6o1OdSBei"
   },
   "source": [
    "## 7. Advanced Topic II: Prompt-based Segmentation with MedSAM\n",
    "\n",
    "We've built a great model, but how does it stack up against a massive, pre-trained **Foundation Model**? We'll now evaluate **MedSAM**, a version of the Segment Anything Model adapted for medical images.\n",
    "\n",
    "MedSAM is **prompt-based**: you give it an image and a \"prompt\" (like a bounding box) to tell it what to segment. This makes it incredibly flexible. For a fair comparison, we'll \"cheat\" by giving it the perfect prompt: the bounding box derived from the ground truth mask.\n",
    "\n",
    "> We evaluate MedSAM with a **bounding-box from the ground truth** to simulate a best-case clinical prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G3VNAhJUSX6o",
   "metadata": {
    "id": "G3VNAhJUSX6o"
   },
   "source": [
    "### Q6: Evaluate MedSAM's Performance\n",
    "**Your Task**: Run the following cells to install MedSAM, load its weights, and evaluate its performance on our validation set. Compare the final Dice score to the one you achieved with your U-Net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gRpUqoqFTDX4",
   "metadata": {
    "id": "gRpUqoqFTDX4"
   },
   "source": [
    "#### Setup MedSAM (8 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eVhqZR22TBDR",
   "metadata": {
    "id": "eVhqZR22TBDR"
   },
   "outputs": [],
   "source": [
    "# Setup MedSAM\n",
    "# Optional install (only needed once)\n",
    "!pip install -q git+https://github.com/bowang-lab/MedSAM.git\n",
    "\n",
    "# Download MedSAM weights if not present (~350MB)\n",
    "MEDSAM_WEIGHTS_PATH = \"./medsam_vit_b.pth\"\n",
    "if not os.path.exists(MEDSAM_WEIGHTS_PATH):\n",
    "    print(\"Downloading MedSAM weights...\")\n",
    "    !wget -q -O {MEDSAM_WEIGHTS_PATH} https://zenodo.org/records/10689643/files/medsam_vit_b.pth\n",
    "\n",
    "medsam_model = None\n",
    "try:\n",
    "    from segment_anything import sam_model_registry\n",
    "    medsam_model = sam_model_registry['vit_b'](checkpoint=MEDSAM_WEIGHTS_PATH).to(device).eval()\n",
    "    print(\"MedSAM model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load MedSAM. Skipping this section. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4Fo1pu1IY2eZ",
   "metadata": {
    "id": "4Fo1pu1IY2eZ"
   },
   "source": [
    "#### MedSAM Helper Functions: Engine of Prompt-Based Inference\n",
    "\n",
    "To interact with a powerful foundation model like MedSAM, we use two key helper functions. Together, they form a robust pipeline to query the model, turning a simple box prompt into a high-resolution segmentation.\n",
    "\n",
    "#### `get_bounding_box_from_mask(mask_tensor)`\n",
    "\n",
    "This utility function acts as our \"digital ruler.\" It takes a ground truth segmentation mask and automatically calculates the tightest possible bounding box around the region of interest (the tumor). It finds all non-zero pixels in the mask, identifies their minimum and maximum coordinates, and returns them in the standard `[x_min, y_min, x_max, y_max]` format. This is essential for generating the \"perfect\" prompt needed for our systematic evaluation.\n",
    "\n",
    "#### `medsam_inference_with_box(...)`\n",
    "\n",
    "This is the main workhorse function that orchestrates the MedSAM prediction. Its workflow involves several key steps:\n",
    "\n",
    "*   **Coordinate Normalization**: This is the most critical step for performance. It translates the bounding box coordinates from our image's space into MedSAM's native 1024x1024 space, which is what its `prompt_encoder` expects.\n",
    "*   **Prompt Encoding**: The normalized box is converted into mathematical embeddings that represent the spatial query.\n",
    "*   **Mask Decoding**: The model's `mask_decoder` intelligently fuses information from the **image embedding** (what the image contains) with the **prompt embedding** (what we want to segment) to produce a low-resolution prediction.\n",
    "*   **Upsampling and Binarization**: This low-resolution output is upsampled to the original image size using bilinear interpolation and then converted into a final, clean binary mask by applying a sigmoid and a 0.5 threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4wSb5-fZb_SV",
   "metadata": {
    "id": "4wSb5-fZb_SV"
   },
   "outputs": [],
   "source": [
    "def get_bounding_box_from_mask(mask_tensor):\n",
    "    \"\"\"Extracts the bounding box [x_min, y_min, x_max, y_max] from a binary mask tensor.\"\"\"\n",
    "    coords = torch.nonzero(mask_tensor.squeeze())\n",
    "    if coords.numel() == 0: return [0, 0, 1, 1] # Fallback for empty mask\n",
    "    y_min, x_min = coords.min(0).values\n",
    "    y_max, x_max = coords.max(0).values\n",
    "    return [x_min.item(), y_min.item(), x_max.item(), y_max.item()]\n",
    "\n",
    "@torch.no_grad()\n",
    "def medsam_inference_with_box(model, img_embedding, box_prompt, original_img_size):\n",
    "    \"\"\"\n",
    "    Performs MedSAM inference using a bounding box prompt.\n",
    "    Handles coordinate normalization internally.\n",
    "    \"\"\"\n",
    "    H, W = original_img_size\n",
    "    # SAM's prompt encoder expects coordinates normalized to its native 1024x1024 space.\n",
    "    # This is a key step for performance.\n",
    "    box_norm = np.array(box_prompt) / np.array([W, H, W, H]) * 1024\n",
    "    box_torch = torch.as_tensor(box_norm, dtype=torch.float, device=model.device).unsqueeze(0)\n",
    "\n",
    "    sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "        points=None,\n",
    "        boxes=box_torch,\n",
    "        masks=None,\n",
    "    )\n",
    "    low_res_logits, _ = model.mask_decoder(\n",
    "        image_embeddings=img_embedding,\n",
    "        image_pe=model.prompt_encoder.get_dense_pe(),\n",
    "        sparse_prompt_embeddings=sparse_embeddings,\n",
    "        dense_prompt_embeddings=dense_embeddings,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    # Upsample the low-resolution mask to the original image size\n",
    "    upscaled_logits = nn.functional.interpolate(\n",
    "        low_res_logits,\n",
    "        size=(H, W),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    # Return the final binary mask\n",
    "    return (torch.sigmoid(upscaled_logits) > 0.5).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1Qt78IsYmEG5",
   "metadata": {
    "id": "1Qt78IsYmEG5"
   },
   "source": [
    "#### Run MedSAM Validation\n",
    "\n",
    "To streamline the process of testing the MedSAM model and to make our experimentation more robust, we have encapsulated the entire evaluation workflow into a single, comprehensive function: `run_medsam_evaluation`.\n",
    "\n",
    "This function serves as a self-contained \"test harness.\" It takes the MedSAM model and a DataFrame defining the dataset split (e.g., `val_df` or `test_df`) and returns a dictionary containing a full suite of performance metrics. This approach makes it easy to systematically benchmark the model on different data subsets.\n",
    "\n",
    "#### How It Works: A Step-by-Step Breakdown\n",
    "\n",
    "The function executes the following logical steps:\n",
    "\n",
    "1.  **Data Preparation**: It begins by creating a new `SegmentationDataset` and `DataLoader` instance, specifically configured with the image `size` required by MedSAM (typically 1024x1024).\n",
    "2.  **Iterating Through the Dataset**: It loops through every single sample in the provided data loader.\n",
    "3.  **Image Preprocessing**: MedSAM's Vision Transformer (ViT) encoder was trained on standard RGB images. Therefore, for each single-channel grayscale MRI slice, the function converts it into a 3-channel image by repeating the channel dimension three times.\n",
    "4.  **Generating the 'Perfect' Prompt**: To give MedSAM its best chance to perform well, we simulate a perfect user interaction. The function generates a bounding box prompt directly from the ground truth mask for each image.\n",
    "5.  **Running Inference**: It calls our helper function, `medsam_inference_with_box`, using the prepared image embedding and the bounding box prompt to get the final predicted segmentation mask.\n",
    "6.  **Calculating Metrics**: The function then calculates a full suite of performance metrics (Dice, IoU, Precision, and Recall) by comparing the model's prediction to the ground truth.\n",
    "\n",
    "*Note on hard dice*: It is important to note that this function calculates metrics on a **binarized** prediction (i.e., after applying a `> 0.5` threshold to the model's probability outputs). This approach, sometimes called a **\"Hard Dice\"**, measures how well the final *mask* overlaps with the ground truth. This ensures consistency with how we typically interpret metrics like Precision and Recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RQ7KMNUPk85d",
   "metadata": {
    "id": "RQ7KMNUPk85d"
   },
   "outputs": [],
   "source": [
    "def run_medsam_evaluation(\n",
    "    medsam_model,\n",
    "    df,\n",
    "    data_path,\n",
    "    device,\n",
    "    num_workers=4,\n",
    "    threshold_iou=0.5,\n",
    "    threshold_precision_recall=0.5,\n",
    "    size=1024,\n",
    "    eps=1e-6\n",
    "):\n",
    "    if medsam_model is None:\n",
    "        print(\"MedSAM model not provided.\")\n",
    "        return None\n",
    "\n",
    "    ds_sam = SegmentationDataset(df, data_path, size=size, augment=False)\n",
    "    loader_sam = DataLoader(ds_sam, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    medsam_dice_scores, medsam_iou, medsam_prec, medsam_rec = [], [], [], []\n",
    "\n",
    "    start_time = time.time()\n",
    "    medsam_model.eval()\n",
    "\n",
    "    for img_t, msk_t in tqdm(loader_sam, desc=\"MedSAM Evaluation\"):\n",
    "        img_t = img_t.to(device)         # [1, H, W]\n",
    "        msk_t = (msk_t > 0.5).float().to(device)  # ensure binary GT\n",
    "\n",
    "        # 3-channel input for MedSAM encoder\n",
    "        img_3c = img_t.repeat(1, 3, 1, 1)  # [1, 3, H, W]\n",
    "\n",
    "        # GT-box prompt (coords must match this resized space)\n",
    "        box_prompt = get_bounding_box_from_mask(msk_t)\n",
    "\n",
    "        # Get embedding + prediction (probabilities in [0,1])\n",
    "        with torch.no_grad():\n",
    "            img_embedding = medsam_model.image_encoder(img_3c)\n",
    "            pred_prob = medsam_inference_with_box(\n",
    "                medsam_model, img_embedding, box_prompt, original_img_size=(size, size)\n",
    "            )  # expected shape [1, H, W] or [H, W]\n",
    "\n",
    "        # Ensure [H, W] tensor on same device\n",
    "        if pred_prob.dim() == 3:\n",
    "            pred_prob = pred_prob.squeeze(0)\n",
    "        pred_prob = pred_prob.to(device).clamp(0, 1)\n",
    "\n",
    "        # Metrics\n",
    "        # Option A: Hard Dice (binary) – matches your IoU/Prec/Rec style\n",
    "        pred_bin = (pred_prob > threshold_precision_recall).float()\n",
    "\n",
    "        dice_val = dice_coefficient(pred_bin, msk_t).item()  # <- Dice on binaries\n",
    "\n",
    "        iou_val  = compute_iou(pred_bin, msk_t, threshold=0.5)  # already binary, thr unused\n",
    "        p, r     = compute_precision_recall(pred_bin, msk_t, threshold=0.5)\n",
    "\n",
    "        medsam_dice_scores.append(dice_val)\n",
    "        medsam_iou.append(iou_val)\n",
    "        medsam_prec.append(p)\n",
    "        medsam_rec.append(r)\n",
    "\n",
    "        # If you instead want soft-dice with probabilities, replace Dice line with:\n",
    "        # dice_val = soft_dice(pred_prob, msk_t, eps=eps)  # requires a soft_dice implementation\n",
    "\n",
    "        # Or Dice expecting logits, compute true logits:\n",
    "        # logits = torch.log(pred_prob + eps) - torch.log(1 - pred_prob + eps)\n",
    "        # dice_val = dice_coefficient(logits, msk_t).item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nMedSAM Finished in {end_time - start_time:.2f} seconds ---\")\n",
    "    print(\"MedSAM (with GT box prompt)\")\n",
    "    print(f\"Dice:      {np.mean(medsam_dice_scores):.4f}\")\n",
    "    print(f\"IoU:       {np.mean(medsam_iou):.4f}\")\n",
    "    print(f\"Precision: {np.mean(medsam_prec):.4f}\")\n",
    "    print(f\"Recall:    {np.mean(medsam_rec):.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"dice_scores\": medsam_dice_scores,\n",
    "        \"iou_scores\": medsam_iou,\n",
    "        \"precision\": medsam_prec,\n",
    "        \"recall\": medsam_rec\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G80-6U__h3kI",
   "metadata": {
    "id": "G80-6U__h3kI"
   },
   "outputs": [],
   "source": [
    "print(\"Compute validation\")\n",
    "threshold_iou = 0.5 #@param {type: \"slider\", min: 0, max: 1, step:0.01}\n",
    "threshold_precision_recall = 0.5 #@param {type: \"slider\", min: 0, max: 1, step:0.01}\n",
    "\n",
    "run_medsam_evaluation(...) #COMPLETE\n",
    "print(\"Validation results finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B_Hk3fcPgM7u",
   "metadata": {
    "id": "B_Hk3fcPgM7u"
   },
   "source": [
    "### 7.2 Visual Comparison: U-Net vs. MedSAM\n",
    "\n",
    "Metrics tell us the average performance, but a visual comparison can reveal the different qualitative behaviors of the two models. Let's take a random sample from our validation set and see how our fine-tuned, fully automatic U-Net compares to the prompt-driven MedSAM.\n",
    "\n",
    "The following code will:\n",
    "1.  Select a sample from the validation set.\n",
    "2.  Generate a prediction using your best-trained U-Net model.\n",
    "3.  Generate a prediction using MedSAM with a ground truth bounding box prompt.\n",
    "4.  Display the input image, ground truth, U-Net prediction, and MedSAM prediction side-by-side.\n",
    "\n",
    "**Reflection**:\n",
    "-   Which model produces a \"cleaner\" or more plausible segmentation?\n",
    "-   Does one model handle the boundaries better than the other?\n",
    "-   How does the prompt-based nature of MedSAM influence its output compared to the fully automatic U-Net?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NczsafczfdDr",
   "metadata": {
    "id": "NczsafczfdDr"
   },
   "outputs": [],
   "source": [
    "# Visualization Code\n",
    "def visualize_comparison(\n",
    "    unet_model,\n",
    "    medsam_model,\n",
    "    unet_ds,\n",
    "    medsam_ds,\n",
    "    idx_to_show,\n",
    "    device=\"cuda\",\n",
    "    thr=0.5,\n",
    "    overlay_alpha=0.25,\n",
    "    contour_width=1.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Compares U-Net and MedSAM predictions for a given sample index, showing:\n",
    "      - Input image\n",
    "      - Ground truth\n",
    "      - Combined overlay: original image + semi-transparent masks + contours\n",
    "        * U-Net (final model): red contour\n",
    "        * MedSAM: orange contour\n",
    "    \"\"\"\n",
    "    import torch, numpy as np, matplotlib.pyplot as plt\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    print(f\"--- Visualizing Comparison for Sample Index: {idx_to_show} ---\")\n",
    "\n",
    "    # U-Net prediction\n",
    "    unet_model.eval()\n",
    "    img_t_unet, msk_t_unet = unet_ds[idx_to_show]    # shapes: [1,H,W]\n",
    "    with torch.no_grad():\n",
    "        logits_unet = unet_model(img_t_unet.unsqueeze(0).to(device))  # [1,1,H,W]\n",
    "        pred_unet = (torch.sigmoid(logits_unet) > thr).float().cpu().squeeze(0).squeeze(0)  # [H,W]\n",
    "\n",
    "    # MedSAM prediction\n",
    "    if medsam_model is not None:\n",
    "        medsam_model.eval()\n",
    "        img_t_sam, msk_t_sam = medsam_ds[idx_to_show]  # [1,1024,1024]\n",
    "        with torch.no_grad():\n",
    "            img_3c = img_t_sam.unsqueeze(0).to(device).repeat(1, 3, 1, 1)  # [1,3,Hs,Ws]\n",
    "            box     = get_bounding_box_from_mask(msk_t_sam.to(device))     # (x1,y1,x2,y2) or similar\n",
    "            emb     = medsam_model.image_encoder(img_3c)\n",
    "            pred_medsam_prob = medsam_inference_with_box(\n",
    "                medsam_model, emb, box, original_img_size=tuple(img_t_sam.shape[-2:])\n",
    "            ).cpu().squeeze()  # [Hs,Ws], probs in [0,1]\n",
    "            pred_medsam = (pred_medsam_prob > thr).float()\n",
    "    else:\n",
    "        pred_medsam = torch.zeros_like(pred_unet)\n",
    "        print(\"Warning: MedSAM model not available. Showing placeholder for its prediction.\")\n",
    "\n",
    "    # Harmonize sizes for overlay (use U-Net's image as reference)\n",
    "    H, W = img_t_unet.shape[-2], img_t_unet.shape[-1]\n",
    "    if pred_medsam.shape != (H, W):\n",
    "        pred_medsam = F.interpolate(\n",
    "            pred_medsam[None, None, ...], size=(H, W), mode=\"nearest\"\n",
    "        ).squeeze(0).squeeze(0)\n",
    "\n",
    "    # Prepare arrays for plotting\n",
    "    img_np   = img_t_unet.squeeze().cpu().numpy()      # [H,W] grayscale\n",
    "    gt_np    = msk_t_unet.squeeze().cpu().numpy()      # [H,W]\n",
    "    unet_np  = pred_unet.cpu().numpy().astype(np.float32)     # [H,W] {0,1}\n",
    "    sam_np   = pred_medsam.cpu().numpy().astype(np.float32)   # [H,W] {0,1}\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "\n",
    "    # 1) Input\n",
    "    axes[0].imshow(img_np, cmap=\"gray\")\n",
    "    axes[0].set_title(f'Input ({getattr(unet_ds,\"size\",W)}×{getattr(unet_ds,\"size\",H)})')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # 2) Ground Truth\n",
    "    axes[1].imshow(img_np, cmap=\"gray\")\n",
    "    axes[1].imshow(np.ma.masked_where(gt_np == 0, gt_np), alpha=overlay_alpha)\n",
    "    axes[1].contour(gt_np, colors='lime', linewidths=contour_width)\n",
    "    axes[1].set_title('Ground Truth (overlay)')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # 3) Combined overlay (original + both predictions)\n",
    "    ax = axes[2]\n",
    "    ax.imshow(img_np, cmap=\"gray\")\n",
    "\n",
    "    # semi-transparent fills\n",
    "    ax.imshow(np.ma.masked_where(unet_np == 0, unet_np), alpha=overlay_alpha)   # U-Net fill\n",
    "    ax.imshow(np.ma.masked_where(sam_np  == 0, sam_np ), alpha=overlay_alpha)   # MedSAM fill\n",
    "\n",
    "    # contours\n",
    "    ax.contour(unet_np, colors='red',    linewidths=contour_width, levels=[0.5])\n",
    "    ax.contour(sam_np,  colors='orange', linewidths=contour_width, levels=[0.5])\n",
    "\n",
    "    ax.set_title('Overlay: U-Net (red) + MedSAM (orange)')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Legend proxy handles\n",
    "    from matplotlib.lines import Line2D\n",
    "    handles = [\n",
    "        Line2D([0],[0], color='red',    lw=contour_width, label='U-Net contour'),\n",
    "        Line2D([0],[0], color='orange', lw=contour_width, label='MedSAM contour'),\n",
    "        Line2D([0],[0], color='lime',   lw=contour_width, label='GT contour')\n",
    "    ]\n",
    "    axes[2].legend(handles=handles, loc='lower right', frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sample_index = 200  #@param {type: \"number\"}\n",
    "\n",
    "test_ds_sam = SegmentationDataset(test_df, DATA_PATH, size=1024, augment=False)\n",
    "visualize_comparison( # COMPLETE\n",
    "    unet_model=...,      # Your best U-Net model from the experiments\n",
    "    medsam_model=...,   # The loaded MedSAM model\n",
    "    unet_ds=...,              # The validation set sized for U-Net (e.g., 256 or 512)\n",
    "    medsam_ds=...,        # The validation set sized for MedSAM (1024)\n",
    "    idx_to_show=sample_index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kN6wNmY3aYgt",
   "metadata": {
    "id": "kN6wNmY3aYgt"
   },
   "source": [
    "\n",
    "\n",
    "> 85 minutes in\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "etmnoiBHM8Hq",
   "metadata": {
    "id": "etmnoiBHM8Hq"
   },
   "source": [
    "Which Model to Use?\n",
    "\n",
    "- **Fine-tuned U-Net:** great for **fully automatic** and fine-grained pipelines when you have labeled data and time to train.\n",
    "- **MedSAM (prompt-based):** great for **interactive tools, annotation, few-shot settings**—fast and flexible with human guidance.\n",
    "\n",
    "## Conclusion and final thoughts\n",
    "\n",
    "Congratulations on completing this comprehensive journey through biomedical image segmentation!\n",
    "\n",
    "You have successfully:\n",
    "- Built a robust data pipeline for real-world medical images.\n",
    "- Trained and optimized a U-Net model by experimenting with different loss functions.\n",
    "- Performed rigorous quantitative and visual quality control, identifying your model's failure modes.\n",
    "- Benchmarked your custom model against a powerful foundation model, MedSAM.\n",
    "- Implemented an uncertainty quantification system to measure your model's confidence and automatically flag difficult cases.\n",
    "\n",
    "\n",
    "> **Next steps:** Now that you know how to train 2D models, you could try 2.5D/3D context, uncertainty estimates, data curation by failure modes, and foundation-model pretraining.\n",
    ">\n",
    "> **Remember**: Building a good-performing model is only half the battle. For clinical applications, a model that is **interpretable, reliable, and knows its own limitations** is not just better—it's essential.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
